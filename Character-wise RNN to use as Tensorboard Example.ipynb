{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-wise RNN to use as Tensorboard Example\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on \"Anna Karenina\". It'll be able to generate new text based on the text from the book. The network's graph, and parameters will be visualized and debugged using Tensorboard\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77, 51, 46, 35, 28, 18, 48, 53, 20, 58, 58, 58, 45, 46, 35, 35, 52,\n",
       "       53, 23, 46, 50, 67, 72, 67, 18,  8, 53, 46, 48, 18, 53, 46, 72, 72,\n",
       "       53, 46, 72, 67, 17, 18, 68, 53, 18, 44, 18, 48, 52, 53,  1, 37, 51,\n",
       "       46, 35, 35, 52, 53, 23, 46, 50, 67, 72, 52, 53, 67,  8, 53,  1, 37,\n",
       "       51, 46, 35, 35, 52, 53, 67, 37, 53, 67, 28,  8, 53,  6, 33, 37, 58,\n",
       "       33, 46, 52, 39, 58, 58, 60, 44, 18, 48, 52, 28, 51, 67, 37], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77, 51, 46, 35, 28, 18, 48, 53, 20, 58],\n",
       "       [69, 37, 19, 53, 51, 18, 53, 50,  6, 44],\n",
       "       [53, 24, 46, 28, 24, 51, 67, 37, 63, 53],\n",
       "       [ 6, 28, 51, 18, 48, 53, 33,  6,  1, 72],\n",
       "       [53, 28, 51, 18, 53, 72, 46, 37, 19, 55],\n",
       "       [53,  5, 51, 48,  6,  1, 63, 51, 53, 72],\n",
       "       [28, 53, 28,  6, 58, 19,  6, 39, 58, 58],\n",
       "       [ 6, 53, 51, 18, 48,  8, 18, 72, 23,  0],\n",
       "       [51, 46, 28, 53, 67,  8, 53, 28, 51, 18],\n",
       "       [18, 48,  8, 18, 72, 23, 53, 46, 37, 19]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "\n",
    "    \n",
    "    with tf.name_scope(\"targets\"):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper( tf.contrib.rnn.BasicLSTMCell(lstm_size)) for _ in range(num_layers)])\n",
    "\n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope(\"sequence_reshape\"):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope(\"logits\"):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram(\"softmax_w\",softmax_w)\n",
    "        tf.summary.histogram(\"softmax_b\",softmax_b)\n",
    "\n",
    "    with tf.name_scope(\"predictions\"):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram(\"predictions\",preds)\n",
    "    \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar(\"cost\",cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope(\"train\"):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "    summaries = tf.summary.merge_all()\n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer','summaries']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "model = build_rnn(len(vocab),\n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter(\"./logs/train\",sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(\"./logs/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1/178 Training loss: 4.4165 0.1873 sec/batch\n",
      "Epoch 1/1  Iteration 2/178 Training loss: 4.3668 0.1474 sec/batch\n",
      "Epoch 1/1  Iteration 3/178 Training loss: 4.1579 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 4/178 Training loss: 4.2689 0.1492 sec/batch\n",
      "Epoch 1/1  Iteration 5/178 Training loss: 4.1726 0.1477 sec/batch\n",
      "Epoch 1/1  Iteration 6/178 Training loss: 4.0751 0.1513 sec/batch\n",
      "Epoch 1/1  Iteration 7/178 Training loss: 3.9772 0.1495 sec/batch\n",
      "Epoch 1/1  Iteration 8/178 Training loss: 3.8872 0.1477 sec/batch\n",
      "Epoch 1/1  Iteration 9/178 Training loss: 3.8096 0.1510 sec/batch\n",
      "Epoch 1/1  Iteration 10/178 Training loss: 3.7503 0.1478 sec/batch\n",
      "Epoch 1/1  Iteration 11/178 Training loss: 3.7003 0.1495 sec/batch\n",
      "Epoch 1/1  Iteration 12/178 Training loss: 3.6585 0.1474 sec/batch\n",
      "Epoch 1/1  Iteration 13/178 Training loss: 3.6211 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 14/178 Training loss: 3.5895 0.1471 sec/batch\n",
      "Epoch 1/1  Iteration 15/178 Training loss: 3.5605 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 16/178 Training loss: 3.5338 0.1514 sec/batch\n",
      "Epoch 1/1  Iteration 17/178 Training loss: 3.5094 0.1572 sec/batch\n",
      "Epoch 1/1  Iteration 18/178 Training loss: 3.4895 0.1575 sec/batch\n",
      "Epoch 1/1  Iteration 19/178 Training loss: 3.4709 0.1574 sec/batch\n",
      "Epoch 1/1  Iteration 20/178 Training loss: 3.4524 0.1550 sec/batch\n",
      "Epoch 1/1  Iteration 21/178 Training loss: 3.4369 0.1474 sec/batch\n",
      "Epoch 1/1  Iteration 22/178 Training loss: 3.4228 0.1520 sec/batch\n",
      "Epoch 1/1  Iteration 23/178 Training loss: 3.4093 0.1592 sec/batch\n",
      "Epoch 1/1  Iteration 24/178 Training loss: 3.3972 0.1624 sec/batch\n",
      "Epoch 1/1  Iteration 25/178 Training loss: 3.3854 0.1572 sec/batch\n",
      "Epoch 1/1  Iteration 26/178 Training loss: 3.3754 0.1562 sec/batch\n",
      "Epoch 1/1  Iteration 27/178 Training loss: 3.3662 0.1574 sec/batch\n",
      "Epoch 1/1  Iteration 28/178 Training loss: 3.3566 0.1572 sec/batch\n",
      "Epoch 1/1  Iteration 29/178 Training loss: 3.3481 0.1612 sec/batch\n",
      "Epoch 1/1  Iteration 30/178 Training loss: 3.3402 0.1730 sec/batch\n",
      "Epoch 1/1  Iteration 31/178 Training loss: 3.3337 0.1560 sec/batch\n",
      "Epoch 1/1  Iteration 32/178 Training loss: 3.3266 0.1711 sec/batch\n",
      "Epoch 1/1  Iteration 33/178 Training loss: 3.3195 0.1641 sec/batch\n",
      "Epoch 1/1  Iteration 34/178 Training loss: 3.3137 0.1680 sec/batch\n",
      "Epoch 1/1  Iteration 35/178 Training loss: 3.3075 0.1700 sec/batch\n",
      "Epoch 1/1  Iteration 36/178 Training loss: 3.3022 0.1764 sec/batch\n",
      "Epoch 1/1  Iteration 37/178 Training loss: 3.2965 0.1501 sec/batch\n",
      "Epoch 1/1  Iteration 38/178 Training loss: 3.2911 0.1732 sec/batch\n",
      "Epoch 1/1  Iteration 39/178 Training loss: 3.2858 0.1600 sec/batch\n",
      "Epoch 1/1  Iteration 40/178 Training loss: 3.2810 0.1698 sec/batch\n",
      "Epoch 1/1  Iteration 41/178 Training loss: 3.2763 0.1605 sec/batch\n",
      "Epoch 1/1  Iteration 42/178 Training loss: 3.2719 0.1619 sec/batch\n",
      "Epoch 1/1  Iteration 43/178 Training loss: 3.2676 0.1653 sec/batch\n",
      "Epoch 1/1  Iteration 44/178 Training loss: 3.2635 0.1639 sec/batch\n",
      "Epoch 1/1  Iteration 45/178 Training loss: 3.2595 0.1624 sec/batch\n",
      "Epoch 1/1  Iteration 46/178 Training loss: 3.2560 0.1764 sec/batch\n",
      "Epoch 1/1  Iteration 47/178 Training loss: 3.2528 0.1659 sec/batch\n",
      "Epoch 1/1  Iteration 48/178 Training loss: 3.2498 0.1713 sec/batch\n",
      "Epoch 1/1  Iteration 49/178 Training loss: 3.2468 0.1489 sec/batch\n",
      "Epoch 1/1  Iteration 50/178 Training loss: 3.2440 0.1474 sec/batch\n",
      "Epoch 1/1  Iteration 51/178 Training loss: 3.2411 0.1492 sec/batch\n",
      "Epoch 1/1  Iteration 52/178 Training loss: 3.2382 0.1474 sec/batch\n",
      "Epoch 1/1  Iteration 53/178 Training loss: 3.2356 0.1493 sec/batch\n",
      "Epoch 1/1  Iteration 54/178 Training loss: 3.2327 0.1473 sec/batch\n",
      "Epoch 1/1  Iteration 55/178 Training loss: 3.2303 0.1581 sec/batch\n",
      "Epoch 1/1  Iteration 56/178 Training loss: 3.2276 0.1580 sec/batch\n",
      "Epoch 1/1  Iteration 57/178 Training loss: 3.2251 0.1581 sec/batch\n",
      "Epoch 1/1  Iteration 58/178 Training loss: 3.2228 0.1634 sec/batch\n",
      "Epoch 1/1  Iteration 59/178 Training loss: 3.2204 0.1660 sec/batch\n",
      "Epoch 1/1  Iteration 60/178 Training loss: 3.2183 0.1670 sec/batch\n",
      "Epoch 1/1  Iteration 61/178 Training loss: 3.2163 0.1619 sec/batch\n",
      "Epoch 1/1  Iteration 62/178 Training loss: 3.2146 0.1478 sec/batch\n",
      "Epoch 1/1  Iteration 63/178 Training loss: 3.2131 0.1681 sec/batch\n",
      "Epoch 1/1  Iteration 64/178 Training loss: 3.2108 0.1624 sec/batch\n",
      "Epoch 1/1  Iteration 65/178 Training loss: 3.2088 0.1489 sec/batch\n",
      "Epoch 1/1  Iteration 66/178 Training loss: 3.2072 0.1506 sec/batch\n",
      "Epoch 1/1  Iteration 67/178 Training loss: 3.2056 0.1517 sec/batch\n",
      "Epoch 1/1  Iteration 68/178 Training loss: 3.2033 0.1717 sec/batch\n",
      "Epoch 1/1  Iteration 69/178 Training loss: 3.2015 0.1600 sec/batch\n",
      "Epoch 1/1  Iteration 70/178 Training loss: 3.2000 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 71/178 Training loss: 3.1983 0.1490 sec/batch\n",
      "Epoch 1/1  Iteration 72/178 Training loss: 3.1970 0.1481 sec/batch\n",
      "Epoch 1/1  Iteration 73/178 Training loss: 3.1954 0.1494 sec/batch\n",
      "Epoch 1/1  Iteration 74/178 Training loss: 3.1940 0.1533 sec/batch\n",
      "Epoch 1/1  Iteration 75/178 Training loss: 3.1927 0.1581 sec/batch\n",
      "Epoch 1/1  Iteration 76/178 Training loss: 3.1914 0.1586 sec/batch\n",
      "Epoch 1/1  Iteration 77/178 Training loss: 3.1901 0.1622 sec/batch\n",
      "Epoch 1/1  Iteration 78/178 Training loss: 3.1888 0.1473 sec/batch\n",
      "Epoch 1/1  Iteration 79/178 Training loss: 3.1874 0.1579 sec/batch\n",
      "Epoch 1/1  Iteration 80/178 Training loss: 3.1859 0.1589 sec/batch\n",
      "Epoch 1/1  Iteration 81/178 Training loss: 3.1845 0.1578 sec/batch\n",
      "Epoch 1/1  Iteration 82/178 Training loss: 3.1833 0.1577 sec/batch\n",
      "Epoch 1/1  Iteration 83/178 Training loss: 3.1821 0.1497 sec/batch\n",
      "Epoch 1/1  Iteration 84/178 Training loss: 3.1808 0.1490 sec/batch\n",
      "Epoch 1/1  Iteration 85/178 Training loss: 3.1793 0.1477 sec/batch\n",
      "Epoch 1/1  Iteration 86/178 Training loss: 3.1779 0.1501 sec/batch\n",
      "Epoch 1/1  Iteration 87/178 Training loss: 3.1765 0.1659 sec/batch\n",
      "Epoch 1/1  Iteration 88/178 Training loss: 3.1751 0.1582 sec/batch\n",
      "Epoch 1/1  Iteration 89/178 Training loss: 3.1739 0.1559 sec/batch\n",
      "Epoch 1/1  Iteration 90/178 Training loss: 3.1728 0.1505 sec/batch\n",
      "Epoch 1/1  Iteration 91/178 Training loss: 3.1717 0.1488 sec/batch\n",
      "Epoch 1/1  Iteration 92/178 Training loss: 3.1704 0.1477 sec/batch\n",
      "Epoch 1/1  Iteration 93/178 Training loss: 3.1692 0.1477 sec/batch\n",
      "Epoch 1/1  Iteration 94/178 Training loss: 3.1681 0.1471 sec/batch\n",
      "Epoch 1/1  Iteration 95/178 Training loss: 3.1668 0.1475 sec/batch\n",
      "Epoch 1/1  Iteration 96/178 Training loss: 3.1654 0.1472 sec/batch\n",
      "Epoch 1/1  Iteration 97/178 Training loss: 3.1642 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 98/178 Training loss: 3.1628 0.1486 sec/batch\n",
      "Epoch 1/1  Iteration 99/178 Training loss: 3.1614 0.1490 sec/batch\n",
      "Epoch 1/1  Iteration 100/178 Training loss: 3.1599 0.1488 sec/batch\n",
      "Epoch 1/1  Iteration 101/178 Training loss: 3.1586 0.1478 sec/batch\n",
      "Epoch 1/1  Iteration 102/178 Training loss: 3.1570 0.1474 sec/batch\n",
      "Epoch 1/1  Iteration 103/178 Training loss: 3.1555 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 104/178 Training loss: 3.1538 0.1471 sec/batch\n",
      "Epoch 1/1  Iteration 105/178 Training loss: 3.1520 0.1485 sec/batch\n",
      "Epoch 1/1  Iteration 106/178 Training loss: 3.1503 0.1492 sec/batch\n",
      "Epoch 1/1  Iteration 107/178 Training loss: 3.1485 0.1474 sec/batch\n",
      "Epoch 1/1  Iteration 108/178 Training loss: 3.1465 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 109/178 Training loss: 3.1447 0.1519 sec/batch\n",
      "Epoch 1/1  Iteration 110/178 Training loss: 3.1425 0.1477 sec/batch\n",
      "Epoch 1/1  Iteration 111/178 Training loss: 3.1405 0.1479 sec/batch\n",
      "Epoch 1/1  Iteration 112/178 Training loss: 3.1385 0.1485 sec/batch\n",
      "Epoch 1/1  Iteration 113/178 Training loss: 3.1363 0.1479 sec/batch\n",
      "Epoch 1/1  Iteration 114/178 Training loss: 3.1340 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 115/178 Training loss: 3.1316 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 116/178 Training loss: 3.1293 0.1475 sec/batch\n",
      "Epoch 1/1  Iteration 117/178 Training loss: 3.1272 0.1479 sec/batch\n",
      "Epoch 1/1  Iteration 118/178 Training loss: 3.1252 0.1585 sec/batch\n",
      "Epoch 1/1  Iteration 119/178 Training loss: 3.1230 0.1512 sec/batch\n",
      "Epoch 1/1  Iteration 120/178 Training loss: 3.1207 0.1492 sec/batch\n",
      "Epoch 1/1  Iteration 121/178 Training loss: 3.1186 0.1488 sec/batch\n",
      "Epoch 1/1  Iteration 122/178 Training loss: 3.1163 0.1521 sec/batch\n",
      "Epoch 1/1  Iteration 123/178 Training loss: 3.1138 0.1486 sec/batch\n",
      "Epoch 1/1  Iteration 124/178 Training loss: 3.1114 0.1475 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 125/178 Training loss: 3.1087 0.1485 sec/batch\n",
      "Epoch 1/1  Iteration 126/178 Training loss: 3.1059 0.1492 sec/batch\n",
      "Epoch 1/1  Iteration 127/178 Training loss: 3.1033 0.1477 sec/batch\n",
      "Epoch 1/1  Iteration 128/178 Training loss: 3.1007 0.1477 sec/batch\n",
      "Epoch 1/1  Iteration 129/178 Training loss: 3.0980 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 130/178 Training loss: 3.0952 0.1479 sec/batch\n",
      "Epoch 1/1  Iteration 131/178 Training loss: 3.0926 0.1489 sec/batch\n",
      "Epoch 1/1  Iteration 132/178 Training loss: 3.0895 0.1493 sec/batch\n",
      "Epoch 1/1  Iteration 133/178 Training loss: 3.0867 0.1479 sec/batch\n",
      "Epoch 1/1  Iteration 134/178 Training loss: 3.0837 0.1569 sec/batch\n",
      "Epoch 1/1  Iteration 135/178 Training loss: 3.0805 0.1576 sec/batch\n",
      "Epoch 1/1  Iteration 136/178 Training loss: 3.0772 0.1568 sec/batch\n",
      "Epoch 1/1  Iteration 137/178 Training loss: 3.0741 0.1564 sec/batch\n",
      "Epoch 1/1  Iteration 138/178 Training loss: 3.0711 0.1534 sec/batch\n",
      "Epoch 1/1  Iteration 139/178 Training loss: 3.0713 0.1572 sec/batch\n",
      "Epoch 1/1  Iteration 140/178 Training loss: 3.0685 0.1583 sec/batch\n",
      "Epoch 1/1  Iteration 141/178 Training loss: 3.0657 0.1474 sec/batch\n",
      "Epoch 1/1  Iteration 142/178 Training loss: 3.0628 0.1484 sec/batch\n",
      "Epoch 1/1  Iteration 143/178 Training loss: 3.0599 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 144/178 Training loss: 3.0569 0.1479 sec/batch\n",
      "Epoch 1/1  Iteration 145/178 Training loss: 3.0541 0.1490 sec/batch\n",
      "Epoch 1/1  Iteration 146/178 Training loss: 3.0513 0.1485 sec/batch\n",
      "Epoch 1/1  Iteration 147/178 Training loss: 3.0485 0.1477 sec/batch\n",
      "Epoch 1/1  Iteration 148/178 Training loss: 3.0457 0.1476 sec/batch\n",
      "Epoch 1/1  Iteration 149/178 Training loss: 3.0426 0.1480 sec/batch\n",
      "Epoch 1/1  Iteration 150/178 Training loss: 3.0396 0.1484 sec/batch\n",
      "Epoch 1/1  Iteration 151/178 Training loss: 3.0368 0.1492 sec/batch\n",
      "Epoch 1/1  Iteration 152/178 Training loss: 3.0341 0.1566 sec/batch\n",
      "Epoch 1/1  Iteration 153/178 Training loss: 3.0311 0.1538 sec/batch\n",
      "Epoch 1/1  Iteration 154/178 Training loss: 3.0282 0.1499 sec/batch\n",
      "Epoch 1/1  Iteration 155/178 Training loss: 3.0251 0.1559 sec/batch\n",
      "Epoch 1/1  Iteration 156/178 Training loss: 3.0220 0.1474 sec/batch\n",
      "Epoch 1/1  Iteration 157/178 Training loss: 3.0189 0.1479 sec/batch\n",
      "Epoch 1/1  Iteration 158/178 Training loss: 3.0158 0.1509 sec/batch\n",
      "Epoch 1/1  Iteration 159/178 Training loss: 3.0125 0.1474 sec/batch\n",
      "Epoch 1/1  Iteration 160/178 Training loss: 3.0095 0.1492 sec/batch\n",
      "Epoch 1/1  Iteration 161/178 Training loss: 3.0066 0.1480 sec/batch\n",
      "Epoch 1/1  Iteration 162/178 Training loss: 3.0045 0.1604 sec/batch\n",
      "Epoch 1/1  Iteration 163/178 Training loss: 3.0017 0.1592 sec/batch\n",
      "Epoch 1/1  Iteration 164/178 Training loss: 2.9989 0.1548 sec/batch\n",
      "Epoch 1/1  Iteration 165/178 Training loss: 2.9961 0.1538 sec/batch\n",
      "Epoch 1/1  Iteration 166/178 Training loss: 2.9934 0.1543 sec/batch\n",
      "Epoch 1/1  Iteration 167/178 Training loss: 2.9907 0.1546 sec/batch\n",
      "Epoch 1/1  Iteration 168/178 Training loss: 2.9879 0.1595 sec/batch\n",
      "Epoch 1/1  Iteration 169/178 Training loss: 2.9853 0.1532 sec/batch\n",
      "Epoch 1/1  Iteration 170/178 Training loss: 2.9824 0.1482 sec/batch\n",
      "Epoch 1/1  Iteration 171/178 Training loss: 2.9797 0.1566 sec/batch\n",
      "Epoch 1/1  Iteration 172/178 Training loss: 2.9771 0.1579 sec/batch\n",
      "Epoch 1/1  Iteration 173/178 Training loss: 2.9746 0.1484 sec/batch\n",
      "Epoch 1/1  Iteration 174/178 Training loss: 2.9721 0.1479 sec/batch\n",
      "Epoch 1/1  Iteration 175/178 Training loss: 2.9696 0.1478 sec/batch\n",
      "Epoch 1/1  Iteration 176/178 Training loss: 2.9668 0.1478 sec/batch\n",
      "Epoch 1/1  Iteration 177/178 Training loss: 2.9641 0.1479 sec/batch\n",
      "Epoch 1/1  Iteration 178/178 Training loss: 2.9612 0.1515 sec/batch\n",
      "Validation loss: 2.46336 Saving checkpoint!\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            summaries,batch_loss, new_state, _ = sess.run([model.summaries,model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            train_writer.add_summary(summaries,iteration)\n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summaries,batch_loss, new_state = sess.run([model.summaries,model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                test_writer.add_summary(summaries,iteration)\n",
    "                \n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter selection\n",
    "Tensorboard can be used for hyperparameter selection, to show this, i will create a train_network function that takes a model, an epoch number and a file_writer and trains(and stores tensorboard summaries) data about the model with that selected hyperparameters,then we use tensorboard to compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, file_writer):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Use the line below to load a checkpoint and resume training\n",
    "        #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "\n",
    "        n_batches = int(train_x.shape[1]/num_steps)\n",
    "        iterations = n_batches * epochs\n",
    "        for e in range(epochs):\n",
    "\n",
    "            # Train network\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0\n",
    "            for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "                iteration = e*n_batches + b\n",
    "                start = time.time()\n",
    "                feed = {model.inputs: x,\n",
    "                        model.targets: y,\n",
    "                        model.keep_prob: 0.5,\n",
    "                        model.initial_state: new_state}\n",
    "                summary, batch_loss, new_state, _ = sess.run([model.summaries, model.cost, \n",
    "                                                              model.final_state, model.optimizer], \n",
    "                                                              feed_dict=feed)\n",
    "                loss += batch_loss\n",
    "                end = time.time()\n",
    "                print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                      'Iteration {}/{}'.format(iteration, iterations),\n",
    "                      'Training loss: {:.4f}'.format(loss/b),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "                file_writer.add_summary(summary, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4229 0.0392 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.4120 0.0256 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3998 0.0267 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.3825 0.0269 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3489 0.0259 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.2688 0.0261 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.1706 0.0256 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.0814 0.0279 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.0002 0.0281 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.9280 0.0278 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.8621 0.0258 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.8055 0.0276 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.7563 0.0258 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.7144 0.0271 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.6770 0.0247 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.6438 0.0249 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.6136 0.0278 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.5887 0.0282 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.5651 0.0276 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.5419 0.0333 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.5219 0.0254 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.5037 0.0257 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.4867 0.0286 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.4711 0.0248 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.4563 0.0263 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.4434 0.0254 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.4315 0.0249 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.4195 0.0249 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.4085 0.0251 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.3985 0.0245 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.3899 0.0284 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.3808 0.0287 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.3718 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.3639 0.0258 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.3559 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.3489 0.0261 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.3413 0.0257 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.3342 0.0249 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.3274 0.0271 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.3211 0.0284 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.3148 0.0284 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.3090 0.0271 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.3033 0.0284 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.2978 0.0250 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.2924 0.0250 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.2876 0.0317 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.2830 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.2787 0.0245 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.2745 0.0270 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.2704 0.0251 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.2664 0.0337 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.2622 0.0252 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.2585 0.0282 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.2545 0.0264 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.2509 0.0266 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.2471 0.0283 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.2435 0.0339 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.2402 0.0286 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.2367 0.0351 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.2335 0.0290 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.2303 0.0267 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.2276 0.0275 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.2250 0.0261 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.2217 0.0263 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.2186 0.0273 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.2160 0.0280 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.2133 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.2099 0.0289 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.2070 0.0343 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.2045 0.0293 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2018 0.0342 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.1994 0.0290 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.1968 0.0274 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.1942 0.0283 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.1919 0.0284 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.1896 0.0284 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.1872 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.1848 0.0357 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.1823 0.0277 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.1798 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.1773 0.0309 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.1750 0.0289 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.1728 0.0353 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.1704 0.0290 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.1679 0.0345 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.1654 0.0299 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.1630 0.0305 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.1606 0.0301 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.1584 0.0363 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.1562 0.0301 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.1539 0.0373 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.1516 0.0320 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.1494 0.0324 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.1471 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.1447 0.0326 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.1423 0.0320 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.1401 0.0317 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.1377 0.0320 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.1354 0.0331 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.1331 0.0318 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.1308 0.0327 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.1285 0.0324 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.1262 0.0318 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.1238 0.0332 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.1214 0.0328 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.1191 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.1166 0.0337 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.1141 0.0371 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.1118 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.1091 0.0344 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.1066 0.0336 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.1043 0.0346 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.1018 0.0380 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.0992 0.0341 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.0966 0.0341 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.0940 0.0346 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.0915 0.0349 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.0892 0.0357 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.0868 0.0352 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.0843 0.0349 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.0821 0.0350 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.0796 0.0347 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.0772 0.0357 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.0748 0.0354 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.0723 0.0351 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.0696 0.0353 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.0671 0.0407 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.0647 0.0372 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.0621 0.0350 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.0596 0.0350 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.0572 0.0410 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.0546 0.0440 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.0521 0.0367 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.0496 0.0353 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.0467 0.0427 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.0441 0.0367 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.0414 0.0350 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.0388 0.0353 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.0364 0.0352 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.0337 0.0354 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.0313 0.0387 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.0286 0.0380 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.0260 0.0359 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.0233 0.0355 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.0208 0.0366 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.0183 0.0411 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.0158 0.0420 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.0134 0.0365 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.0107 0.0359 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.0081 0.0360 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.0058 0.0358 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.0035 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.0010 0.0416 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 2.9986 0.0358 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 2.9960 0.0381 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 2.9935 0.0359 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 2.9909 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 2.9883 0.0359 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 2.9856 0.0414 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 2.9831 0.0415 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 2.9807 0.0373 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 2.9779 0.0363 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 2.9753 0.0377 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 2.9728 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 2.9703 0.0371 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 2.9678 0.0364 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 2.9654 0.0393 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 2.9629 0.0369 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 2.9606 0.0369 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 2.9580 0.0365 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 2.9556 0.0392 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 2.9534 0.0375 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 2.9512 0.0418 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 2.9491 0.0442 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 2.9469 0.0373 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 2.9445 0.0366 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 2.9421 0.0372 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 2.9396 0.0393 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.5768 0.0420 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.5287 0.0370 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.5167 0.0370 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.5126 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.5094 0.0375 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.5063 0.0378 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.5056 0.0367 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.5056 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.5060 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.5045 0.0372 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.5015 0.0375 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.5005 0.0366 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.4990 0.0394 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.4997 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.4987 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.4978 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.4969 0.0367 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.4975 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.4965 0.0365 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.4942 0.0369 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.4926 0.0371 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.4925 0.0400 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.4910 0.0372 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.4895 0.0406 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.4879 0.0374 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.4869 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.4857 0.0370 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.4843 0.0397 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.4835 0.0372 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.4825 0.0372 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.4821 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.4806 0.0375 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.4789 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.4778 0.0377 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.4764 0.0377 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.4755 0.0371 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.4741 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.4721 0.0375 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.4707 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.4691 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.4675 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.4660 0.0397 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.4644 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.4629 0.0394 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.4615 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.4596 0.0375 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.4587 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.4575 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.4563 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.4557 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.4544 0.0374 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.4533 0.0426 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.4520 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.4507 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.4494 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.4484 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.4473 0.0377 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.4460 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.4449 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.4441 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.4429 0.0415 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.4421 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.4413 0.0370 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.4403 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.4390 0.0428 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.4383 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.4373 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.4359 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.4347 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.4339 0.0448 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.4329 0.0428 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.4321 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.4312 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.4302 0.0397 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.4292 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.4288 0.0410 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4278 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4271 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4260 0.0391 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4251 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4241 0.0438 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.4233 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.4223 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.4213 0.0376 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.4199 0.0429 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.4189 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.4180 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.4171 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.4161 0.0376 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.4153 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.4144 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.4136 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.4127 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.4117 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.4106 0.0374 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.4096 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.4088 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.4078 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.4070 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.4060 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.4054 0.0378 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.4045 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.4035 0.0376 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.4026 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.4017 0.0456 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.4009 0.0434 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.4000 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.3994 0.0403 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.3988 0.0400 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.3977 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.3970 0.0439 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.3963 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.3955 0.0378 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.3946 0.0434 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.3938 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.3928 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.3920 0.0377 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.3913 0.0378 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.3908 0.0428 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.3901 0.0429 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.3895 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.3888 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.3880 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.3874 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.3867 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.3858 0.0400 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.3851 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.3846 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.3838 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.3832 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.3825 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.3816 0.0438 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.3810 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.3804 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.3795 0.0454 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.3789 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.3782 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.3775 0.0400 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.3771 0.0435 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.3764 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.3759 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.3752 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.3745 0.0438 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.3738 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.3731 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.3727 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.3721 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.3716 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.3709 0.0399 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.3701 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.3697 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.3694 0.0434 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.3688 0.0406 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.3683 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.3677 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.3670 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.3663 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.3656 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.3648 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.3644 0.0458 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.3638 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.3630 0.0391 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.3623 0.0409 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.3616 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.3611 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.3605 0.0395 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.3600 0.0438 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.3594 0.0460 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.3588 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.3582 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.3576 0.0434 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.3571 0.0410 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.3568 0.0420 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.3565 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.3561 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.3556 0.0454 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.3548 0.0404 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.3542 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.3126 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.2590 0.0382 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.2463 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.2432 0.0426 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.2407 0.0436 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.2386 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.2391 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.2402 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.2411 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.2409 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.2391 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.2383 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.2383 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.2401 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.2396 0.0387 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.2390 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.2387 0.0405 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.2402 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.2400 0.0420 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.2388 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.2380 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.2394 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.2386 0.0438 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.2375 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.2368 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.2361 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.2353 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.2349 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.2352 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.2351 0.0387 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.2352 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.2343 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.2334 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.2332 0.0385 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.2326 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.2321 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.2315 0.0406 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.2301 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.2293 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.2283 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.2273 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.2265 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.2256 0.0438 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.2246 0.0405 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.2238 0.0410 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.2223 0.0445 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.2220 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.2211 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.2205 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.2205 0.0429 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.2196 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.2193 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.2186 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.2177 0.0418 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.2170 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.2167 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.2162 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.2155 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.2149 0.0463 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.2148 0.0445 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.2142 0.0388 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.2139 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.2137 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.2132 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.2125 0.0443 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.2123 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.2119 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.2111 0.0411 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.2105 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.2101 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.2097 0.0446 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.2095 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.2092 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.2086 0.0454 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.2080 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.2082 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.2075 0.0406 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.2074 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.2066 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.2061 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.2053 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.2050 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.2043 0.0412 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.2037 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.2027 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.2020 0.0385 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.2015 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.2009 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.2003 0.0411 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.2000 0.0441 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.1994 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.1990 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.1984 0.0446 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.1978 0.0441 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.1970 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.1965 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.1960 0.0438 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.1955 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.1950 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.1944 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.1941 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.1937 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.1930 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.1925 0.0447 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.1919 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.1915 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.1910 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.1908 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.1905 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.1899 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.1895 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.1892 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.1887 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.1882 0.0419 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.1878 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.1871 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.1866 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.1863 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.1861 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.1857 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.1855 0.1087 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.1851 0.0469 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.1846 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.1843 0.0442 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.1839 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.1834 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.1830 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.1828 0.0421 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.1824 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.1822 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.1818 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.1812 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.1808 0.0439 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.1806 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.1801 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.1798 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.1795 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.1792 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.1791 0.0429 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.1787 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.1785 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.1781 0.0413 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.1778 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.1774 0.0413 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.1770 0.0410 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.1769 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.1766 0.0413 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.1764 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.1760 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.1755 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.1753 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.1753 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.1751 0.0448 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.1749 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.1745 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.1742 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.1737 0.0442 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.1733 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.1728 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.1728 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.1725 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.1720 0.0416 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.1716 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.1713 0.0433 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.1710 0.0432 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.1707 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.1705 0.0444 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.1702 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.1700 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.1696 0.0424 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.1693 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.1691 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.1691 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.1691 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.1690 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.1688 0.0424 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.1684 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.1680 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.1798 0.0388 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.1281 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.1143 0.0390 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.1093 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.1066 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.1038 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.1041 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.1053 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.1064 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.1061 0.0425 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.1042 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.1030 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.1033 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.1054 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.1049 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.1039 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.1036 0.0445 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.1054 0.0463 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.1053 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.1044 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.1036 0.0421 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.1057 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.1049 0.0417 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.1037 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.1032 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.1024 0.0420 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.1015 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.1014 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.1021 0.0447 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.1021 0.0440 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.1022 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.1014 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.1007 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.1010 0.0431 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.1005 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.1001 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.0996 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.0984 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.0976 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.0967 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.0959 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.0956 0.0423 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.0949 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.0942 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.0935 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.0921 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.0920 0.0449 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.0913 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.0909 0.0418 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.0912 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.0904 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.0905 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.0900 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.0894 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.0888 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.0888 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.0886 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.0880 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.0875 0.0422 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.0878 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.0873 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.0874 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.0873 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.0871 0.0425 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.0866 0.0413 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.0866 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.0863 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.0856 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.0853 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.0850 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.0849 0.0420 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.0849 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.0848 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.0843 0.0465 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.0839 0.0443 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.0842 0.0436 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.0838 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.0838 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.0832 0.0443 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.0828 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.0822 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.0820 0.0419 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.0815 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.0810 0.0450 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.0801 0.0387 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.0796 0.0445 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.0792 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.0787 0.0436 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.0782 0.0450 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.0781 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.0777 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.0774 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 2.0769 0.0449 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 2.0764 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 2.0759 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 2.0754 0.0414 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 2.0752 0.0426 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 2.0747 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 2.0742 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 2.0738 0.0443 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 2.0736 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 2.0734 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 2.0729 0.0449 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 2.0725 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 2.0721 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 2.0719 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 2.0715 0.0419 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 2.0714 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 2.0713 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 2.0709 0.0431 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 2.0706 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 2.0704 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 2.0701 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 2.0697 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 2.0694 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 2.0688 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 2.0685 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 2.0683 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 2.0682 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 2.0680 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 2.0679 0.0425 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 2.0676 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 2.0673 0.0420 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 2.0672 0.0424 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 2.0669 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 2.0665 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 2.0663 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 2.0662 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 2.0660 0.0433 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 2.0658 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 2.0655 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 2.0651 0.0423 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 2.0649 0.0420 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 2.0648 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 2.0645 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 2.0644 0.0425 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 2.0642 0.0446 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 2.0641 0.0435 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 2.0641 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 2.0638 0.0453 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 2.0638 0.0418 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 2.0635 0.0462 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 2.0634 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 2.0632 0.0452 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 2.0629 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 2.0629 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 2.0628 0.0419 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 2.0627 0.0420 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 2.0625 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 2.0621 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 2.0621 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 2.0622 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 2.0621 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 2.0620 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 2.0618 0.0445 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 2.0615 0.0414 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 2.0613 0.0433 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 2.0610 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 2.0607 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 2.0607 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 2.0606 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 2.0603 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 2.0601 0.0430 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 2.0598 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 2.0597 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 2.0595 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 2.0594 0.0419 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 2.0593 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 2.0592 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 2.0589 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 2.0588 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 2.0587 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 2.0589 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 2.0590 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 2.0591 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 2.0589 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 2.0586 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 2.0584 0.0489 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.0934 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 2.0428 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 2.0300 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 2.0250 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 2.0222 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 2.0185 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 2.0183 0.0423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 720/3560 Training loss: 2.0192 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 2.0206 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 2.0203 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 2.0183 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 2.0168 0.0413 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 2.0171 0.0443 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 2.0194 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 2.0190 0.0448 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 2.0178 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 2.0175 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 2.0195 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 2.0192 0.0447 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 2.0186 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 2.0179 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 2.0202 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 2.0193 0.0425 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 2.0184 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 2.0180 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 2.0173 0.0453 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 2.0164 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 2.0163 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 2.0173 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 2.0173 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 2.0173 0.0446 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 2.0165 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 2.0159 0.0446 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 2.0164 0.0450 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 2.0160 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 2.0158 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 2.0153 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 2.0142 0.0422 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 2.0133 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 2.0125 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 2.0117 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 2.0115 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 2.0109 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 2.0102 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 2.0097 0.0421 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 2.0083 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 2.0083 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 2.0077 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 2.0073 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 2.0079 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 2.0071 0.0446 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 2.0074 0.0448 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 2.0070 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 2.0065 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 2.0061 0.0420 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 2.0062 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 2.0061 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 2.0056 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 2.0052 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 2.0056 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 2.0053 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 2.0055 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 2.0055 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 2.0054 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 2.0050 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 2.0051 0.0422 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 2.0050 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 2.0044 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 2.0042 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 2.0040 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 2.0039 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 2.0040 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 2.0041 0.0450 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 2.0036 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 2.0034 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 2.0037 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 2.0034 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 2.0035 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 2.0030 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 2.0027 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 2.0020 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 2.0020 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 2.0014 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 2.0011 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 2.0003 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.9998 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.9995 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.9990 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.9986 0.0423 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.9985 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.9982 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.9980 0.0428 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.9976 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.9971 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.9967 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.9963 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.9961 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.9957 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.9953 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.9949 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.9947 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.9946 0.0446 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.9942 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.9939 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.9935 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.9934 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.9931 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.9930 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.9929 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.9927 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.9924 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.9922 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.9919 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.9917 0.0423 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.9914 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.9908 0.0446 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.9906 0.0423 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.9904 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.9903 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.9902 0.0433 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.9901 0.0426 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.9899 0.0423 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.9896 0.0393 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.9895 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.9893 0.0427 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.9889 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.9888 0.0404 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.9887 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.9886 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.9885 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.9883 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.9879 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.9878 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.9877 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.9875 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.9874 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.9873 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.9872 0.0432 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.9873 0.0422 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.9872 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.9872 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.9870 0.0421 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.9869 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.9868 0.0444 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.9866 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.9867 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.9866 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.9866 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.9864 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.9861 0.0426 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.9861 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.9863 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.9863 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.9862 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.9860 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.9859 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.9857 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.9855 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.9852 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.9854 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.9853 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.9850 0.0451 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.9849 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.9847 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.9847 0.0445 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.9844 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.9844 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.9844 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.9843 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.9841 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.9840 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.9840 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.9842 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.9843 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.9844 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.9843 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.9841 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.9840 0.0451 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 2.0269 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.9786 0.0432 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.9665 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.9619 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.9588 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.9544 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.9539 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.9547 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.9563 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.9559 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.9536 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.9519 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.9520 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.9545 0.0445 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.9542 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.9529 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.9527 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.9547 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.9543 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.9540 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.9533 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.9557 0.0451 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.9548 0.0397 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.9539 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.9536 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.9528 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.9519 0.0431 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.9520 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.9530 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.9532 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.9531 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.9522 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.9517 0.0419 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.9523 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.9519 0.0452 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.9517 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.9512 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.9502 0.0422 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.9492 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.9484 0.0428 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.9476 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.9476 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.9469 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.9462 0.0426 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.9458 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.9445 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.9445 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.9439 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.9436 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.9443 0.0426 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.9436 0.0392 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.9440 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.9436 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.9432 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.9429 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.9430 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.9430 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.9425 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.9421 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.9426 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.9423 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.9427 0.0422 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.9428 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.9428 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.9424 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.9426 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.9426 0.0448 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.9421 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.9419 0.0397 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.9418 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.9418 0.0423 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.9419 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.9421 0.0468 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.9416 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.9414 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.9418 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.9416 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.9417 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.9412 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.9409 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.9402 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.9402 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.9396 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.9394 0.0453 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.9387 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.9382 0.0417 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.9379 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.9376 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.9371 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.9371 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.9368 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.9366 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.9362 0.0467 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.9358 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.9354 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.9352 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.9350 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.9345 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.9342 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.9337 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.9337 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.9336 0.0461 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.9332 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.9329 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.9326 0.0417 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.9325 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.9322 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.9322 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.9322 0.0397 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.9320 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.9318 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.9315 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.9313 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.9310 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.9307 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.9302 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.9300 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.9299 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.9298 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.9296 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.9296 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.9293 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.9290 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.9290 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.9289 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.9285 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.9284 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.9284 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.9283 0.0429 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.9283 0.0470 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.9280 0.0397 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.9277 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.9276 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.9275 0.0427 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.9274 0.0458 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.9273 0.0446 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.9273 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.9272 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.9274 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.9272 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.9273 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.9271 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.9271 0.0422 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.9270 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.9268 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.9269 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.9268 0.0419 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.9269 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.9267 0.0422 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.9265 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.9265 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.9267 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.9266 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.9266 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.9265 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.9263 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.9262 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.9261 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.9258 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.9260 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.9259 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.9257 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.9257 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.9255 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.9255 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.9253 0.0397 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.9253 0.0448 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.9254 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.9253 0.0450 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.9251 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.9250 0.0424 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.9251 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.9253 0.0395 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.9254 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.9256 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.9255 0.0447 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.9252 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.9252 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.9739 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.9268 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.9151 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.9098 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.9066 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.9012 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.9011 0.0430 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.9015 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.9033 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.9028 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.9007 0.0410 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.8987 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.8990 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.9014 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.9013 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.8998 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.8997 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.9017 0.0450 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.9013 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.9013 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.9006 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.9030 0.0447 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.9021 0.0432 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.9014 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.9010 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.9002 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.8992 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.8994 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.9005 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.9007 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.9006 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.8996 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.8992 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.8997 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.8993 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.8991 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.8986 0.0450 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.8975 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.8965 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.8957 0.0448 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.8949 0.0408 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.8949 0.0423 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.8943 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.8936 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.8933 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.8921 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.8920 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.8914 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.8911 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.8918 0.0392 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.8911 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.8916 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.8912 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.8910 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.8907 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.8909 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.8909 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.8904 0.0430 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.8900 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.8905 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.8903 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.8908 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.8910 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.8910 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.8907 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.8910 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.8911 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.8905 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.8904 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.8903 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.8904 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.8906 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.8907 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.8903 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.8901 0.0446 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.8906 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.8903 0.0436 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.8905 0.0453 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.8900 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.8898 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.8891 0.0443 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.8891 0.0446 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.8885 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.8883 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.8877 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.8873 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.8870 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.8867 0.0446 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.8862 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.8862 0.0463 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.8859 0.0446 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.8858 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.8854 0.0427 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.8850 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.8846 0.0391 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.8844 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.8843 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.8838 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.8835 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.8831 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.8830 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.8830 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.8826 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.8824 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.8821 0.0470 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.8820 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.8818 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.8818 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.8817 0.0457 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.8816 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.8814 0.0408 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.8812 0.0434 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.8809 0.0421 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.8807 0.0470 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.8804 0.0444 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.8800 0.0606 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.8798 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.8796 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.8795 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.8794 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.8794 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.8791 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.8789 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.8789 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.8788 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.8784 0.0431 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.8784 0.0469 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.8784 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.8783 0.0440 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.8783 0.0447 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.8781 0.0402 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.8777 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.8777 0.0451 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.8776 0.0423 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.8775 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.8775 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.8775 0.0426 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.8775 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.8777 0.0469 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.8775 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.8777 0.0419 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.8775 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.8775 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.8775 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.8773 0.0426 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.8774 0.0449 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.8774 0.0448 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.8775 0.0435 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.8774 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.8772 0.0438 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.8772 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.8774 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.8774 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.8773 0.0429 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.8772 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.8771 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.8771 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.8769 0.0408 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.8767 0.0434 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.8769 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.8769 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.8767 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.8767 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.8765 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.8765 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.8764 0.0443 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.8763 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.8765 0.0408 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.8764 0.0471 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.8763 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.8762 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.8762 0.0456 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.8765 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.8766 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.8768 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.8767 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.8765 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.8765 0.0448 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.9297 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.8852 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.8736 0.0394 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.8678 0.0448 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.8647 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.8586 0.0445 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.8590 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.8589 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.8612 0.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.8605 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.8586 0.0422 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.8564 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.8565 0.0405 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.8591 0.0463 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.8586 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.8572 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.8571 0.0392 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.8591 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.8588 0.0419 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.8588 0.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.8582 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.8603 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.8594 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.8587 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.8583 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.8574 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.8564 0.0418 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.8567 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.8578 0.0424 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.8580 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.8578 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.8569 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.8565 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.8570 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.8567 0.0431 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.8565 0.0430 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.8560 0.0401 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.8549 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.8538 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.8531 0.0446 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.8524 0.0405 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.8524 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.8517 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.8511 0.0445 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.8508 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.8496 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.8495 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.8488 0.0418 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.8486 0.0426 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.8493 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.8486 0.0392 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.8492 0.0448 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.8488 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.8486 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.8483 0.0453 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.8484 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.8485 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.8481 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.8476 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.8481 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.8480 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.8485 0.0425 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.8487 0.0391 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.8487 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.8484 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.8488 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.8489 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.8484 0.0394 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.8482 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.8482 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.8483 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.8484 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.8486 0.0400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.8482 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.8481 0.0414 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.8485 0.0426 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.8483 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.8484 0.0452 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.8480 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.8477 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.8470 0.0394 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.8470 0.0442 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.8465 0.0410 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.8463 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.8457 0.0446 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.8453 0.0401 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.8450 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.8446 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.8442 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.8442 0.0424 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.8439 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.8438 0.0405 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.8434 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.8430 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.8426 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.8425 0.0472 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.8423 0.0452 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.8418 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.8415 0.0423 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.8410 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.8410 0.0445 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.8409 0.0423 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.8406 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.8403 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.8400 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.8400 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.8397 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.8397 0.0423 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.8396 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.8395 0.0405 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.8394 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.8391 0.0442 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.8389 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.8387 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.8384 0.0424 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.8380 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.8377 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.8376 0.0444 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.8375 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.8374 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.8374 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.8371 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.8369 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.8369 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.8368 0.0410 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.8364 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.8364 0.0456 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.8364 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.8363 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.8364 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.8361 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.8358 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.8358 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.8357 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.8356 0.0423 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.8356 0.0449 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.8356 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.8356 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.8358 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.8357 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.8359 0.0446 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.8357 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.8357 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.8357 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.8356 0.0409 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.8357 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.8357 0.0394 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.8358 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.8357 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.8356 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.8355 0.0426 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.8358 0.0419 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.8357 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.8357 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.8357 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.8356 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.8356 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.8355 0.0447 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.8352 0.0423 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.8355 0.0401 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.8355 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.8353 0.0445 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.8353 0.0405 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.8352 0.0417 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.8352 0.0401 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.8350 0.0401 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.8350 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.8352 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.8351 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.8350 0.0419 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.8349 0.0390 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.8350 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.8352 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.8354 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.8355 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.8355 0.0423 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.8353 0.0425 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.8353 0.0391 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.8924 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.8491 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.8385 0.0443 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.8307 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.8281 0.0419 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.8210 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.8217 0.0471 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.8213 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.8234 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.8228 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.8204 0.0408 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.8183 0.0405 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.8180 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.8206 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.8200 0.0425 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.8184 0.0425 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.8185 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.8203 0.0444 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.8202 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.8201 0.0405 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.8195 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.8215 0.0420 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.8205 0.0448 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.8199 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.8195 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.8187 0.0447 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.8176 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.8179 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.8191 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.8193 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.8191 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.8182 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.8179 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.8184 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.8180 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.8179 0.0405 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.8174 0.0405 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.8162 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.8152 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.8144 0.0451 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.8137 0.0413 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.8137 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.8131 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.8123 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.8121 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.8109 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.8108 0.0457 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.8102 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.8099 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.8107 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.8100 0.0433 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.8107 0.0394 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.8103 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.8102 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.8099 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.8100 0.0447 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.8102 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.8097 0.0444 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.8092 0.0421 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.8098 0.0448 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.8097 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.8103 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.8105 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.8106 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.8104 0.0452 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.8108 0.0452 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.8109 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.8104 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.8103 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.8102 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.8104 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.8106 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.8108 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.8104 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.8103 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.8107 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.8105 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.8107 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.8102 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.8100 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.8093 0.0451 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.8093 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.8088 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.8087 0.0442 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.8081 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.8077 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.8074 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.8071 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.8066 0.0420 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.8067 0.0444 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.8064 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.8063 0.0460 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.8059 0.0419 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.8055 0.0447 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.8051 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.8050 0.0393 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.8048 0.0448 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.8044 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.8040 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.8036 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.8035 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.8035 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.8031 0.0443 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.8029 0.0471 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.8026 0.0405 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.8026 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.8023 0.0468 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.8023 0.0448 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.8022 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.8021 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.8020 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.8018 0.0426 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.8015 0.0441 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.8014 0.0468 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.8011 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.8006 0.0457 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.8004 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.8003 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.8002 0.0421 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.8001 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.8001 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.7998 0.0408 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.7996 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.7997 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.7995 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.7992 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.7992 0.0456 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.7993 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.7992 0.0425 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.7992 0.0449 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.7990 0.0405 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.7986 0.0456 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.7986 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.7986 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.7985 0.0405 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.7985 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.7985 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.7986 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.7987 0.0393 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.7986 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.7989 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.7987 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.7987 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.7988 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.7986 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.7988 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.7988 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.7989 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.7989 0.0448 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.7988 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.7987 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.7989 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.7989 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.7989 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.7989 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.7988 0.0425 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.7988 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.7988 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.7985 0.0405 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.7988 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.7988 0.0408 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.7986 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.7986 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.7985 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.7985 0.0394 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.7984 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.7984 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.7986 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.7985 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.7984 0.0452 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.7983 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.7984 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.7987 0.0392 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.7988 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.7990 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.7989 0.0424 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.7988 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.7988 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.8590 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.8156 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.8048 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.7967 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.7932 0.0394 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.7860 0.0427 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.7864 0.0468 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.7861 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.7882 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.7877 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.7852 0.0458 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.7831 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.7828 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.7852 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.7847 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.7832 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.7832 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.7851 0.0414 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.7850 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.7851 0.0420 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.7845 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.7864 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.7854 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.7848 0.0434 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.7844 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.7836 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.7825 0.0450 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.7829 0.0452 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.7840 0.0448 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.7843 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.7840 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.7832 0.0452 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.7829 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.7834 0.0461 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.7831 0.0423 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.7829 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.7825 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.7814 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.7802 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.7795 0.0426 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.7788 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.7789 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.7783 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.7775 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.7774 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.7762 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.7761 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.7755 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.7752 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.7760 0.0427 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.7754 0.0442 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.7761 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.7758 0.0421 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.7757 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.7754 0.0419 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.7756 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.7758 0.0471 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.7753 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.7749 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.7755 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.7754 0.0423 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.7761 0.0450 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.7763 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.7764 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.7762 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.7766 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.7768 0.0471 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.7763 0.0454 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.7763 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.7762 0.0448 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.7765 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.7766 0.0400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.7769 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.7765 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.7764 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.7768 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.7766 0.0397 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.7767 0.0428 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.7763 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.7761 0.0443 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.7755 0.0393 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.7755 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.7750 0.0459 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.7749 0.0426 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.7744 0.0469 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.7740 0.0442 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.7737 0.0436 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.7734 0.0439 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.7729 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.7730 0.0489 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.7728 0.0415 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.7727 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.7723 0.0420 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.7720 0.0459 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.7716 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.7715 0.0440 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.7713 0.0425 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.7709 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.7705 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.7701 0.0409 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.7700 0.0417 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.7700 0.0426 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.7696 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.7694 0.0433 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.7691 0.0427 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.7691 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.7689 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.7689 0.0394 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.7688 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.7687 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.7686 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.7684 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.7681 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.7680 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.7677 0.0420 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.7673 0.0451 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.7671 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.7670 0.0429 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.7669 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.7667 0.0397 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.7668 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.7665 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.7663 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.7663 0.0430 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.7662 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.7659 0.0393 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.7660 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.7660 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.7659 0.0454 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.7659 0.0438 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.7657 0.0449 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.7654 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.7654 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.7653 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.7653 0.0450 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.7653 0.0428 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.7653 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.7654 0.0421 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.7656 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.7654 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.7657 0.0409 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.7656 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.7656 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.7656 0.0430 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.7655 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.7657 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.7657 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.7659 0.0397 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.7659 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.7658 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.7657 0.0446 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.7659 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.7659 0.0419 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.7659 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.7659 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.7658 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.7658 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.7658 0.0427 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.7656 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.7658 0.0453 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.7658 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.7657 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.7657 0.0422 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.7656 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.7656 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.7655 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.7655 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.7658 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.7657 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.7656 0.0446 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.7655 0.0450 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.7655 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.7658 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.7660 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.7661 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.7661 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.7659 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.7660 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.8252 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.7849 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.7736 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.7658 0.0395 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.7620 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.7546 0.0442 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.7550 0.0469 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.7546 0.0423 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.7570 0.0400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.7564 0.0409 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.7538 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.7518 0.0427 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.7515 0.0396 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.7539 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.7534 0.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.7519 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.7520 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.7539 0.0471 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.7539 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.7541 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.7535 0.0427 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.7554 0.0449 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.7544 0.0456 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.7538 0.0425 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.7533 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.7525 0.0430 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.7514 0.0452 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.7519 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.7529 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.7532 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.7530 0.0473 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.7521 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.7519 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.7524 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.7521 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.7520 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.7515 0.0448 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.7504 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.7492 0.0396 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.7485 0.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.7478 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.7480 0.0433 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.7473 0.0452 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.7466 0.0452 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.7465 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.7453 0.0409 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.7452 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.7446 0.0444 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.7443 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.7451 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.7446 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.7453 0.0509 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.7451 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.7450 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.7448 0.0416 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.7449 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.7451 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.7447 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.7443 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.7450 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.7449 0.0425 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.7456 0.0472 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.7459 0.0393 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.7460 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.7458 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.7463 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.7464 0.0474 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.7460 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.7459 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.7459 0.0418 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.7462 0.0454 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.7463 0.0433 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.7466 0.0426 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.7462 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.7461 0.0394 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.7465 0.0418 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.7463 0.0453 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.7465 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.7461 0.0419 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.7459 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.7453 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.7453 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.7449 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.7448 0.0472 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.7443 0.0416 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.7439 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.7436 0.0396 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.7433 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.7429 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.7430 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.7428 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.7427 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.7423 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.7420 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.7416 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.7416 0.0425 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.7414 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.7409 0.0477 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.7406 0.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.7402 0.0450 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.7401 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.7401 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.7398 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.7396 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.7393 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.7392 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.7391 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.7390 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.7390 0.0447 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.7389 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.7388 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.7386 0.0429 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.7384 0.0439 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.7382 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.7380 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.7375 0.0426 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.7374 0.0394 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.7373 0.0472 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.7372 0.0431 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.7370 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.7371 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.7368 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.7365 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.7366 0.0420 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.7365 0.0442 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.7362 0.0448 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.7363 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.7363 0.0396 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.7362 0.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.7362 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.7360 0.0436 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.7357 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.7357 0.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.7357 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.7356 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.7357 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.7357 0.0424 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.7358 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.7360 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.7359 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.7361 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.7360 0.0433 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.7360 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.7361 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.7360 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.7362 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.7362 0.0446 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.7364 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.7364 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.7363 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.7362 0.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.7364 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.7364 0.0433 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.7364 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.7364 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.7363 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.7364 0.0395 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.7364 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.7361 0.0442 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.7364 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.7364 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.7363 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.7363 0.0431 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.7362 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.7363 0.0409 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.7362 0.0433 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.7362 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.7365 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.7364 0.0477 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.7363 0.0576 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.7362 0.0504 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.7363 0.0427 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.7365 0.0430 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.7367 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.7369 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.7368 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.7367 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.7367 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.7961 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.7578 0.0424 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.7465 0.0424 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.7390 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.7349 0.0423 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.7274 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.7279 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.7274 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.7300 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.7293 0.0451 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.7264 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.7246 0.0456 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.7243 0.0450 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.7266 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.7259 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.7246 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.7247 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.7265 0.0430 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.7267 0.0421 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.7269 0.0432 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.7263 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.7281 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.7271 0.0426 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.7265 0.0427 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.7260 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.7251 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.7241 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.7245 0.0423 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.7255 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.7259 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.7256 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.7247 0.0437 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.7247 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.7251 0.0454 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.7248 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.7248 0.0426 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.7242 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.7232 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.7219 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.7212 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.7205 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.7207 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.7201 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.7193 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.7192 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.7181 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.7180 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.7174 0.0429 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.7172 0.0423 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.7179 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.7174 0.0452 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.7182 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.7180 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.7180 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.7178 0.0452 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.7179 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.7181 0.0474 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.7177 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.7172 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.7179 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.7179 0.0450 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.7186 0.0427 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.7189 0.0441 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.7190 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.7189 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.7193 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.7195 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.7190 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.7190 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.7190 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.7193 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.7194 0.0427 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.7197 0.0428 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.7194 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.7193 0.0421 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.7196 0.0424 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.7195 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.7196 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.7192 0.0454 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.7191 0.0419 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.7185 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.7185 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.7181 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.7180 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.7175 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.7172 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.7169 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.7166 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.7162 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.7164 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.7161 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.7160 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.7156 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.7154 0.0448 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.7150 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.7149 0.0473 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.7148 0.0424 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.7143 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.7140 0.0473 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.7136 0.0451 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.7135 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.7135 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.7132 0.0450 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.7130 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.7127 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.7126 0.0420 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.7125 0.0452 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.7125 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.7124 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.7123 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.7122 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.7120 0.0424 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.7118 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.7117 0.0412 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.7114 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.7110 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.7108 0.0433 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.7107 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.7106 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.7105 0.0428 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.7106 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.7103 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.7100 0.0474 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.7101 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.7100 0.0425 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.7097 0.0433 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.7098 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.7098 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.7098 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.7098 0.0457 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.7095 0.0447 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.7092 0.0428 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.7092 0.0453 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.7092 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.7092 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.7092 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.7092 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.7093 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.7095 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.7094 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.7097 0.0447 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.7096 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.7096 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.7097 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.7096 0.0417 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.7098 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.7099 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.7100 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.7100 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.7100 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.7098 0.0431 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.7100 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.7100 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.7100 0.0422 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.7100 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.7100 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.7100 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.7100 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.7098 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.7100 0.0393 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.7101 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.7100 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.7100 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.7099 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.7100 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.7099 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.7099 0.0421 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.7102 0.0412 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.7101 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.7101 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.7100 0.0396 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.7100 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.7103 0.0428 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.7104 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.7105 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.7105 0.0400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.7104 0.0422 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.7104 0.0464 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.7700 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.7326 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.7209 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.7140 0.0457 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.7096 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.7019 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.7026 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.7020 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.7046 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.7040 0.0452 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.7010 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.6992 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.6989 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.7013 0.0473 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.7005 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.6992 0.0405 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.6994 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.7011 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.7015 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.7018 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.7012 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.7029 0.0471 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.7019 0.0405 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.7013 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.7008 0.0431 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.7000 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.6989 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.6994 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.7005 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.7009 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.7005 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.6997 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.6998 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.7003 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.7000 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.6999 0.0450 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.6993 0.0428 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.6983 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.6970 0.0459 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.6963 0.0455 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.6956 0.0408 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.6959 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.6952 0.0444 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.6944 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.6944 0.0428 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.6934 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.6933 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.6927 0.0433 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.6924 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.6932 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.6927 0.0451 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.6935 0.0475 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.6933 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.6934 0.0408 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.6932 0.0431 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.6933 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.6935 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.6931 0.0427 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.6927 0.0473 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.6933 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.6933 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.6940 0.0451 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.6944 0.0400 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.6945 0.0421 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.6944 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.6948 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.6950 0.0456 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.6946 0.0451 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.6946 0.0454 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.6945 0.0425 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.6949 0.0426 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.6950 0.0453 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.6953 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.6950 0.0427 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.6949 0.0491 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.6952 0.0400 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.6951 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.6952 0.0420 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.6948 0.0405 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.6947 0.0453 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.6941 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.6941 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.6937 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.6937 0.0427 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.6932 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.6929 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.6926 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.6923 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.6919 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.6921 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.6918 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.6917 0.0405 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.6914 0.0400 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.6911 0.0408 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.6907 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.6907 0.0428 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.6905 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.6901 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.6898 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.6893 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.6893 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.6893 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.6890 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.6888 0.0433 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.6885 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.6884 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.6883 0.0428 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.6883 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.6882 0.0431 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.6882 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.6881 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.6878 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.6877 0.0410 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.6875 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.6873 0.0441 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.6869 0.0423 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.6867 0.0429 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.6866 0.0422 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.6865 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.6864 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.6864 0.0428 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.6861 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.6859 0.0451 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.6860 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.6859 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.6855 0.0444 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.6857 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.6857 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.6857 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.6856 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.6853 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.6851 0.0432 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.6851 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.6851 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.6851 0.0459 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.6851 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.6851 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.6852 0.0469 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.6854 0.0422 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.6853 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.6856 0.0458 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.6855 0.0457 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.6855 0.0420 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.6856 0.0454 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.6855 0.0433 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.6857 0.0459 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.6858 0.0408 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.6860 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.6860 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.6859 0.0451 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.6857 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.6859 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.6859 0.0423 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.6860 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.6860 0.0464 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.6859 0.0427 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.6860 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.6860 0.0422 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.6858 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.6859 0.0431 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.6860 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.6859 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.6860 0.0429 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.6859 0.0436 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.6859 0.0428 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.6858 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.6859 0.0461 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.6862 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.6862 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.6861 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.6860 0.0420 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.6860 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.6862 0.0472 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.6863 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.6864 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.6864 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.6863 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.6863 0.0461 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.7463 0.0426 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.7104 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.6979 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.6914 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.6867 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.6788 0.0432 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.6794 0.0448 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.6787 0.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.6814 0.0433 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.6807 0.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.6775 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.6759 0.0454 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.6757 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.6782 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.6773 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.6761 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.6763 0.0460 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.6779 0.0431 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.6784 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.6789 0.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.6782 0.0432 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.6798 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.6788 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.6783 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.6778 0.0465 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.6769 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.6759 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.6765 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.6775 0.0435 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.6779 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.6776 0.0425 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.6768 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.6769 0.0428 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.6774 0.0431 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.6772 0.0458 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.6771 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.6764 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.6754 0.0402 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.6741 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.6734 0.0452 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.6728 0.0440 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.6731 0.0470 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.6724 0.0422 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.6716 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.6716 0.0431 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.6706 0.0427 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.6705 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.6700 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.6697 0.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.6705 0.0424 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.6700 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.6708 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.6706 0.0426 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.6707 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.6705 0.0424 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.6706 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.6709 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.6705 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.6700 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.6707 0.0438 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.6707 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.6714 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.6717 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.6719 0.0453 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.6718 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.6722 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.6724 0.0446 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.6720 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.6720 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.6720 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.6724 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.6725 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.6728 0.0458 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.6725 0.0404 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.6723 0.0424 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.6727 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.6726 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.6727 0.0474 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.6723 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.6722 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.6716 0.0434 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.6717 0.0455 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.6712 0.0453 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.6712 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.6707 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.6705 0.0438 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.6702 0.0441 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.6699 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.6695 0.0453 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.6697 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.6694 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.6693 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.6690 0.0439 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.6687 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.6683 0.0404 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.6683 0.0427 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.6682 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.6678 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.6674 0.0429 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.6670 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.6670 0.0453 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.6669 0.0427 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.6666 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.6664 0.0429 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.6662 0.0437 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.6661 0.0404 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.6660 0.0426 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.6660 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.6658 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.6658 0.0451 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.6657 0.0439 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.6655 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.6653 0.0407 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.6652 0.0407 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.6650 0.0427 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.6646 0.0427 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.6644 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.6643 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.6643 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.6641 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.6641 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.6639 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.6636 0.0432 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.6637 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.6636 0.0433 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.6633 0.0422 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.6634 0.0459 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.6635 0.0457 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.6634 0.0407 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.6634 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.6631 0.0437 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.6628 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.6628 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.6629 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.6628 0.0457 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.6628 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.6629 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.6630 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.6632 0.0456 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.6631 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.6633 0.0422 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.6633 0.0455 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.6632 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.6634 0.0460 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.6633 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.6635 0.0444 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.6636 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.6638 0.0407 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.6638 0.0457 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.6637 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.6635 0.0407 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.6637 0.0468 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.6637 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.6637 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.6637 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.6637 0.0428 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.6637 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.6637 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.6635 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.6637 0.0404 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.6638 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.6637 0.0457 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.6637 0.0436 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.6637 0.0457 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.6637 0.0409 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.6636 0.0458 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.6637 0.0430 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.6640 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.6640 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.6639 0.0425 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.6638 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.6638 0.0452 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.6639 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.6640 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.6641 0.0456 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.6641 0.0428 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.6639 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.6640 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.7240 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.6891 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.6768 0.0491 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.6706 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.6655 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.6573 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.6580 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.6574 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.6600 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.6594 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.6561 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.6545 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.6543 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.6569 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.6559 0.0446 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.6546 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.6549 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.6565 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.6570 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.6576 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.6569 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.6584 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.6575 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.6569 0.0452 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.6564 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.6555 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.6546 0.0444 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.6552 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.6561 0.0422 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.6565 0.0428 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.6562 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.6555 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.6557 0.0425 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.6562 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.6560 0.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.6559 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.6552 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.6542 0.0432 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.6529 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.6522 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.6516 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.6520 0.0433 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.6512 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.6504 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.6505 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.6495 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.6494 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.6489 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.6487 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.6494 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.6489 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.6498 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.6496 0.0443 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.6497 0.0451 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.6495 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.6496 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.6499 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.6495 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.6490 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.6497 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.6497 0.0429 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.6504 0.0440 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.6508 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.6510 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.6509 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.6514 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.6515 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.6512 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.6512 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.6511 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.6515 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.6517 0.0456 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.6520 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.6517 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.6515 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.6519 0.0457 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.6518 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.6519 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.6515 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.6514 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.6508 0.0431 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.6509 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.6505 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.6504 0.0457 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.6500 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.6497 0.0423 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.6495 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.6492 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.6488 0.0423 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.6490 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.6488 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.6486 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.6483 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.6480 0.0456 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.6477 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.6476 0.0422 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.6475 0.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.6472 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.6468 0.0463 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.6464 0.0427 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.6464 0.0428 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.6463 0.0405 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.6460 0.0428 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.6458 0.0431 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.6456 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.6455 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.6454 0.0432 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.6454 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.6453 0.0438 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.6453 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.6452 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.6450 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.6448 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.6447 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.6445 0.0422 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.6441 0.0456 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.6439 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.6439 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.6438 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.6437 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.6437 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.6434 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.6431 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.6432 0.0455 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.6432 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.6428 0.0424 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.6430 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.6430 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.6430 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.6429 0.0402 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.6426 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.6424 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.6424 0.0437 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.6424 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.6424 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.6424 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.6425 0.0465 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.6426 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.6427 0.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.6426 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.6429 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.6428 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.6428 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.6430 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.6429 0.0436 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.6431 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.6432 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.6434 0.0424 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.6434 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.6433 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.6431 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.6433 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.6433 0.0518 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.6433 0.0455 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.6433 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.6433 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.6433 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.6433 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.6431 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.6433 0.0426 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.6434 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.6433 0.0456 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.6434 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.6433 0.0430 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.6434 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.6433 0.0475 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.6434 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.6437 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.6437 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.6436 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.6435 0.0450 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.6434 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.6436 0.0443 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.6436 0.0421 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.6437 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.6437 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.6435 0.0433 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.6436 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.7040 0.0441 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.6700 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.6576 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.6518 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.6462 0.0423 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.6379 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.6386 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.6380 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.6407 0.0425 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.6400 0.0434 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.6367 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.6353 0.0431 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.6350 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.6376 0.0448 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.6366 0.0430 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.6353 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.6356 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.6372 0.0458 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.6378 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.6384 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.6378 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.6391 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.6382 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.6377 0.0405 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.6372 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.6362 0.0422 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.6353 0.0458 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.6360 0.0425 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.6369 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.6373 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.6370 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.6364 0.0437 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.6366 0.0405 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.6371 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.6369 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.6368 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.6361 0.0478 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.6352 0.0423 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.6338 0.0411 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.6331 0.0475 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.6326 0.0434 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.6329 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.6322 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.6314 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.6315 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.6305 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.6304 0.0404 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.6299 0.0432 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.6297 0.0429 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.6304 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.6299 0.0420 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.6308 0.0422 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.6306 0.0433 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.6308 0.0425 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.6306 0.0432 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.6307 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.6310 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.6305 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.6301 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.6307 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.6307 0.0419 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.6315 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.6319 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.6321 0.0475 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.6321 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.6325 0.0433 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.6327 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.6323 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.6323 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.6323 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.6327 0.0422 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.6329 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.6332 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.6329 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.6327 0.0422 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.6331 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.6330 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.6331 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.6327 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.6326 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.6320 0.0427 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.6321 0.0432 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.6317 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.6317 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.6312 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.6310 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.6307 0.0405 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.6305 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.6301 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.6303 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.6300 0.0449 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.6299 0.0439 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.6296 0.0463 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.6293 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.6289 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.6289 0.0464 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.6289 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.6285 0.0431 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.6282 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.6277 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.6277 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.6277 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.6274 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.6272 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.6270 0.0430 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.6269 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.6268 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.6268 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.6267 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.6267 0.0477 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.6266 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.6264 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.6262 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.6261 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.6259 0.0458 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.6255 0.0455 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.6254 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.6253 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.6253 0.0431 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.6251 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.6251 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.6249 0.0455 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.6246 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.6247 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.6246 0.0440 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.6243 0.0423 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.6244 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.6245 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.6244 0.0477 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.6243 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.6240 0.0431 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.6238 0.0404 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.6238 0.0419 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.6239 0.0405 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.6238 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.6239 0.0446 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.6239 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.6240 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.6242 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.6241 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.6244 0.0423 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.6243 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.6243 0.0427 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.6244 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.6244 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.6246 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.6246 0.0423 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.6249 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.6249 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.6248 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.6246 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.6247 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.6247 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.6248 0.0407 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.6248 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.6248 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.6248 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.6248 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.6246 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.6247 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.6248 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.6248 0.0478 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.6248 0.0427 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.6248 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.6248 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.6248 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.6249 0.0454 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.6252 0.0429 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.6252 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.6251 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.6250 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.6249 0.0429 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.6250 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.6251 0.0457 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.6252 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.6251 0.0420 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.6250 0.0426 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.6251 0.0431 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.6852 0.0426 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.6520 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.6396 0.0425 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.6341 0.0404 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.6283 0.0437 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.6200 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.6207 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.6201 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.6227 0.0456 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.6221 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.6188 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.6174 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.6172 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.6198 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.6187 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.6174 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.6178 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.6194 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.6200 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.6206 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.6200 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.6212 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.6203 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.6197 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.6192 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.6182 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.6174 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.6181 0.0436 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.6190 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.6194 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.6191 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.6184 0.0437 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.6188 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.6193 0.0456 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.6191 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.6190 0.0431 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.6183 0.0429 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.6174 0.0482 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.6159 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.6153 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.6148 0.0404 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.6152 0.0430 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.6144 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.6136 0.0428 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.6137 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.6129 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.6127 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.6122 0.0435 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.6121 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.6128 0.0408 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.6123 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.6132 0.0425 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.6130 0.0444 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.6132 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.6129 0.0402 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.6131 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.6134 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.6130 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.6125 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.6130 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.6131 0.0434 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.6139 0.0445 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.6143 0.0427 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.6145 0.0402 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.6145 0.0435 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.6149 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.6151 0.0429 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.6148 0.0461 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.6148 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.6147 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.6152 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.6153 0.0455 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.6157 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.6154 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.6152 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.6155 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.6154 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.6156 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.6152 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.6151 0.0478 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.6145 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.6146 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.6142 0.0482 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.6142 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.6138 0.0461 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.6135 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.6133 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.6130 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.6127 0.0434 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.6128 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.6126 0.0417 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.6125 0.0423 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.6122 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.6119 0.0429 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.6115 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.6115 0.0404 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.6115 0.0426 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.6111 0.0460 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.6108 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.6104 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.6104 0.0478 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.6103 0.0461 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.6100 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.6098 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.6096 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.6095 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.6094 0.0408 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.6094 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.6093 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.6093 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.6092 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.6090 0.0462 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.6088 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.6087 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.6085 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.6082 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.6080 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.6080 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.6079 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.6078 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.6078 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.6075 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.6072 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.6073 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.6073 0.0456 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.6069 0.0408 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.6071 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.6072 0.0449 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.6071 0.0437 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.6070 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.6067 0.0459 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.6065 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.6065 0.0408 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.6065 0.0428 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.6065 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.6065 0.0457 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.6066 0.0430 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.6067 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.6068 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.6067 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.6070 0.0427 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.6070 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.6069 0.0431 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.6071 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.6070 0.0461 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.6072 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.6073 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.6075 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.6076 0.0431 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.6075 0.0460 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.6072 0.0455 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.6073 0.0467 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.6074 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.6074 0.0429 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.6074 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.6074 0.0430 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.6074 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.6074 0.0435 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.6072 0.0465 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.6074 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.6075 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.6074 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.6075 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.6074 0.0436 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.6075 0.0408 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.6074 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.6075 0.0441 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.6079 0.0435 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.6078 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.6078 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.6077 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.6076 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.6077 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.6077 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.6078 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.6077 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.6076 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.6077 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.6679 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.6354 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.6231 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.6179 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.6117 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.6033 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.6038 0.0429 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.6033 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.6059 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.6052 0.0465 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.6019 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.6007 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.6004 0.0404 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.6030 0.0458 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.6020 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.6007 0.0433 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.6011 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.6027 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.6033 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.6039 0.0405 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.6034 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.6045 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.6036 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.6030 0.0430 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.6025 0.0451 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.6015 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.6007 0.0405 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.6015 0.0432 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.6023 0.0457 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.6027 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.6025 0.0431 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.6017 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.6021 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.6026 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.6024 0.0422 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.6023 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.6016 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.6007 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.5993 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.5987 0.0436 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.5981 0.0425 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.5986 0.0457 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.5978 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.5971 0.0435 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.5972 0.0452 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.5963 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.5962 0.0455 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.5958 0.0507 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.5956 0.0444 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.5963 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.5958 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.5967 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.5966 0.0441 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.5968 0.0434 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.5965 0.0430 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.5966 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.5970 0.0469 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.5965 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.5961 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.5966 0.0451 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.5967 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.5975 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.5980 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.5982 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.5982 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.5986 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.5988 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.5984 0.0437 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.5985 0.0464 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.5984 0.0441 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.5989 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.5990 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.5994 0.0479 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.5991 0.0437 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.5989 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.5992 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.5991 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.5993 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.5989 0.0445 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.5989 0.0470 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.5983 0.0479 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.5983 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.5979 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.5979 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.5975 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.5973 0.0473 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.5970 0.0436 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.5968 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.5964 0.0431 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.5966 0.0449 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.5964 0.0438 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.5963 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.5959 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.5957 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.5953 0.0461 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.5953 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.5953 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.5949 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.5946 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.5942 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.5942 0.0426 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.5941 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.5939 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.5937 0.0457 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.5935 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.5933 0.0421 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.5933 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.5933 0.0461 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.5931 0.0431 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.5931 0.0439 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.5930 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.5929 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.5927 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.5926 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.5924 0.0464 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.5921 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.5920 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.5919 0.0429 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.5919 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.5917 0.0436 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.5917 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.5914 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.5912 0.0461 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.5913 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.5912 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.5909 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.5910 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.5911 0.0404 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.5911 0.0425 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.5909 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.5906 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.5904 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.5904 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.5905 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.5905 0.0459 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.5905 0.0442 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.5906 0.0479 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.5907 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.5908 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.5907 0.0464 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.5910 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.5910 0.0407 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.5909 0.0414 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.5911 0.0482 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.5910 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.5912 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.5913 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.5916 0.0438 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.5916 0.0464 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.5915 0.0430 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.5912 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.5914 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.5914 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.5914 0.0477 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.5914 0.0437 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.5914 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.5914 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.5915 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.5913 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.5914 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.5915 0.0448 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.5914 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.5915 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.5915 0.0480 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.5915 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.5915 0.0435 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.5916 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.5920 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.5919 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.5919 0.0431 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.5917 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.5916 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.5917 0.0437 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.5918 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.5918 0.0425 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.5918 0.0407 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.5916 0.0427 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.5918 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.6518 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.6200 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.6078 0.0430 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.6030 0.0432 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.5965 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.5880 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.5885 0.0462 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.5880 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.5907 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.5899 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.5867 0.0437 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.5855 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.5852 0.0440 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.5879 0.0460 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.5868 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.5855 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.5860 0.0473 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.5876 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.5882 0.0417 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.5888 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.5883 0.0433 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.5893 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.5885 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.5880 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.5874 0.0430 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.5864 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.5856 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.5864 0.0404 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.5872 0.0408 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.5876 0.0480 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.5874 0.0433 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.5866 0.0483 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.5870 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.5875 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.5874 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.5872 0.0426 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.5865 0.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.5856 0.0431 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.5842 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.5836 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.5831 0.0431 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.5836 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.5828 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.5821 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.5822 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.5814 0.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.5813 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.5808 0.0430 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.5807 0.0407 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.5814 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.5809 0.0459 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.5818 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.5817 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.5819 0.0430 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.5816 0.0408 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.5817 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.5821 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.5817 0.0456 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.5812 0.0441 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.5817 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.5818 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.5827 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.5832 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.5834 0.0417 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.5834 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.5838 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.5840 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.5837 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.5837 0.0405 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.5836 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.5841 0.0437 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.5843 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.5846 0.0456 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.5843 0.0403 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.5842 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.5845 0.0455 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.5844 0.0443 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.5845 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.5842 0.0436 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.5841 0.0440 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.5835 0.0434 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.5836 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.5831 0.0408 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.5832 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.5827 0.0417 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.5825 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.5823 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.5821 0.0435 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.5817 0.0407 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.5819 0.0432 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.5817 0.0417 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.5816 0.0439 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.5812 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.5810 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.5806 0.0431 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.5806 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.5806 0.0408 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.5803 0.0455 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.5800 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.5796 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.5796 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.5795 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.5792 0.0407 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.5791 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.5789 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.5787 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.5787 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.5787 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.5785 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.5785 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.5784 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.5783 0.0429 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.5781 0.0420 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.5780 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.5778 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.5775 0.0429 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.5774 0.0464 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.5774 0.1046 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.5773 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.5772 0.0455 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.5772 0.0453 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.5769 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.5766 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.5767 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.5767 0.0467 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.5763 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.5765 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.5766 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.5765 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.5764 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.5761 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.5759 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.5759 0.0436 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.5760 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.5759 0.0441 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.5760 0.0435 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.5761 0.0405 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.5761 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.5763 0.0428 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.5762 0.0436 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.5765 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.5764 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.5764 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.5766 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.5765 0.0457 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.5767 0.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.5768 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.5771 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.5771 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.5770 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.5767 0.0461 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.5768 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.5769 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.5769 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.5769 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.5769 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.5769 0.0458 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.5770 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.5768 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.5769 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.5770 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.5770 0.0456 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.5770 0.0494 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.5770 0.0420 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.5770 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.5770 0.0505 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.5771 0.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.5775 0.0450 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.5775 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.5775 0.0437 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.5773 0.0434 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.5772 0.0429 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.5772 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.5773 0.0459 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.5774 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.5773 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.5772 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.5773 0.0456 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.6366 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.6059 0.0445 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.5938 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.5892 0.0428 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.5826 0.0423 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.5741 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.5745 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.5741 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.5767 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.5760 0.0429 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.5727 0.0438 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.5717 0.0428 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.5714 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.5740 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.5729 0.0411 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.5716 0.0430 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.5721 0.0411 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.5738 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.5743 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.5749 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.5745 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.5754 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.5745 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.5741 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.5736 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.5725 0.0440 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.5717 0.0431 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.5725 0.0430 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.5733 0.0431 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.5737 0.0437 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.5734 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.5727 0.0411 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.5731 0.0438 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.5736 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.5734 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.5733 0.0426 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.5725 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.5717 0.0423 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.5702 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.5697 0.0469 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.5691 0.0425 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.5697 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.5689 0.0432 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.5682 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.5684 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.5675 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.5674 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.5670 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.5669 0.0433 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.5676 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.5671 0.0438 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.5680 0.0460 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.5679 0.0445 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.5681 0.0440 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.5679 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.5680 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.5683 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.5679 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.5675 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.5680 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.5681 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.5690 0.0447 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.5695 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.5697 0.0423 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.5697 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.5702 0.0405 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.5703 0.0426 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.5700 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.5700 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.5700 0.0478 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.5705 0.0464 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.5707 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.5710 0.0424 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.5707 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.5706 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.5709 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.5708 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.5709 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.5706 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.5705 0.0478 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.5699 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.5699 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.5695 0.0443 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.5695 0.0422 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.5691 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.5689 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.5686 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.5684 0.0457 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.5681 0.0436 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.5683 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.5681 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.5680 0.0444 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.5676 0.0431 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.5674 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.5670 0.0405 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.5670 0.0428 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.5671 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.5667 0.0433 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.5664 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.5660 0.0406 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.5660 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.5659 0.0432 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.5657 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.5655 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.5653 0.0432 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.5652 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.5652 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.5651 0.0470 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.5650 0.0411 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.5650 0.0468 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.5649 0.0481 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.5647 0.0434 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.5646 0.0468 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.5645 0.0405 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.5643 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.5640 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.5639 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.5638 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.5638 0.0454 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.5637 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.5637 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.5634 0.0436 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.5631 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.5632 0.0460 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.5632 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.5628 0.0429 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.5630 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.5631 0.0439 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.5630 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.5629 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.5626 0.0424 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.5624 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.5624 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.5625 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.5624 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.5625 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.5626 0.0449 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.5627 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.5628 0.0464 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.5627 0.0425 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.5630 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.5630 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.5630 0.0429 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.5631 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.5631 0.0454 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.5633 0.0433 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.5633 0.0433 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.5636 0.0406 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.5637 0.0429 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.5636 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.5633 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.5634 0.0411 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.5634 0.0456 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.5634 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.5634 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.5634 0.0440 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.5635 0.0436 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.5635 0.0470 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.5633 0.0484 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.5634 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.5635 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.5635 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.5635 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.5635 0.0466 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.5636 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.5635 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.5637 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.5641 0.0405 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.5640 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.5640 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.5639 0.0433 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.5637 0.0442 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.5638 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.5638 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.5639 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.5638 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.5637 0.0463 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.5639 0.0425 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4236 0.0488 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.4178 0.0259 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.4116 0.0259 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.4052 0.0258 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3984 0.0339 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.3907 0.0264 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.3811 0.0275 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.3680 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.3481 0.0418 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 4.3161 0.0323 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 4.2695 0.0336 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 4.2169 0.0275 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 4.1631 0.0266 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 4.1121 0.0368 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 4.0637 0.0270 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 4.0184 0.0280 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.9755 0.0292 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.9376 0.0265 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.9013 0.0296 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.8658 0.0282 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.8338 0.0260 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.8038 0.0322 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.7758 0.0291 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.7500 0.0253 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.7256 0.0314 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.7036 0.0273 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.6832 0.0279 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.6630 0.0274 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.6443 0.0259 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.6271 0.0261 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.6117 0.0257 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.5962 0.0317 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.5812 0.0258 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.5678 0.0301 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.5545 0.0269 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.5425 0.0252 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.5303 0.0275 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.5188 0.0264 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.5078 0.0254 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.4975 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.4875 0.0263 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.4780 0.0250 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.4689 0.0302 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.4602 0.0309 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.4518 0.0262 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.4440 0.0270 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.4367 0.0258 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.4298 0.0251 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.4231 0.0255 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.4166 0.0256 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.4102 0.0339 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.4039 0.0332 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3980 0.0278 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3921 0.0265 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3866 0.0254 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3809 0.0250 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3757 0.0256 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3706 0.0253 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3656 0.0260 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.3609 0.0254 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.3564 0.0276 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.3523 0.0277 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.3484 0.0278 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.3439 0.0381 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.3397 0.0258 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.3360 0.0269 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.3323 0.0264 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.3281 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.3242 0.0346 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.3208 0.0279 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.3173 0.0257 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.3143 0.0268 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.3109 0.0265 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.3078 0.0261 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.3048 0.0270 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.3020 0.0300 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.2991 0.0261 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.2962 0.0288 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.2934 0.0273 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2904 0.0260 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2876 0.0265 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2851 0.0261 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2826 0.0273 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2800 0.0260 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2773 0.0260 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2748 0.0263 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2722 0.0261 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2698 0.0347 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2676 0.0278 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2654 0.0285 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2632 0.0260 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2610 0.0274 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2589 0.0282 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2568 0.0358 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2546 0.0253 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.2525 0.0269 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.2506 0.0266 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.2485 0.0325 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2466 0.0320 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2446 0.0373 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2427 0.0369 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2409 0.0323 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2391 0.0269 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2372 0.0275 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2354 0.0336 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2336 0.0259 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2317 0.0286 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2297 0.0270 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2280 0.0292 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2260 0.0339 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2243 0.0288 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2226 0.0305 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2208 0.0263 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2190 0.0344 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2172 0.0271 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.2155 0.0280 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2138 0.0333 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2123 0.0340 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2107 0.0288 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2090 0.0285 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.2077 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.2061 0.0360 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.2046 0.0286 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.2032 0.0294 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.2015 0.0298 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.1998 0.0303 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.1983 0.0302 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.1969 0.0303 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.1953 0.0354 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.1938 0.0309 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.1924 0.0319 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1908 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1894 0.0341 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.1878 0.0370 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.1860 0.0301 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1844 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1828 0.0303 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1812 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1798 0.0305 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1783 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1768 0.0311 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1751 0.0316 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1736 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1720 0.0330 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1705 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1690 0.0318 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1676 0.0321 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1663 0.0374 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.1647 0.0338 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.1632 0.0320 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.1618 0.0379 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.1605 0.0378 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.1591 0.0340 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.1576 0.0325 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.1560 0.0319 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.1545 0.0324 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.1529 0.0345 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.1513 0.0325 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.1496 0.0332 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.1480 0.0332 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.1465 0.0323 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.1448 0.0336 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.1431 0.0328 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.1415 0.0331 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.1399 0.0336 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.1383 0.0339 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.1367 0.0343 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.1352 0.0337 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.1336 0.0337 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.1319 0.0351 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.1304 0.0345 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.1290 0.0420 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.1276 0.0367 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.1263 0.0339 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.1248 0.0359 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.1233 0.0354 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.1216 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.1199 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.8978 0.0341 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.8487 0.0354 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.8329 0.0404 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.8266 0.0415 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.8229 0.0358 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.8202 0.0351 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.8186 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.8172 0.0377 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.8148 0.0353 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.8123 0.0427 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.8083 0.0357 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.8064 0.0391 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.8043 0.0365 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.8037 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.8022 0.0445 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.8008 0.0429 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.7987 0.0349 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.7987 0.0344 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.7970 0.0355 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.7939 0.0352 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.7918 0.0429 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.7902 0.0350 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.7881 0.0354 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.7860 0.0349 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.7836 0.0376 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.7821 0.0360 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.7805 0.0353 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.7783 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.7765 0.0360 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.7748 0.0363 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.7738 0.0360 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.7717 0.0357 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.7694 0.0355 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.7677 0.0360 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.7655 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.7640 0.0354 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.7618 0.0366 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.7594 0.0361 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.7571 0.0362 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.7551 0.0355 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.7528 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.7507 0.0440 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.7484 0.0362 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.7462 0.0357 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.7441 0.0356 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.7420 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.7404 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.7387 0.0414 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.7369 0.0353 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.7354 0.0371 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.7336 0.0391 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.7318 0.0365 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.7299 0.0444 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.7279 0.0484 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.7261 0.0437 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.7243 0.0521 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.7226 0.0489 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.7207 0.0431 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.7188 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.7174 0.0413 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.7157 0.0372 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.7143 0.0422 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.7130 0.0430 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.7112 0.0423 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.7093 0.0369 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.7080 0.0445 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.7066 0.0443 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.7044 0.0527 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.7025 0.0440 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.7011 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.6995 0.0375 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.6981 0.0397 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.6966 0.0375 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.6950 0.0454 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.6936 0.0411 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.6924 0.0444 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.6909 0.0427 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.6895 0.0439 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.6879 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.6864 0.0436 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.6849 0.0427 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.6835 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.6821 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.6806 0.0439 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.6787 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.6771 0.0443 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.6757 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.6742 0.0432 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.6727 0.0471 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.6714 0.0437 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.6700 0.0442 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.6687 0.0452 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.6673 0.0372 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.6658 0.0376 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.6642 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.6627 0.0366 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.6614 0.0434 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.6600 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.6588 0.0369 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.6574 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.6562 0.0377 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.6548 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.6534 0.0446 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.6520 0.0368 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.6507 0.0421 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.6494 0.0375 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.6480 0.0394 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.6468 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.6457 0.0368 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.6441 0.0368 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.6429 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.6418 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.6405 0.0423 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.6391 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.6378 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.6363 0.0371 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.6350 0.0376 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.6339 0.0397 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.6329 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.6317 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.6308 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.6296 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.6285 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.6274 0.0376 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.6262 0.0375 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.6249 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.6239 0.0393 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.6229 0.0378 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.6217 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.6206 0.0428 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.6195 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.6183 0.0377 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.6173 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.6162 0.0378 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.6149 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.6138 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.6126 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.6115 0.0397 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.6106 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.6095 0.0399 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.6086 0.0395 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.6075 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.6065 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.6054 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.6044 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.6035 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.6025 0.0374 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.6017 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.6005 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.5995 0.0454 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.5987 0.0395 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.5980 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.5971 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.5963 0.0370 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.5952 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.5942 0.0431 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.5932 0.0378 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.5922 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.5911 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.5902 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.5893 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.5882 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.5871 0.0426 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.5862 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.5853 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.5844 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.5835 0.0394 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.5826 0.0399 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.5817 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.5807 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.5798 0.0404 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.5791 0.0393 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.5785 0.0376 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.5778 0.0429 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.5772 0.0378 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.5763 0.0422 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.5753 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.5743 0.0378 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.4929 0.0378 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.4353 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.4210 0.0387 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.4164 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.4130 0.0380 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.4100 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.4102 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.4107 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.4111 0.0453 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.4105 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.4086 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.4087 0.0451 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.4080 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.4095 0.0379 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.4093 0.0377 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.4091 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.4089 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.4103 0.0436 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.4100 0.0375 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.4084 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.4075 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.4081 0.0377 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.4072 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.4062 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.4051 0.0429 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.4045 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.4039 0.0374 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.4032 0.0385 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.4031 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.4027 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.4029 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.4020 0.0441 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.4009 0.0453 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.4005 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.3998 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.3994 0.0376 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.3987 0.0439 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.3974 0.0385 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.3964 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.3954 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.3945 0.0377 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.3936 0.0377 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.3925 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.3916 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.3906 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.3892 0.0378 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.3889 0.0382 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.3882 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.3876 0.0380 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.3875 0.0388 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.3867 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.3863 0.0380 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.3855 0.0426 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.3847 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.3841 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.3836 0.0426 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.3831 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.3823 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.3817 0.0379 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.3814 0.0429 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.3808 0.0379 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.3803 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.3801 0.0387 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.3795 0.0374 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.3787 0.0381 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.3785 0.0442 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.3781 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.3771 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.3763 0.0378 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.3760 0.0378 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.3755 0.0385 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.3752 0.0377 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.3748 0.0375 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.3742 0.0389 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.3737 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.3737 0.0376 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.3732 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.3729 0.0377 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.3723 0.0378 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.3717 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.3711 0.0381 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.3708 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.3703 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.3697 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.3687 0.0379 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.3681 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.3676 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.3671 0.0387 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.3665 0.0385 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.3662 0.0377 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.3657 0.0379 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.3653 0.0432 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.3648 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.3642 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.3636 0.0377 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.3630 0.0381 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.3625 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.3620 0.0380 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.3615 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.3610 0.0388 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.3607 0.0379 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.3603 0.0381 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.3596 0.0429 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.3591 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.3586 0.0381 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.3582 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.3576 0.0381 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.3574 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.3571 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.3564 0.0432 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.3560 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.3557 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.3553 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.3547 0.0454 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.3542 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.3535 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.3531 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.3527 0.0381 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.3525 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.3521 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.3519 0.0388 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.3515 0.0387 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.3510 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.3507 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.3503 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.3497 0.0382 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.3495 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.3492 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.3488 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.3484 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.3481 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.3475 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.3472 0.0387 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.3469 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.3464 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.3461 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.3457 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.3453 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.3452 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.3448 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.3446 0.0388 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.3441 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.3438 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.3434 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.3430 0.0378 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.3428 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.3425 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.3423 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.3418 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.3414 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.3412 0.0436 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.3412 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.3410 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.3408 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.3404 0.0388 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.3400 0.0388 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.3396 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.3392 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.3387 0.0379 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.3385 0.0387 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.3383 0.0388 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.3378 0.0433 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.3374 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.3370 0.0382 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.3368 0.0384 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.3364 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.3361 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.3359 0.0442 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.3356 0.0433 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.3352 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.3349 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.3347 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.3346 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.3346 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.3345 0.0388 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.3342 0.0405 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.3338 0.0454 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.3334 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.3479 0.0388 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.2917 0.0382 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.2765 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.2727 0.0436 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.2701 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.2675 0.0494 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.2679 0.0390 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.2690 0.0388 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.2704 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.2700 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.2687 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.2681 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.2682 0.0386 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.2704 0.0388 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.2702 0.0394 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.2700 0.0436 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.2699 0.0387 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.2717 0.0438 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.2717 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.2706 0.0432 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.2699 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.2712 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.2707 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.2697 0.0387 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.2690 0.0383 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.2684 0.0434 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.2679 0.0384 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.2676 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.2679 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.2678 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.2681 0.0385 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.2674 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.2667 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.2666 0.0428 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.2662 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.2660 0.0437 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.2655 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.2644 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.2635 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.2628 0.0388 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.2621 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.2615 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.2606 0.0415 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.2598 0.0390 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.2590 0.0387 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.2576 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.2575 0.0421 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.2570 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.2565 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.2566 0.0390 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.2559 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.2558 0.0449 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.2552 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.2546 0.0386 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.2541 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.2539 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.2535 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.2529 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.2524 0.0384 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.2525 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.2520 0.0384 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.2518 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.2518 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.2514 0.0385 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.2509 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.2508 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.2505 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.2498 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.2493 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.2491 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.2489 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.2488 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.2487 0.0390 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.2482 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.2479 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.2481 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.2477 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.2476 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.2471 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.2466 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.2461 0.0415 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.2460 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.2455 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.2450 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.2442 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.2437 0.0387 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.2434 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.2430 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.2425 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.2423 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.2420 0.0384 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.2417 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 2.2413 0.0384 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 2.2409 0.0387 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 2.2404 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 2.2399 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 2.2396 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 2.2392 0.0386 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 2.2389 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 2.2384 0.0386 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 2.2383 0.0458 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 2.2380 0.0441 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 2.2376 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 2.2372 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 2.2368 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 2.2365 0.0438 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 2.2361 0.0455 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 2.2360 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 2.2359 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 2.2354 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 2.2352 0.0385 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 2.2351 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 2.2347 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 2.2343 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 2.2340 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 2.2335 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 2.2332 0.0415 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 2.2330 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 2.2329 0.0387 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 2.2326 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 2.2325 0.0388 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 2.2322 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 2.2319 0.0385 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 2.2317 0.0386 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 2.2315 0.0390 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 2.2310 0.0441 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 2.2309 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 2.2307 0.0421 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 2.2305 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 2.2303 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 2.2300 0.0386 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 2.2296 0.0415 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 667/3560 Training loss: 2.2294 0.0384 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 2.2293 0.0419 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 2.2290 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 2.2288 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 2.2285 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 2.2284 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 2.2283 0.0384 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 2.2281 0.0439 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 2.2280 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 2.2277 0.0395 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 2.2275 0.0437 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 2.2272 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 2.2270 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 2.2270 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 2.2268 0.0388 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 2.2267 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 2.2264 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 2.2261 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 2.2261 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 2.2262 0.0391 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 2.2261 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 2.2260 0.0442 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 2.2257 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 2.2255 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 2.2252 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 2.2250 0.0417 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 2.2246 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 2.2246 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 2.2245 0.0386 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 2.2242 0.0389 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 2.2239 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 2.2237 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 2.2236 0.0424 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 2.2234 0.0387 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 2.2232 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 2.2231 0.0386 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 2.2229 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 2.2227 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 2.2225 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 2.2225 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 2.2226 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 2.2226 0.0438 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 2.2227 0.0394 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 2.2226 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 2.2223 0.0392 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 2.2220 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.2584 0.0380 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 2.2030 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 2.1882 0.0388 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 2.1833 0.0388 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 2.1809 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 2.1783 0.0389 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 2.1787 0.0389 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 2.1803 0.0391 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 2.1817 0.0390 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 2.1814 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 2.1797 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 2.1787 0.0388 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 2.1789 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 2.1812 0.0390 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 2.1807 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 2.1801 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 2.1803 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 2.1820 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 2.1824 0.0440 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 2.1815 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 2.1810 0.0389 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 2.1828 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 2.1823 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 2.1814 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 2.1808 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 2.1802 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 2.1796 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 2.1794 0.0464 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 2.1800 0.0393 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 2.1800 0.0428 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 2.1802 0.0392 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 2.1795 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 2.1789 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 2.1792 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 2.1787 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 2.1785 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 2.1781 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 2.1769 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 2.1762 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 2.1754 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 2.1748 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 2.1744 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 2.1736 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 2.1729 0.0427 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 2.1722 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 2.1708 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 2.1708 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 2.1702 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 2.1699 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 2.1702 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 2.1695 0.0444 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 2.1696 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 2.1691 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 2.1686 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 2.1681 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 2.1680 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 2.1678 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 2.1672 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 2.1668 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 2.1670 0.0440 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 2.1665 0.0453 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 2.1665 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 2.1666 0.0391 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 2.1663 0.0442 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 2.1659 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 2.1659 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 2.1657 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 2.1651 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 2.1647 0.0441 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 2.1646 0.0441 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 2.1645 0.0391 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 2.1645 0.0458 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 785/3560 Training loss: 2.1645 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 2.1641 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 2.1638 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 2.1641 0.0393 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 2.1638 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 2.1638 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 2.1632 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 2.1628 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 2.1623 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 2.1622 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 2.1618 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 2.1613 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 2.1606 0.0392 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 2.1601 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 2.1599 0.0392 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 2.1595 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 2.1591 0.0391 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 2.1590 0.0448 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 2.1586 0.0432 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 2.1584 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 2.1580 0.0425 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 2.1576 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 2.1572 0.0393 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 2.1569 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 2.1566 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 2.1563 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 2.1559 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 2.1556 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 2.1555 0.0442 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 2.1553 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 2.1549 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 2.1546 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 2.1543 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 2.1541 0.0410 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 2.1537 0.0444 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 2.1537 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 2.1536 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 2.1533 0.0420 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 2.1531 0.0392 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 2.1530 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 2.1528 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 2.1524 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 2.1521 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 2.1517 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 2.1515 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 2.1513 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 2.1513 0.0410 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 2.1511 0.0421 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 2.1510 0.0423 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 2.1508 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 2.1505 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 2.1504 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 2.1502 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 2.1499 0.0420 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 2.1498 0.0421 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 2.1497 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 2.1495 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 2.1494 0.0392 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 2.1492 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 2.1488 0.0447 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 2.1486 0.0445 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 2.1486 0.0393 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 2.1484 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 2.1483 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 2.1481 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 2.1480 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 2.1481 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 2.1479 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 2.1479 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 2.1476 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 2.1475 0.0420 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 2.1473 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 2.1471 0.0443 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 2.1472 0.0393 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 2.1470 0.0423 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 2.1470 0.0421 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 2.1468 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 2.1466 0.0392 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 2.1466 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 2.1467 0.0392 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 2.1467 0.0391 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 2.1467 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 2.1465 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 2.1463 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 2.1461 0.0442 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 2.1459 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 2.1456 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 2.1457 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 2.1456 0.0392 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 2.1454 0.0390 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 2.1452 0.0396 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 2.1451 0.0447 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 2.1450 0.0445 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 2.1449 0.0438 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 2.1448 0.0392 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 2.1447 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 2.1446 0.0423 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 2.1444 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 2.1444 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 2.1444 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 2.1445 0.0394 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 2.1446 0.0391 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 2.1448 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 2.1447 0.0439 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 2.1444 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 2.1442 0.0388 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 2.1928 0.0397 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 2.1365 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 2.1221 0.0391 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 2.1166 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 2.1133 0.0394 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 2.1113 0.0392 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 2.1114 0.0390 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 2.1136 0.0395 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 2.1150 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 2.1147 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 2.1128 0.0390 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 2.1113 0.0394 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 2.1116 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 2.1137 0.0390 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 2.1132 0.0406 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 906/3560 Training loss: 2.1124 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 2.1123 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 2.1142 0.0389 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 2.1146 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 2.1139 0.0392 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 2.1134 0.0441 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 2.1155 0.0448 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 2.1151 0.0394 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 2.1142 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 2.1138 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 2.1131 0.0417 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 2.1125 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 2.1123 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 2.1131 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 2.1132 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 2.1133 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 2.1126 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 2.1122 0.0395 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 2.1125 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 2.1122 0.0465 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 2.1120 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 2.1116 0.0394 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 2.1104 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 2.1097 0.0462 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 2.1090 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 2.1085 0.0395 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 2.1082 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 2.1075 0.0388 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 2.1068 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 2.1063 0.0392 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 2.1049 0.0397 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 2.1049 0.0441 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 2.1044 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 2.1042 0.0392 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 2.1046 0.0394 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 2.1039 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 2.1042 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 2.1038 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 2.1033 0.0394 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 2.1029 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 2.1029 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 2.1027 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 2.1022 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 2.1018 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 2.1021 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 2.1017 0.0394 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 2.1018 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 2.1019 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 2.1017 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 2.1013 0.0452 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 2.1015 0.0441 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 2.1013 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 2.1008 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 2.1004 0.0453 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 2.1003 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 2.1003 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 2.1004 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 2.1005 0.0395 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 2.1001 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 2.0999 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 2.1002 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 2.0999 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 2.1000 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 2.0994 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 2.0991 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 2.0986 0.0460 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 2.0986 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 2.0981 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 2.0977 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 2.0970 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 2.0966 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 2.0964 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 2.0961 0.0390 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 2.0957 0.0443 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 2.0956 0.0449 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 2.0953 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 2.0951 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 2.0947 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 2.0944 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 2.0941 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 2.0938 0.0466 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 2.0936 0.0442 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 2.0932 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 2.0929 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 2.0925 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 2.0925 0.0397 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 2.0924 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 2.0921 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 2.0918 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 2.0915 0.0426 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 2.0914 0.0417 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 2.0911 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 2.0911 0.0531 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 2.0911 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 2.0908 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 2.0906 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 2.0906 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 2.0904 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 2.0901 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 2.0898 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 2.0894 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 2.0892 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 2.0891 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 2.0891 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 2.0890 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 2.0889 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 2.0887 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 2.0885 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 2.0885 0.0395 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 2.0883 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 2.0881 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 2.0880 0.0392 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 2.0879 0.0390 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 2.0878 0.0465 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 2.0878 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 2.0876 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 2.0873 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 2.0871 0.0397 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 2.0871 0.0391 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 2.0870 0.0396 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1026/3560 Training loss: 2.0870 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 2.0869 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 2.0868 0.0394 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 2.0869 0.0397 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 2.0867 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 2.0868 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 2.0866 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 2.0866 0.0430 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 2.0865 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 2.0862 0.0397 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 2.0863 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 2.0863 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 2.0863 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 2.0861 0.0396 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 2.0859 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 2.0860 0.0389 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 2.0862 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 2.0862 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 2.0862 0.0395 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 2.0860 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 2.0859 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 2.0857 0.0393 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 2.0856 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 2.0853 0.0390 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 2.0855 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 2.0855 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 2.0853 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 2.0852 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 2.0850 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 2.0850 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 2.0849 0.0449 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 2.0849 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 2.0849 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 2.0849 0.0394 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 2.0847 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 2.0847 0.0432 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 2.0847 0.0446 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 2.0849 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 2.0851 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 2.0853 0.0395 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 2.0852 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 2.0850 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 2.0849 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 2.1400 0.0430 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 2.0851 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 2.0714 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 2.0655 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 2.0618 0.0412 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 2.0599 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 2.0601 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 2.0623 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 2.0639 0.0421 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 2.0638 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 2.0617 0.0389 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 2.0598 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 2.0601 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 2.0621 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 2.0616 0.0392 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 2.0606 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 2.0605 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 2.0625 0.0390 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 2.0627 0.0390 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 2.0622 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 2.0617 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 2.0639 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 2.0635 0.0466 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 2.0626 0.0390 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 2.0623 0.0464 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 2.0616 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 2.0609 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 2.0608 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 2.0616 0.0467 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 2.0617 0.0393 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 2.0617 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 2.0610 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 2.0607 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 2.0611 0.0443 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 2.0607 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 2.0606 0.0443 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 2.0602 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 2.0591 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 2.0584 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 2.0576 0.0444 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 2.0571 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 2.0570 0.0442 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 2.0563 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 2.0557 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 2.0551 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 2.0538 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 2.0539 0.0421 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 2.0534 0.0434 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 2.0531 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 2.0537 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 2.0530 0.0433 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 2.0534 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 2.0530 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 2.0525 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 2.0522 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 2.0522 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 2.0521 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 2.0516 0.0450 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 2.0512 0.0446 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 2.0516 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 2.0512 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 2.0514 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 2.0516 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 2.0514 0.0389 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 2.0510 0.0391 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 2.0512 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 2.0511 0.0389 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 2.0506 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 2.0503 0.0390 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 2.0502 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 2.0503 0.0467 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 2.0504 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 2.0505 0.0398 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1142/3560 Training loss: 2.0502 0.0460 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 2.0499 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 2.0503 0.0443 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 2.0501 0.0393 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 2.0501 0.0390 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 2.0496 0.0393 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 2.0494 0.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 2.0489 0.0393 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 2.0488 0.0391 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 2.0484 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 2.0481 0.0391 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 2.0473 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 2.0470 0.0393 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 2.0468 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 2.0465 0.0392 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 2.0460 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 2.0460 0.0391 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 2.0457 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 2.0456 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 2.0451 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 2.0448 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 2.0445 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 2.0443 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 2.0441 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 2.0437 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 2.0434 0.0408 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 2.0430 0.0391 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 2.0430 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 2.0430 0.0393 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 2.0426 0.0391 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 2.0424 0.0393 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 2.0421 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 2.0420 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 2.0418 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 2.0418 0.0391 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 2.0418 0.0391 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 2.0415 0.0445 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 2.0414 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 2.0414 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 2.0411 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 2.0409 0.0393 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 2.0407 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 2.0402 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 2.0401 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 2.0399 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 2.0400 0.0388 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 2.0398 0.0391 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 2.0398 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 2.0396 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 2.0394 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 2.0395 0.0392 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 2.0393 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 2.0391 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 2.0390 0.0470 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 2.0390 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 2.0389 0.0419 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 2.0389 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 2.0387 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 2.0384 0.0487 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 2.0383 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 2.0383 0.0399 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 2.0382 0.0447 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 2.0382 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 2.0381 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 2.0381 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 2.0382 0.0390 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 2.0380 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 2.0381 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 2.0380 0.0393 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 2.0380 0.0394 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 2.0379 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 2.0377 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 2.0378 0.0467 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 2.0377 0.0433 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 2.0378 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 2.0377 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 2.0375 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 2.0375 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 2.0377 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 2.0377 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 2.0377 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 2.0376 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 2.0375 0.0468 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 2.0373 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 2.0372 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 2.0369 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 2.0372 0.0421 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 2.0372 0.0398 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 2.0370 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 2.0370 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 2.0368 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 2.0368 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 2.0368 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 2.0367 0.0396 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 2.0368 0.0397 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 2.0367 0.0468 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 2.0366 0.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 2.0366 0.0460 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 2.0366 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 2.0368 0.0427 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 2.0370 0.0441 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 2.0372 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 2.0371 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 2.0370 0.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 2.0369 0.0394 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 2.0966 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 2.0420 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 2.0297 0.0422 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 2.0235 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 2.0193 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 2.0167 0.0443 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 2.0167 0.0390 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 2.0189 0.0394 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 2.0210 0.0394 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 2.0208 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 2.0184 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 2.0164 0.0400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1259/3560 Training loss: 2.0169 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 2.0190 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 2.0184 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 2.0173 0.0451 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 2.0172 0.0412 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 2.0192 0.0429 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 2.0193 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 2.0189 0.0422 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 2.0183 0.0454 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 2.0207 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 2.0202 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 2.0193 0.0392 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 2.0190 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 2.0183 0.0424 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 2.0174 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 2.0173 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 2.0181 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 2.0183 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 2.0180 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 2.0172 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 2.0169 0.0401 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 2.0174 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 2.0170 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 2.0168 0.0472 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 2.0165 0.0429 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 2.0153 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 2.0145 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 2.0138 0.0401 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 2.0133 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 2.0132 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 2.0125 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 2.0119 0.0409 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 2.0115 0.0394 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 2.0101 0.0440 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 2.0101 0.0447 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 2.0096 0.0454 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 2.0094 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 2.0100 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 2.0093 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 2.0097 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 2.0093 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 2.0089 0.0455 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 2.0086 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 2.0086 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 2.0085 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 2.0081 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 2.0076 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 2.0080 0.0417 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 2.0077 0.0447 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 2.0080 0.0423 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 2.0082 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 2.0080 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 2.0076 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 2.0078 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 2.0077 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 2.0072 0.0446 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 2.0069 0.0423 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 2.0068 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 2.0070 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 2.0071 0.0422 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 2.0073 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 2.0069 0.0409 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 2.0067 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 2.0071 0.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 2.0069 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 2.0069 0.0442 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 2.0064 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 2.0062 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 2.0057 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 2.0057 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 2.0052 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 2.0049 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 2.0042 0.0389 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 2.0038 0.0391 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 2.0037 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 2.0033 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 2.0029 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 2.0029 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 2.0026 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 2.0025 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 2.0020 0.0423 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 2.0017 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 2.0014 0.0441 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 2.0012 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 2.0010 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 2.0007 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 2.0003 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.9999 0.0450 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.9999 0.0397 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.9998 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.9995 0.0428 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.9993 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.9990 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.9990 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.9987 0.0443 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.9988 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.9988 0.0449 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.9986 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.9985 0.0472 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.9984 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.9982 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.9980 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.9978 0.0460 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.9973 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.9972 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.9971 0.0401 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.9971 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.9970 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.9970 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.9968 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.9965 0.0464 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.9967 0.0391 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.9966 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.9963 0.0394 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.9963 0.0393 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.9963 0.0393 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.9962 0.0395 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.9962 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.9960 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.9957 0.0419 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.9957 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.9956 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.9956 0.0403 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.9956 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.9956 0.0428 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.9955 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.9957 0.0427 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.9955 0.0392 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.9956 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.9956 0.0394 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.9955 0.0418 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.9954 0.0460 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.9952 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.9954 0.0401 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.9953 0.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.9954 0.0396 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.9953 0.0431 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.9951 0.0401 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.9951 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.9953 0.0426 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.9954 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.9953 0.0451 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.9952 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.9952 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.9950 0.0410 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.9949 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.9947 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.9949 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.9950 0.0402 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.9948 0.0469 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.9948 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.9947 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.9947 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.9946 0.0425 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.9946 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.9947 0.0433 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.9946 0.0398 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.9945 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.9944 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.9944 0.0413 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.9946 0.0399 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.9948 0.0421 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.9950 0.0400 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.9950 0.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.9948 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.9947 0.0422 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 2.0571 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 2.0038 0.0424 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.9919 0.0428 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.9849 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.9801 0.0446 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.9768 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.9766 0.0410 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.9782 0.0420 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.9804 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.9801 0.0453 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.9777 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.9753 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.9758 0.0446 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.9780 0.0440 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.9773 0.0413 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.9760 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.9758 0.0466 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.9780 0.0449 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.9780 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.9776 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.9771 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.9794 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.9790 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.9781 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.9779 0.0437 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.9773 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.9764 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.9763 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.9771 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.9774 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.9772 0.0457 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.9763 0.0419 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.9761 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.9766 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.9762 0.0451 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.9761 0.0449 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.9759 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.9747 0.0459 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.9739 0.0394 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.9732 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.9728 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.9728 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.9722 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.9715 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.9712 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.9698 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.9699 0.0420 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.9694 0.0393 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.9692 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.9698 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.9691 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.9696 0.0447 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.9693 0.0466 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.9689 0.0445 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.9686 0.0448 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.9687 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.9687 0.0423 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.9682 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.9677 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.9682 0.0408 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.9679 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.9683 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.9684 0.0427 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.9683 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.9680 0.0394 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.9682 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.9681 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.9676 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.9674 0.0395 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.9673 0.0398 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.9675 0.0468 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.9677 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.9679 0.0405 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.9675 0.0422 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.9673 0.0393 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.9677 0.0449 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.9675 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.9676 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.9671 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.9669 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.9664 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.9664 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.9660 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.9657 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.9651 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.9647 0.0410 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.9646 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.9643 0.0449 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.9639 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.9639 0.0470 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.9636 0.0419 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.9635 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.9631 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.9627 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.9625 0.0448 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.9623 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.9621 0.0469 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.9617 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.9613 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.9609 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.9609 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.9609 0.0451 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.9606 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.9604 0.0449 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.9601 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.9600 0.0420 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.9599 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.9600 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.9599 0.0430 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.9598 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.9597 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.9596 0.0450 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.9594 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.9592 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.9590 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.9586 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.9585 0.0394 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.9584 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.9584 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.9583 0.0393 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.9583 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.9581 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.9579 0.0395 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.9580 0.0444 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.9580 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.9577 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.9576 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.9577 0.0405 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.9576 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.9577 0.0392 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.9574 0.0393 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.9572 0.0395 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.9571 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.9571 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.9571 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.9571 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.9571 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.9571 0.0467 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.9573 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.9571 0.0405 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.9572 0.0419 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.9572 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.9572 0.0395 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.9571 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.9569 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.9570 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.9570 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.9571 0.0450 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.9570 0.0392 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.9568 0.0395 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.9568 0.0395 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.9571 0.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.9571 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.9571 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.9570 0.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.9570 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.9569 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.9568 0.0410 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.9565 0.0468 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.9568 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.9569 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.9567 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.9567 0.0395 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.9566 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.9566 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.9566 0.0419 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.9566 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.9567 0.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.9566 0.0399 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.9565 0.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.9565 0.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.9565 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.9567 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.9568 0.0448 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.9570 0.0400 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.9570 0.0454 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.9569 0.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.9568 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 2.0212 0.0390 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.9699 0.0445 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.9584 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.9511 0.0453 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.9461 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.9420 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.9416 0.0422 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.9428 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.9451 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.9450 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.9425 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.9401 0.0400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.9405 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.9428 0.0425 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.9421 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.9406 0.0391 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.9406 0.0424 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.9426 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.9427 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.9425 0.0421 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.9420 0.0419 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.9443 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.9440 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.9431 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.9430 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.9424 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.9415 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.9414 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.9423 0.0423 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.9425 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.9423 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.9415 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.9413 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.9419 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.9414 0.0458 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.9413 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.9411 0.0409 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.9400 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.9391 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.9384 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.9379 0.0425 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.9380 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.9374 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.9367 0.0466 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.9364 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.9351 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.9351 0.0451 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.9346 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.9344 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.9351 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.9344 0.0431 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.9350 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.9347 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.9343 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.9340 0.0417 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.9341 0.0448 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.9341 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.9336 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.9332 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.9336 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.9334 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.9338 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.9340 0.0409 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.9338 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.9335 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.9337 0.0449 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.9336 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.9332 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.9330 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.9329 0.0446 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.9332 0.0438 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.9333 0.0394 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.9335 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.9332 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.9330 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.9333 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.9331 0.0420 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.9332 0.0397 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.9328 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.9326 0.0397 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.9321 0.0414 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.9321 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.9317 0.0440 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.9315 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.9308 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.9305 0.0448 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.9304 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.9301 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.9296 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.9297 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.9294 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.9292 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.9288 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.9284 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.9282 0.0460 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.9280 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.9278 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.9274 0.0421 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.9270 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.9265 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.9266 0.0451 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.9265 0.0443 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.9262 0.0446 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.9261 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.9258 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.9257 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.9255 0.0448 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.9256 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.9256 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.9255 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.9255 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.9254 0.0404 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.9252 0.0415 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.9250 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.9248 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.9244 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.9243 0.0417 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.9242 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.9241 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.9241 0.0450 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.9240 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.9239 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.9236 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.9238 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.9237 0.0454 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.9234 0.0447 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.9234 0.0409 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.9235 0.0466 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.9234 0.0397 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.9235 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.9233 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.9230 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.9230 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.9230 0.0400 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.9230 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.9230 0.0466 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.9230 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.9230 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.9232 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.9230 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.9232 0.0397 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.9231 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.9231 0.0422 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.9231 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.9229 0.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.9230 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.9230 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.9231 0.0456 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.9230 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.9229 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.9229 0.0454 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.9231 0.0401 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.9232 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.9232 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.9231 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.9231 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.9230 0.0397 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.9229 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.9226 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.9229 0.0427 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.9230 0.0394 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.9229 0.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.9229 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.9228 0.0397 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.9228 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.9227 0.0396 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.9227 0.0463 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.9229 0.0450 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.9228 0.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.9227 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.9227 0.0398 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.9227 0.0422 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.9229 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.9230 0.0420 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.9232 0.0446 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.9232 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.9231 0.0397 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.9230 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.9882 0.0392 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.9389 0.0409 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.9277 0.0470 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.9200 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.9150 0.0442 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.9104 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.9098 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.9106 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.9130 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.9130 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.9106 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.9081 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.9084 0.0421 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.9108 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.9101 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.9086 0.0446 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.9085 0.0447 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.9106 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.9106 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.9104 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.9099 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.9122 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.9119 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.9110 0.0446 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.9109 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.9103 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.9094 0.0420 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.9093 0.0473 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.9101 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.9104 0.0429 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.9101 0.0451 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.9093 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.9092 0.0444 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.9098 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.9093 0.0395 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.9092 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.9089 0.0396 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.9079 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.9071 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.9063 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.9058 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.9058 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.9052 0.0453 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.9046 0.0423 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.9043 0.0424 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.9030 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.9030 0.0452 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.9025 0.0447 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.9023 0.0447 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.9030 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.9024 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.9029 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.9026 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.9022 0.0434 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.9020 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.9022 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.9022 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.9017 0.0422 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.9012 0.0392 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.9017 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.9015 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.9019 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.9021 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.9020 0.0401 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.9017 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.9019 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.9018 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.9014 0.0395 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.9013 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.9012 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.9014 0.0467 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.9016 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.9018 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.9015 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.9013 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.9016 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.9015 0.0396 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.9016 0.0453 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.9011 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.9010 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.9005 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.9005 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.9001 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.8999 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.8992 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.8989 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.8988 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.8985 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.8981 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.8982 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.8979 0.0451 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.8977 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.8973 0.0423 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.8969 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.8966 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.8964 0.0420 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.8963 0.0456 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.8959 0.0452 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.8955 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.8950 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.8950 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.8950 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.8947 0.0428 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.8945 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.8942 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.8942 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.8941 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.8941 0.0397 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.8942 0.0420 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.8941 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.8940 0.0396 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.8939 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.8937 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.8935 0.0420 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.8933 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.8929 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.8928 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.8927 0.0416 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.8927 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.8926 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.8926 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.8924 0.0396 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.8922 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.8924 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.8923 0.0430 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.8920 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.8920 0.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.8921 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.8920 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.8921 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.8918 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.8916 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.8916 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.8916 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.8916 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.8916 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.8916 0.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.8916 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.8918 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.8916 0.0403 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.8918 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.8918 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.8918 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.8918 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.8916 0.0424 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.8917 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.8917 0.0430 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.8918 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.8918 0.0422 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.8916 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.8916 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.8918 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.8919 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.8919 0.0427 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.8918 0.0426 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.8918 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.8917 0.0448 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.8917 0.0424 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.8914 0.0420 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.8917 0.0401 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.8918 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.8917 0.0445 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.8917 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.8916 0.0470 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.8916 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.8915 0.0448 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.8916 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.8917 0.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.8917 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.8916 0.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.8915 0.0428 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.8915 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.8917 0.0428 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.8919 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.8921 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.8921 0.0400 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.8919 0.0399 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.8919 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.9580 0.0399 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.9108 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.8990 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.8909 0.0394 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.8861 0.0397 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.8809 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.8802 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.8806 0.0426 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.8831 0.0447 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.8833 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.8808 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.8783 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.8785 0.0397 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.8811 0.0395 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.8803 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.8787 0.0394 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.8787 0.0396 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.8809 0.0394 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.8809 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.8807 0.0417 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.8802 0.0439 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.8824 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.8820 0.0453 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.8812 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.8811 0.0396 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.8804 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.8794 0.0432 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.8794 0.0420 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.8802 0.0417 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.8806 0.0432 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.8802 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.8794 0.0453 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.8794 0.0462 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.8799 0.0450 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.8795 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.8794 0.0426 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.8791 0.0417 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.8781 0.0429 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.8773 0.0421 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.8765 0.0395 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.8760 0.0442 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.8760 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.8754 0.0460 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.8748 0.0446 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.8746 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.8733 0.0396 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.8732 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.8727 0.0471 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.8725 0.0426 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.8732 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.8726 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.8732 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.8729 0.0433 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.8725 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.8723 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.8725 0.0447 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.8725 0.0447 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.8721 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.8716 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.8721 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.8719 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.8724 0.0455 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.8726 0.0391 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.8725 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.8722 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.8724 0.0467 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.8724 0.0424 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.8720 0.0428 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.8719 0.0424 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.8718 0.0439 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.8721 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.8722 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.8725 0.0460 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.8722 0.0455 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.8720 0.0419 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.8724 0.0449 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.8722 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.8723 0.0467 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.8718 0.0449 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.8717 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.8713 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.8713 0.0470 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.8708 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.8707 0.0395 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.8701 0.0396 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.8697 0.0394 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.8696 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.8693 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.8689 0.0395 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.8690 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.8687 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.8686 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.8681 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.8677 0.0469 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.8674 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.8673 0.0396 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.8671 0.0429 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.8667 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.8664 0.0395 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.8659 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.8659 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.8659 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.8656 0.0416 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.8654 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.8651 0.0396 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.8651 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.8650 0.0397 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.8651 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.8651 0.0419 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.8651 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.8650 0.0430 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.8649 0.0396 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.8647 0.0394 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.8645 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.8644 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.8640 0.0405 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.8639 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.8638 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.8638 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.8637 0.0424 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.8637 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.8635 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.8633 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.8634 0.0420 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.8634 0.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.8631 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.8631 0.0429 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.8632 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.8631 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.8632 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.8629 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.8627 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.8627 0.0435 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.8627 0.0398 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.8627 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.8628 0.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.8628 0.0437 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.8628 0.0417 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.8630 0.0488 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.8628 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.8631 0.0468 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.8630 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.8630 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.8631 0.0442 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.8629 0.0449 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.8630 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.8630 0.0460 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.8631 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.8631 0.0434 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.8630 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.8629 0.0455 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.8632 0.0447 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.8632 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.8632 0.0416 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.8632 0.0400 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.8631 0.0420 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.8631 0.0452 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.8630 0.0449 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.8628 0.0395 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.8631 0.0396 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.8632 0.0397 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.8631 0.0423 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.8631 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.8630 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.8631 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.8630 0.0445 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.8630 0.0461 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.8632 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.8631 0.0452 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.8630 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.8630 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.8629 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.8631 0.0450 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.8633 0.0403 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.8635 0.0445 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.8635 0.0392 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.8633 0.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.8633 0.0394 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.9286 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.8838 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.8721 0.0391 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.8639 0.0453 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.8593 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.8536 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.8529 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.8529 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.8554 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.8558 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.8533 0.0426 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.8507 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.8509 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.8536 0.0429 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.8528 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.8513 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.8513 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.8534 0.0446 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.8534 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.8533 0.0435 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.8528 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.8549 0.0445 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.8546 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.8538 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.8536 0.0444 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.8529 0.0439 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.8520 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.8520 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.8528 0.0408 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.8533 0.0408 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.8529 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.8520 0.0458 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.8521 0.0465 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.8527 0.0469 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.8522 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.8522 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.8519 0.0429 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.8509 0.0474 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.8500 0.0433 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.8493 0.0396 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.8488 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.8488 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.8482 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.8476 0.0455 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.8474 0.0405 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.8461 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.8461 0.0472 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.8456 0.0420 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.8454 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.8461 0.0396 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.8455 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.8461 0.0396 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.8458 0.0395 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.8455 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.8453 0.0408 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.8455 0.0420 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.8456 0.0426 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.8451 0.0427 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.8447 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.8451 0.0393 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.8449 0.0394 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.8455 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.8457 0.0395 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.8456 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.8454 0.0452 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.8456 0.0396 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.8456 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.8452 0.0400 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.8451 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.8450 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.8453 0.0395 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.8455 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.8458 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.8454 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.8452 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.8456 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.8454 0.0447 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.8455 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.8451 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.8450 0.0428 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.8445 0.0400 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.8446 0.0447 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.8441 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.8440 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.8433 0.0448 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.8430 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.8429 0.0394 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.8426 0.0392 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.8422 0.0447 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.8423 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.8420 0.0430 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.8419 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.8414 0.0395 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.8410 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.8407 0.0450 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.8406 0.0532 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.8405 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.8401 0.0446 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.8397 0.0468 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.8392 0.0521 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.8392 0.0514 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.8392 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.8389 0.0457 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.8388 0.0537 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.8385 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.8385 0.0448 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.8384 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.8385 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.8385 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.8385 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.8384 0.0460 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.8383 0.0439 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.8381 0.0427 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.8380 0.0425 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.8378 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.8374 0.0429 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.8373 0.0474 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.8373 0.0474 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.8372 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.8372 0.0425 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.8371 0.0400 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.8369 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.8367 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.8369 0.0448 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.8368 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.8365 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.8366 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.8367 0.0400 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.8366 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.8366 0.0447 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.8364 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.8362 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.8362 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.8362 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.8362 0.0395 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.8363 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.8363 0.0471 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.8363 0.0400 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.8365 0.0403 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.8364 0.0421 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.8366 0.0441 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.8366 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.8366 0.0452 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.8367 0.0399 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.8365 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.8367 0.0422 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.8367 0.0448 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.8368 0.0395 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.8368 0.0396 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.8366 0.0394 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.8366 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.8368 0.0446 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.8369 0.0446 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.8369 0.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.8368 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.8368 0.0427 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.8368 0.0422 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.8368 0.0473 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.8365 0.0447 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.8368 0.0400 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.8369 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.8368 0.0401 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.8368 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.8368 0.0398 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.8368 0.0393 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.8367 0.0396 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.8367 0.0398 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.8369 0.0395 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.8369 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.8368 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.8367 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.8367 0.0443 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.8369 0.0451 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.8370 0.0421 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.8372 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.8371 0.0397 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.8370 0.0394 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.8370 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.9021 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.8588 0.0396 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.8472 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.8390 0.0425 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.8346 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.8283 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.8274 0.0431 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.8273 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.8297 0.0426 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.8302 0.0397 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.8277 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.8252 0.0402 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.8252 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.8280 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.8272 0.0393 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.8257 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.8257 0.0393 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.8278 0.0402 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.8279 0.0467 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.8279 0.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.8274 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.8293 0.0397 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.8290 0.0397 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.8282 0.0392 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.8281 0.0451 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.8273 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.8264 0.0404 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.8264 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.8273 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.8279 0.0424 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.8274 0.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.8266 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.8267 0.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.8273 0.0402 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.8268 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.8268 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.8264 0.0432 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.8255 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.8246 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.8239 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.8234 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.8234 0.0404 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.8228 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.8222 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.8221 0.0394 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.8208 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.8207 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.8202 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.8201 0.0424 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.8208 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.8202 0.0445 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.8208 0.0446 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.8206 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.8203 0.0437 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.8201 0.0395 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.8203 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.8204 0.0402 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.8200 0.0407 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.8195 0.0397 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.8200 0.0447 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.8198 0.0453 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.8204 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.8206 0.0397 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.8206 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.8204 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.8206 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.8206 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.8202 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.8202 0.0402 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.8201 0.0426 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.8204 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.8206 0.0396 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.8209 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.8205 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.8203 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.8207 0.0446 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.8206 0.0402 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.8207 0.0450 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.8202 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.8202 0.0396 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.8197 0.0396 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.8197 0.0393 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.8192 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.8191 0.0396 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.8185 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.8182 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.8181 0.0392 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.8178 0.0391 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.8174 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.8175 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.8172 0.0422 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.8171 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.8166 0.0396 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.8163 0.0449 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.8160 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.8158 0.0395 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.8157 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.8153 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.8149 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.8145 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.8144 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.8144 0.0471 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.8142 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.8140 0.0452 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.8138 0.0450 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.8138 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.8137 0.0402 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.8137 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.8138 0.0447 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.8137 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.8137 0.0390 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.8136 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.8134 0.0392 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.8133 0.0445 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.8131 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.8127 0.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.8126 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.8126 0.0451 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.8125 0.0397 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.8125 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.8125 0.0395 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.8122 0.0430 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.8120 0.0404 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.8122 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.8122 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.8118 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.8119 0.0435 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.8120 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.8120 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.8120 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.8117 0.0468 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.8115 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.8116 0.0402 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.8116 0.0450 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.8116 0.0448 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.8116 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.8117 0.0407 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.8117 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.8119 0.0472 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.8118 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.8121 0.0448 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.8120 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.8121 0.0449 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.8121 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.8119 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.8121 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.8122 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.8123 0.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.8123 0.0395 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.8121 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.8120 0.0396 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.8123 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.8124 0.0423 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.8124 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.8123 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.8123 0.0447 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.8123 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.8123 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.8120 0.0448 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.8123 0.0445 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.8124 0.0404 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.8123 0.0398 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.8124 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.8123 0.0450 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.8124 0.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.8123 0.0404 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.8123 0.0396 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.8125 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.8125 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.8124 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.8123 0.0392 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.8123 0.0397 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.8124 0.0393 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.8125 0.0427 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.8126 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.8126 0.0400 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.8125 0.0401 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.8125 0.0397 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.8769 0.0397 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.8349 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.8234 0.0400 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.8154 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.8110 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.8043 0.0422 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.8034 0.0398 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.8032 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.8055 0.0427 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.8060 0.0439 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.8035 0.0397 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.8012 0.0450 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.8011 0.0394 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.8039 0.0395 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.8031 0.0399 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.8016 0.0454 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.8017 0.0448 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.8038 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.8038 0.0396 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.8039 0.0450 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.8034 0.0398 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.8052 0.0463 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.8049 0.0424 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.8041 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.8040 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.8032 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.8023 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.8024 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.8033 0.0396 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.8039 0.0402 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.8035 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.8026 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.8028 0.0400 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.8034 0.0445 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.8030 0.0398 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.8029 0.0394 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.8025 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.8017 0.0398 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.8008 0.0450 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.8000 0.0445 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.7995 0.0397 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.7996 0.0402 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.7990 0.0402 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.7984 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.7983 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.7970 0.0464 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.7969 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.7964 0.0422 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.7963 0.0445 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.7970 0.0395 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.7965 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.7971 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.7969 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.7966 0.0424 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.7965 0.0399 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.7966 0.0446 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.7968 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.7964 0.0394 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.7959 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.7963 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.7962 0.0447 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.7968 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.7970 0.0397 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.7971 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.7969 0.0399 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.7971 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.7971 0.0470 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.7968 0.0476 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.7968 0.0398 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.7967 0.0469 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.7970 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.7972 0.0397 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.7975 0.0400 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.7971 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.7970 0.0425 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.7973 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.7972 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.7973 0.0398 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.7968 0.0449 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.7968 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.7963 0.0471 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.7964 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.7959 0.0449 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.7958 0.0396 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.7952 0.0449 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.7949 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.7948 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.7945 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.7941 0.0399 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.7942 0.0448 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.7939 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.7938 0.0430 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.7933 0.0436 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.7930 0.0429 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.7927 0.0395 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.7926 0.0426 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.7924 0.0396 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.7920 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.7917 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.7912 0.0395 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.7912 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.7912 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.7909 0.0395 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.7908 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.7905 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.7905 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.7904 0.0397 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.7905 0.0396 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.7905 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.7905 0.0398 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.7905 0.0400 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.7903 0.0421 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.7902 0.0400 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.7901 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.7899 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.7895 0.0402 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.7894 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.7894 0.0431 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.7893 0.0402 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.7893 0.0397 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.7892 0.0402 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.7890 0.0397 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.7888 0.0399 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.7889 0.0423 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.7889 0.0450 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.7886 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.7887 0.0448 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.7887 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.7887 0.0436 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.7887 0.0400 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.7885 0.0451 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.7883 0.0393 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.7883 0.0402 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.7883 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.7883 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.7884 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.7885 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.7885 0.0448 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.7887 0.0429 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.7886 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.7889 0.0441 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.7889 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.7889 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.7890 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.7888 0.0463 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.7890 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.7890 0.0471 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.7891 0.0404 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.7892 0.0400 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.7890 0.0405 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.7889 0.0432 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.7891 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.7892 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.7892 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.7892 0.0450 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.7892 0.0404 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.7891 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.7891 0.0427 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.7889 0.0400 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.7891 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.7893 0.0399 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.7892 0.0400 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.7892 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.7892 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.7892 0.0423 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.7891 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.7892 0.0398 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.7894 0.0424 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.7893 0.0397 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.7893 0.0400 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.7892 0.0421 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.7891 0.0427 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.7892 0.0399 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.7892 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.7893 0.0421 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.7893 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.7892 0.0401 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.7892 0.0450 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.8525 0.0397 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.8118 0.0404 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.8004 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.7925 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.7883 0.0424 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.7812 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.7803 0.0404 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.7800 0.0398 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.7822 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.7828 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.7804 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.7782 0.0420 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.7781 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.7809 0.0468 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.7802 0.0401 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.7787 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.7788 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.7808 0.0477 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.7809 0.0395 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.7810 0.0396 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.7806 0.0397 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.7822 0.0393 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.7819 0.0398 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.7811 0.0399 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.7810 0.0398 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.7802 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.7793 0.0395 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.7794 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.7803 0.0455 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.7810 0.0460 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.7805 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.7797 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.7800 0.0424 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.7806 0.0404 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.7802 0.0431 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.7801 0.0827 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.7797 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.7788 0.0400 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.7779 0.0401 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.7772 0.0399 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.7767 0.0424 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.7768 0.0423 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.7762 0.0399 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.7756 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.7755 0.0469 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.7743 0.0397 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.7742 0.0401 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.7737 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.7735 0.0423 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.7743 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.7737 0.0399 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.7744 0.0399 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.7742 0.0420 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.7739 0.0397 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.7738 0.0425 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.7740 0.0430 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.7742 0.0404 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.7738 0.0446 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.7733 0.0400 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.7737 0.0400 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.7736 0.0401 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.7742 0.0449 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.7744 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.7745 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.7743 0.0398 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.7745 0.0400 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.7746 0.0399 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.7742 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.7742 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.7741 0.0402 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.7745 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.7747 0.0445 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.7750 0.0399 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.7746 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.7744 0.0397 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.7748 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.7747 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.7747 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.7743 0.0397 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.7743 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.7738 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.7738 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.7734 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.7733 0.0397 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.7727 0.0436 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.7724 0.0448 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.7722 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.7719 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.7715 0.0425 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.7716 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.7714 0.0423 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.7713 0.0452 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.7708 0.0452 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.7705 0.0452 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.7702 0.0461 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.7701 0.0472 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.7699 0.0424 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.7695 0.0400 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.7692 0.0398 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.7687 0.0500 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.7687 0.0450 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.7687 0.0394 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.7685 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.7683 0.0420 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.7681 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.7681 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.7680 0.0404 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.7680 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.7680 0.0404 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.7680 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.7680 0.0426 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.7679 0.0423 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.7677 0.0401 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.7676 0.0394 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.7674 0.0422 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.7671 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.7670 0.0402 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.7670 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.7669 0.0394 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.7669 0.0402 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.7668 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.7666 0.0398 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.7663 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.7665 0.0404 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.7664 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.7661 0.0454 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.7662 0.0405 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.7663 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.7662 0.0423 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.7662 0.0405 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.7660 0.0405 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.7658 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.7659 0.0402 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.7659 0.0397 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.7659 0.0405 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.7659 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.7660 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.7661 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.7662 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.7662 0.0438 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.7664 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.7664 0.0447 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.7665 0.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.7666 0.0397 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.7664 0.0405 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.7666 0.0397 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.7666 0.0419 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.7668 0.0435 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.7668 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.7666 0.0491 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.7665 0.0433 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.7667 0.0430 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.7668 0.0459 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.7668 0.0443 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.7668 0.0469 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.7668 0.0457 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.7668 0.0457 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.7668 0.0440 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.7665 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.7667 0.0437 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.7669 0.0435 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.7668 0.0443 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.7668 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.7668 0.0402 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.7668 0.0420 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.7668 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.7668 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.7670 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.7670 0.0429 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.7669 0.0447 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.7668 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.7667 0.0431 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.7667 0.0428 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.7668 0.0424 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.7669 0.0426 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.7669 0.0474 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.7667 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.7667 0.0402 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.8301 0.0395 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.7906 0.0422 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.7793 0.0398 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.7714 0.0441 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.7673 0.0472 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.7597 0.0396 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.7590 0.0453 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.7584 0.0401 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.7607 0.0396 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.7614 0.0427 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.7589 0.0398 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.7568 0.0401 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.7565 0.0445 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.7593 0.0442 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.7586 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.7571 0.0425 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.7571 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.7591 0.0426 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.7593 0.0448 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.7594 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.7590 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.7602 0.0424 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.7599 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.7592 0.0441 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.7591 0.0474 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.7581 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.7572 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.7574 0.0449 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.7583 0.0422 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.7589 0.0478 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.7585 0.0431 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.7577 0.0475 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.7580 0.0534 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.7586 0.0471 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.7582 0.0477 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.7581 0.0503 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.7577 0.0445 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.7569 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.7559 0.0436 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.7553 0.0424 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.7547 0.0430 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.7548 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.7542 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.7536 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.7536 0.0400 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.7524 0.0425 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.7522 0.0394 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.7517 0.0450 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.7516 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.7523 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.7518 0.0423 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.7525 0.0465 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.7523 0.0424 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.7520 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.7519 0.0454 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.7521 0.0402 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.7524 0.0402 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.7519 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.7515 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.7519 0.0445 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.7518 0.0459 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.7525 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.7527 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.7527 0.0396 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.7525 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.7528 0.0424 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.7528 0.0398 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.7524 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.7525 0.0408 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.7524 0.0398 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.7528 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.7529 0.0447 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.7533 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.7529 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.7527 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.7531 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.7529 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.7530 0.0449 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.7526 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.7526 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.7521 0.0423 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.7521 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.7516 0.0401 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.7516 0.0399 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.7510 0.0427 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.7507 0.0453 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.7506 0.0398 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.7503 0.0402 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.7499 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.7500 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.7497 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.7496 0.0453 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.7492 0.0464 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.7488 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.7485 0.0426 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.7484 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.7483 0.0402 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.7479 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.7476 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.7471 0.0398 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.7471 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.7470 0.0422 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.7468 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.7467 0.0454 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.7465 0.0397 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.7465 0.0401 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.7464 0.0449 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.7463 0.0448 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.7464 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.7464 0.0402 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.7464 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.7462 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.7461 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.7460 0.0401 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.7458 0.0434 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.7455 0.0425 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.7454 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.7453 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.7453 0.0397 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.7452 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.7452 0.0423 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.7449 0.0401 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.7447 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.7448 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.7448 0.0397 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.7444 0.0422 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.7445 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.7446 0.0402 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.7446 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.7445 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.7443 0.0402 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.7441 0.0429 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.7442 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.7442 0.0423 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.7442 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.7443 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.7444 0.0425 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.7444 0.0401 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.7446 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.7445 0.0428 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.7448 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.7448 0.0404 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.7448 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.7449 0.0407 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.7447 0.0448 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.7449 0.0400 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.7449 0.0474 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.7451 0.0399 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.7451 0.0400 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.7450 0.0399 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.7448 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.7450 0.0404 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.7451 0.0405 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.7451 0.0401 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.7451 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.7451 0.0397 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.7451 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.7451 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.7448 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.7450 0.0426 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.7451 0.0455 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.7451 0.0423 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.7451 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.7451 0.0404 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.7451 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.7450 0.0404 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.7451 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.7453 0.0408 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.7453 0.0401 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.7452 0.0403 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.7451 0.0423 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.7449 0.0407 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.7450 0.0399 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.7450 0.0397 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.7451 0.0424 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.7451 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.7449 0.0448 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.7450 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.8082 0.0401 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.7700 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.7585 0.0405 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.7508 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.7466 0.0455 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.7387 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.7380 0.0435 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.7371 0.0399 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.7394 0.0399 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.7400 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.7374 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.7355 0.0442 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.7351 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.7379 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.7371 0.0401 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.7357 0.0403 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.7357 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.7377 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.7378 0.0460 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.7380 0.0405 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.7376 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.7386 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.7383 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.7375 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.7374 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.7364 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.7355 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.7357 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.7366 0.0427 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.7373 0.0464 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.7368 0.0474 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.7360 0.0421 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.7364 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.7370 0.0432 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.7366 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.7366 0.0401 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.7361 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.7353 0.0398 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.7344 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.7337 0.0399 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.7332 0.0398 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.7333 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.7327 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.7321 0.0407 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.7321 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.7310 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.7308 0.0399 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.7303 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.7302 0.0435 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.7309 0.0430 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.7305 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.7312 0.0462 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.7310 0.0448 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.7308 0.0430 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.7306 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.7309 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.7312 0.0421 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.7307 0.0400 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.7303 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.7307 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.7306 0.0458 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.7313 0.0446 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.7316 0.0426 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.7317 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.7315 0.0434 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.7317 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.7317 0.0405 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.7314 0.0449 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.7314 0.0400 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.7314 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.7318 0.0398 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.7319 0.0451 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.7323 0.0396 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.7319 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.7318 0.0452 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.7321 0.0472 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.7320 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.7321 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.7316 0.0404 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.7316 0.0451 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.7311 0.0474 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.7312 0.0398 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.7307 0.0397 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.7307 0.0398 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.7301 0.0395 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.7298 0.0402 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.7297 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.7294 0.0404 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.7290 0.0396 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.7291 0.0403 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.7288 0.0397 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.7287 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.7283 0.0402 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.7280 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.7276 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.7276 0.0402 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.7275 0.0401 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.7271 0.0432 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.7268 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.7263 0.0433 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.7262 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.7262 0.0453 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.7260 0.0405 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.7259 0.0460 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.7256 0.0453 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.7256 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.7255 0.0422 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.7255 0.0399 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.7256 0.0400 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.7256 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.7255 0.0438 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.7254 0.0441 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.7253 0.0440 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.7252 0.0447 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.7250 0.0429 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.7247 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.7246 0.0434 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.7245 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.7245 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.7244 0.0398 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.7244 0.0427 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.7241 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.7238 0.0437 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.7240 0.0515 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.7239 0.0466 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.7236 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.7237 0.0470 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.7237 0.0430 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.7237 0.0431 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.7237 0.0465 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.7235 0.0462 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.7233 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.7234 0.0421 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.7234 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.7234 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.7235 0.0401 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.7235 0.0403 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.7236 0.0434 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.7238 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.7237 0.0403 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.7240 0.0472 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.7240 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.7240 0.0403 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.7241 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.7239 0.0401 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.7241 0.0472 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.7241 0.0448 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.7243 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.7243 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.7242 0.0431 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.7240 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.7242 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.7243 0.0407 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.7243 0.0429 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.7243 0.0475 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.7243 0.0470 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.7243 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.7243 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.7240 0.0422 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.7242 0.0439 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.7243 0.0425 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.7243 0.0405 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.7243 0.0405 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.7243 0.0457 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.7243 0.0404 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.7243 0.0402 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.7243 0.0449 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.7246 0.0476 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.7245 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.7245 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.7243 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.7242 0.0454 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.7242 0.0404 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.7243 0.0400 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.7243 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.7243 0.0399 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.7242 0.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.7242 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.7881 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.7510 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.7392 0.0395 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.7316 0.0402 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.7273 0.0398 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.7190 0.0402 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.7184 0.0450 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.7173 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.7194 0.0420 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.7200 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.7174 0.0403 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.7156 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.7151 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.7178 0.0402 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.7170 0.0402 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.7156 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.7157 0.0429 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.7177 0.0418 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.7178 0.0480 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.7181 0.0426 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.7177 0.0477 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.7186 0.0426 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.7182 0.0466 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.7175 0.0452 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.7174 0.0440 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.7164 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.7154 0.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.7157 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.7166 0.0439 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.7172 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.7167 0.0458 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.7160 0.0470 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.7164 0.0443 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.7170 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.7166 0.0401 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.7166 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.7161 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.7153 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.7143 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.7137 0.0399 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.7132 0.0402 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.7134 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.7127 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.7121 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.7122 0.0475 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.7111 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.7109 0.0402 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.7104 0.0432 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.7103 0.0399 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.7110 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.7106 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.7114 0.0471 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.7111 0.0397 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.7110 0.0427 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.7109 0.0402 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.7111 0.0402 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.7114 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.7110 0.0450 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.7106 0.0407 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.7110 0.0455 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.7109 0.0426 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.7117 0.0401 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.7119 0.0427 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.7120 0.0448 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.7119 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.7121 0.0460 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.7121 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.7118 0.0399 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.7118 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.7118 0.0476 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.7122 0.0405 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.7124 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.7128 0.0397 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.7124 0.0431 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.7123 0.0399 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.7126 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.7125 0.0404 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.7125 0.0399 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.7121 0.0433 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.7121 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.7116 0.0405 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.7117 0.0452 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.7112 0.0420 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.7111 0.0466 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.7106 0.0469 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.7103 0.0429 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.7102 0.0460 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.7099 0.0469 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.7095 0.0420 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.7096 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.7094 0.0456 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.7093 0.0407 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.7089 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.7085 0.0436 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.7082 0.0436 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.7081 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.7080 0.0401 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.7076 0.0403 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.7074 0.0405 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.7069 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.7068 0.0405 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.7068 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.7066 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.7065 0.0402 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.7062 0.0397 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.7062 0.0397 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.7061 0.0404 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.7061 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.7061 0.0456 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.7062 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.7061 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.7060 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.7059 0.0433 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.7058 0.0446 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.7056 0.0404 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.7053 0.0455 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.7052 0.0398 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.7052 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.7051 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.7051 0.0403 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.7050 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.7047 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.7044 0.0454 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.7046 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.7045 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.7042 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.7043 0.0401 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.7043 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.7043 0.0399 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.7043 0.0431 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.7040 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.7039 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.7039 0.0428 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.7040 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.7040 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.7040 0.0444 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.7041 0.0477 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.7042 0.0429 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.7044 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.7043 0.0433 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.7046 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.7046 0.0443 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.7046 0.0401 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.7047 0.0435 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.7046 0.0476 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.7047 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.7048 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.7049 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.7050 0.0455 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.7048 0.0427 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.7046 0.0397 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.7048 0.0407 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.7049 0.0461 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.7049 0.0440 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.7049 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.7049 0.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.7049 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.7049 0.0452 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.7047 0.0456 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.7048 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.7050 0.0437 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.7049 0.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.7050 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.7049 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.7050 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.7049 0.0401 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.7049 0.0406 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.7052 0.0403 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.7052 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.7052 0.0397 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.7050 0.0408 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.7049 0.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.7049 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.7050 0.0429 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.7050 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.7050 0.0400 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.7048 0.0403 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.7049 0.0405 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.7703 0.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.7338 0.0422 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.7216 0.0474 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.7140 0.0465 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.7094 0.0447 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.7009 0.0428 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.7003 0.0444 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.6990 0.0434 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.7009 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.7014 0.0482 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.6987 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.6970 0.0436 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.6965 0.0437 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.6992 0.0429 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.6984 0.0465 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.6970 0.0447 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.6970 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.6990 0.0439 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.6992 0.0475 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.6996 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.6992 0.0444 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.7000 0.0454 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.6996 0.0451 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.6989 0.0406 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.6988 0.0425 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.6978 0.0452 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.6968 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.6971 0.0397 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.6980 0.0400 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.6986 0.0396 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.6981 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.6973 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.6977 0.0432 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.6983 0.0453 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.6981 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.6980 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.6975 0.0479 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.6967 0.0465 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.6957 0.0458 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.6951 0.0442 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.6946 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.6948 0.0432 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.6941 0.0468 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.6935 0.0450 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.6936 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.6925 0.0454 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.6923 0.0447 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.6918 0.0482 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.6917 0.0431 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.6924 0.0457 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.6920 0.0473 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.6928 0.0452 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.6926 0.0420 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.6925 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.6924 0.0400 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.6926 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.6930 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.6926 0.0471 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.6922 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.6926 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.6925 0.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.6933 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.6936 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.6937 0.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.6936 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.6938 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.6939 0.0456 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.6935 0.0429 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.6936 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.6936 0.0420 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.6940 0.0453 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.6942 0.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.6946 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.6943 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.6941 0.0450 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.6944 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.6943 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.6944 0.0449 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.6940 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.6940 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.6934 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.6936 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.6931 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.6931 0.0398 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.6925 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.6922 0.0431 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.6921 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.6918 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.6914 0.0406 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.6915 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.6913 0.0406 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.6912 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.6908 0.0473 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.6905 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.6902 0.0476 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.6901 0.0400 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.6900 0.0423 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.6896 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.6893 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.6888 0.0398 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.6888 0.0474 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.6888 0.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.6886 0.0405 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.6885 0.0424 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.6882 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.6882 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.6881 0.0405 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.6881 0.0425 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.6881 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.6881 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.6881 0.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.6879 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.6878 0.0407 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.6878 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.6876 0.0398 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.6873 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.6872 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.6871 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.6871 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.6870 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.6870 0.0470 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.6867 0.0473 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.6864 0.0400 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.6865 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.6864 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.6861 0.0424 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.6862 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.6862 0.0398 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.6862 0.0405 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.6862 0.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.6859 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.6858 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.6858 0.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.6859 0.0459 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.6859 0.0452 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.6859 0.0406 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.6860 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.6861 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.6863 0.0450 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.6862 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.6865 0.0399 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.6865 0.0439 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.6866 0.0448 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.6866 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.6865 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.6867 0.0428 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.6867 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.6869 0.0405 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.6869 0.0400 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.6868 0.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.6865 0.0396 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.6867 0.0473 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.6868 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.6868 0.0406 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.6868 0.0405 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.6868 0.0453 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.6868 0.0485 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.6869 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.6866 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.6868 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.6869 0.0422 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.6869 0.0400 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.6869 0.0474 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.6869 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.6869 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.6869 0.0434 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.6869 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.6872 0.0473 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.6872 0.0429 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.6871 0.0404 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.6870 0.0423 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.6869 0.0406 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.6869 0.0400 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.6869 0.0402 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.6870 0.0475 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.6869 0.0403 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.6868 0.0426 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.6869 0.0452 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4181 0.0799 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3998 0.0447 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3693 0.0466 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.2976 0.0463 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.1841 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.0772 0.0471 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 3.9842 0.0448 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 3.9035 0.0485 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.8323 0.0485 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.7717 0.0449 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.7176 0.0451 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.6719 0.0454 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.6320 0.0448 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.5981 0.0447 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.5682 0.0538 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.5422 0.0448 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.5181 0.0448 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.4988 0.0477 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.4800 0.0453 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.4610 0.0455 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.4448 0.0480 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.4303 0.0485 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.4163 0.0533 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.4038 0.0504 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.3920 0.0472 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.3819 0.0495 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.3727 0.0495 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.3631 0.0462 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.3546 0.0509 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.3468 0.0526 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.3402 0.0451 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.3330 0.0444 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.3259 0.0443 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.3200 0.0441 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.3138 0.0490 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.3084 0.0540 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.3025 0.0513 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.2971 0.0626 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.2918 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.2869 0.0443 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.2821 0.0461 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.2777 0.0444 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.2734 0.0485 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.2693 0.0464 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.2653 0.0450 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.2618 0.0495 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.2585 0.0547 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.2555 0.0455 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.2526 0.0453 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.2497 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.2468 0.0476 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.2439 0.0468 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.2414 0.0475 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.2386 0.0483 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.2361 0.0484 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.2334 0.0546 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.2310 0.0453 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.2287 0.0457 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.2263 0.0458 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.2243 0.0459 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.2222 0.0453 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.2206 0.0456 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.2191 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.2169 0.0493 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.2149 0.0480 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.2134 0.0467 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.2118 0.0515 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.2096 0.0479 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.2078 0.0505 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.2063 0.0475 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2047 0.0475 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.2035 0.0486 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.2020 0.0499 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.2006 0.0452 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.1994 0.0463 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.1982 0.0462 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.1970 0.0541 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.1957 0.0451 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.1944 0.0451 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.1930 0.0512 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.1917 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.1906 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.1895 0.0474 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.1883 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.1870 0.0453 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.1858 0.0517 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.1846 0.0513 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.1834 0.0547 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.1824 0.0532 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.1814 0.0453 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.1804 0.0517 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.1793 0.0486 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.1783 0.0439 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.1773 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.1761 0.0454 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.1750 0.0462 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.1740 0.0462 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.1729 0.0496 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.1719 0.0448 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.1708 0.0459 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.1697 0.0445 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.1687 0.0465 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.1676 0.0456 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.1664 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.1653 0.0454 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.1642 0.0519 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.1628 0.0453 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.1614 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.1602 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.1586 0.0449 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.1573 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.1560 0.0448 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.1545 0.0484 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.1530 0.0451 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.1514 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.1498 0.0484 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.1482 0.0447 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.1468 0.0457 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.1454 0.0465 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.1438 0.0461 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.1424 0.0462 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.1412 0.0475 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.1397 0.0499 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.1383 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.1367 0.0471 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.1349 0.0461 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.1333 0.0524 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.1317 0.0464 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.1299 0.0555 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.1283 0.0476 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.1267 0.0467 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1249 0.0484 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1232 0.0447 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.1213 0.0477 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.1192 0.0496 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1172 0.0449 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1153 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1133 0.0445 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1115 0.0451 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1095 0.0492 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1075 0.0452 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1054 0.0445 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1033 0.0464 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1011 0.0447 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.0991 0.0459 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.0971 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.0950 0.0440 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.0931 0.0456 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.0908 0.0455 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.0886 0.0450 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.0867 0.0498 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.0848 0.0495 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.0826 0.0490 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.0804 0.0492 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.0781 0.0456 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.0759 0.0454 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.0735 0.0453 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.0712 0.0494 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.0687 0.0441 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.0664 0.0585 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.0642 0.0588 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.0616 0.0442 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.0591 0.0488 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.0567 0.0480 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.0543 0.0438 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.0519 0.0443 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.0495 0.0505 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.0471 0.0459 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.0448 0.0490 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.0422 0.0478 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.0399 0.0453 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.0376 0.0465 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.0355 0.0589 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.0333 0.0441 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.0310 0.0586 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.0286 0.0476 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.0262 0.0591 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.0236 0.0442 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.6312 0.0454 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.5895 0.0449 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.5784 0.0483 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.5753 0.0490 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.5715 0.0503 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.5685 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.5669 0.0462 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.5658 0.0473 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.5648 0.0503 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.5625 0.0576 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.5591 0.0478 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.5580 0.0483 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.5565 0.0460 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.5566 0.0475 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.5551 0.0545 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.5540 0.0468 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.5519 0.0449 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.5523 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.5510 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.5480 0.0446 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.5459 0.0486 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.5457 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.5437 0.0448 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.5420 0.0449 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.5396 0.0611 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.5385 0.0484 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.5367 0.0494 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.5352 0.0445 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.5337 0.0490 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.5324 0.0463 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.5316 0.0452 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.5299 0.0457 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.5279 0.0455 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.5264 0.0451 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.5245 0.0475 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.5231 0.0479 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.5212 0.0450 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.5189 0.0485 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.5170 0.0449 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.5151 0.0473 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.5132 0.0449 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.5113 0.0451 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.5094 0.0448 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.5076 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.5058 0.0455 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.5036 0.0458 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.5024 0.0451 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.5009 0.0569 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.4994 0.0477 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.4985 0.0457 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.4971 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.4957 0.0450 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.4942 0.0500 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.4929 0.0455 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.4914 0.0456 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.4901 0.0507 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.4888 0.0470 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.4873 0.0455 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.4860 0.0454 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.4849 0.0456 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.4836 0.0491 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.4825 0.0462 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.4816 0.0450 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.4803 0.0561 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.4788 0.0459 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.4779 0.0458 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.4768 0.0500 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.4751 0.0561 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.4737 0.0455 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.4727 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.4716 0.0574 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.4707 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.4696 0.0465 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.4683 0.0468 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.4672 0.0471 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.4666 0.0489 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4654 0.0523 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4645 0.0456 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4633 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4622 0.0483 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4610 0.0465 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.4601 0.0549 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.4592 0.0468 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.4580 0.0497 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.4565 0.0451 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.4554 0.0489 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.4544 0.0602 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.4534 0.0475 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.4523 0.0472 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.4514 0.0472 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.4505 0.0470 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.4495 0.0589 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.4485 0.0479 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.4474 0.0471 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.4462 0.0470 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.4451 0.0700 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.4441 0.0463 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.4431 0.0488 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.4422 0.0485 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.4411 0.0458 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.4404 0.0459 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.4395 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.4384 0.0491 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.4374 0.0457 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.4364 0.0457 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.4356 0.0465 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.4346 0.0487 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.4338 0.0456 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.4330 0.0591 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.4319 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.4310 0.0454 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.4303 0.0469 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.4294 0.0485 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.4284 0.0571 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.4275 0.0467 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.4264 0.0458 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.4255 0.0457 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.4247 0.0485 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.4241 0.0471 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.4232 0.0483 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.4226 0.0597 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.4218 0.0474 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.4209 0.0575 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.4201 0.0467 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.4193 0.0605 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.4183 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.4175 0.0508 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.4168 0.0463 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.4159 0.0465 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.4151 0.0594 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.4143 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.4133 0.0602 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.4125 0.0474 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.4119 0.0449 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.4109 0.0483 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.4101 0.0477 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.4093 0.0499 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.4085 0.0470 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.4079 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.4071 0.0494 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.4064 0.0474 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.4055 0.0519 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.4048 0.0479 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.4039 0.0470 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.4031 0.0509 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.4026 0.0477 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.4018 0.0471 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.4012 0.0465 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.4004 0.0471 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.3996 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.3990 0.0568 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.3986 0.0497 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.3979 0.0460 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.3973 0.0596 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.3965 0.0463 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.3958 0.0469 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.3950 0.0476 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.3941 0.0505 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.3933 0.0476 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.3927 0.0472 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.3920 0.0478 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.3911 0.0475 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.3903 0.0478 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.3895 0.0473 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.3888 0.0659 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.3881 0.0460 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.3874 0.0500 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.3868 0.0496 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.3861 0.0598 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.3852 0.0493 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.3846 0.0493 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.3841 0.0481 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.3837 0.0455 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.3833 0.0478 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.3829 0.0595 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.3822 0.0465 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.3814 0.0502 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.3806 0.0597 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.3179 0.0593 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.2663 0.0461 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.2516 0.0503 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.2482 0.0500 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.2466 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.2453 0.0595 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.2456 0.0474 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.2464 0.0466 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.2469 0.0526 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.2460 0.0462 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.2442 0.0480 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.2435 0.0468 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.2434 0.0503 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.2458 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.2460 0.0480 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.2458 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.2450 0.0469 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.2464 0.0472 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.2460 0.0759 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.2443 0.0525 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.2434 0.0468 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.2445 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.2437 0.0568 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.2425 0.0493 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.2415 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.2408 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.2397 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.2392 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.2391 0.0501 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.2388 0.0476 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.2387 0.0480 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.2377 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.2368 0.0499 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.2366 0.0604 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.2357 0.0470 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.2352 0.0485 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.2342 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.2328 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.2316 0.0467 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.2304 0.0462 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.2293 0.0491 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.2284 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.2272 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.2263 0.0439 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.2253 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.2238 0.0461 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.2235 0.0569 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.2227 0.0498 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.2219 0.0491 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.2219 0.0571 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.2209 0.0469 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.2206 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.2196 0.0470 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.2188 0.0480 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.2179 0.0472 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.2173 0.0482 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.2167 0.0467 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.2159 0.0490 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.2154 0.0492 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.2153 0.0470 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.2147 0.0466 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.2144 0.0600 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.2141 0.0596 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.2135 0.0466 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.2127 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.2124 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.2118 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.2109 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.2102 0.0501 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.2098 0.0467 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.2093 0.0597 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.2090 0.0470 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.2087 0.0493 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.2080 0.0578 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.2075 0.0497 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.2074 0.0467 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.2067 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.2064 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.2056 0.0468 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.2050 0.0461 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.2042 0.0461 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.2039 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.2031 0.0463 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.2024 0.0494 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.2013 0.0479 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.2006 0.0568 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.1999 0.0469 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.1993 0.0462 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.1986 0.0490 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.1981 0.0578 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.1974 0.0614 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.1969 0.0641 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.1963 0.0541 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.1956 0.0496 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.1948 0.0542 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.1942 0.0602 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.1937 0.0482 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.1931 0.0540 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.1926 0.0602 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.1919 0.0593 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.1916 0.0603 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.1912 0.0702 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.1905 0.0513 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.1898 0.0510 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.1892 0.0504 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.1887 0.0521 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.1881 0.0541 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.1878 0.0472 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.1874 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.1867 0.0470 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.1863 0.0496 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.1858 0.0487 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.1854 0.0499 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.1848 0.0614 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.1843 0.0479 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.1835 0.0577 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.1830 0.0521 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.1826 0.0479 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.1823 0.0583 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.1819 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.1815 0.0463 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.1811 0.0603 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.1806 0.0492 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.1803 0.0601 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.1799 0.0470 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.1794 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.1789 0.0494 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.1788 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.1784 0.0492 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.1780 0.0477 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.1776 0.0482 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.1770 0.0489 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.1768 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.1764 0.0473 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.1760 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.1756 0.0561 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.1753 0.0481 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.1749 0.0479 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.1748 0.0476 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.1743 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.1741 0.0552 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.1737 0.0484 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.1733 0.0473 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.1729 0.0478 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.1724 0.0467 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.1723 0.0472 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.1720 0.0481 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.1717 0.0486 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.1713 0.0479 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.1708 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.1707 0.0491 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.1706 0.0501 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.1703 0.0508 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.1699 0.0588 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.1695 0.0470 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.1691 0.0483 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.1686 0.0521 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.1682 0.0504 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.1676 0.0484 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.1674 0.0504 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.1670 0.0476 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.1665 0.0488 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.1660 0.0480 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.1656 0.0607 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.1653 0.0483 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.1648 0.0491 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.1645 0.0535 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.1642 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.1638 0.0489 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.1633 0.0505 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.1630 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.1628 0.0495 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.1627 0.0479 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.1626 0.0472 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.1626 0.0488 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.1622 0.0473 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.1616 0.0517 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.1612 0.0484 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.1571 0.0477 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.1040 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.0896 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.0832 0.0529 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.0812 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.0793 0.0493 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.0795 0.0669 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.0800 0.0505 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.0816 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.0817 0.0477 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.0800 0.0532 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.0793 0.0477 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.0797 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.0816 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.0810 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.0804 0.0466 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.0799 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.0815 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.0812 0.0510 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.0801 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.0794 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.0813 0.0574 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.0805 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.0795 0.0481 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.0789 0.0574 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.0783 0.0678 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.0774 0.0521 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.0773 0.0681 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.0778 0.0500 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.0780 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.0779 0.0505 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.0770 0.0492 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.0765 0.0517 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.0769 0.0606 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.0762 0.0542 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.0759 0.0503 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.0753 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.0741 0.0485 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.0732 0.0479 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.0722 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.0713 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.0707 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.0697 0.0574 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.0691 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.0684 0.0600 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.0670 0.0576 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.0669 0.0511 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.0663 0.0509 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.0659 0.0487 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.0662 0.0462 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.0654 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.0654 0.0479 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.0648 0.0488 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.0641 0.0477 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.0634 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.0632 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.0628 0.0487 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.0622 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.0616 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.0616 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.0612 0.0510 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.0612 0.0502 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.0610 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.0606 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.0600 0.0467 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.0600 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.0597 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.0591 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.0585 0.0482 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.0583 0.0501 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.0581 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.0580 0.0575 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.0580 0.0515 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.0575 0.0505 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.0572 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.0574 0.0474 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.0569 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.0567 0.0500 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.0561 0.0504 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.0557 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.0551 0.0599 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.0548 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.0541 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.0536 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.0528 0.0483 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.0522 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.0518 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.0513 0.0577 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.0507 0.0501 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.0504 0.0509 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.0498 0.0501 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.0495 0.0503 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 2.0490 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 2.0484 0.0591 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 2.0479 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 2.0475 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 2.0471 0.0495 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 2.0466 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 2.0462 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 2.0456 0.0500 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 2.0453 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 2.0451 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 2.0445 0.0610 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 2.0440 0.0498 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 2.0436 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 2.0433 0.0504 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 2.0429 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 2.0428 0.0516 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 2.0426 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 2.0420 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 2.0417 0.0575 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 2.0414 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 2.0410 0.0472 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 2.0406 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 2.0402 0.0480 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 2.0396 0.0581 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 2.0392 0.0483 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 2.0389 0.0489 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 2.0388 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 2.0385 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 2.0383 0.0606 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 2.0380 0.0479 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 2.0376 0.0498 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 2.0375 0.0500 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 2.0372 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 2.0366 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 2.0364 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 2.0362 0.0472 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 2.0359 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 2.0357 0.0484 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 2.0353 0.0583 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 2.0349 0.0477 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 2.0348 0.0602 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 2.0346 0.0479 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 2.0343 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 2.0341 0.0497 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 2.0338 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 2.0336 0.0531 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 2.0336 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 2.0333 0.0572 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 2.0333 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 2.0330 0.0503 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 2.0328 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 2.0325 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 2.0322 0.0485 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 2.0322 0.0472 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 2.0320 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 2.0320 0.0503 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 2.0317 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 2.0314 0.0485 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 2.0312 0.0494 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 2.0314 0.0472 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 2.0312 0.0513 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 2.0310 0.0487 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 2.0308 0.0486 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 2.0305 0.0483 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 2.0302 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 2.0299 0.0504 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 2.0294 0.0492 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 2.0294 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 2.0292 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 2.0288 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 2.0286 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 2.0283 0.0495 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 2.0281 0.0503 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 2.0277 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 2.0276 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 2.0274 0.0577 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 2.0272 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 2.0269 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 2.0267 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 2.0266 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 2.0266 0.0484 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 2.0266 0.0488 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 2.0267 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 2.0264 0.0464 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 2.0260 0.0502 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 2.0257 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.0481 0.0514 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.9969 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.9829 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.9759 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.9732 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.9700 0.0486 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.9702 0.0508 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.9706 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.9728 0.0495 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.9735 0.0504 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.9715 0.0496 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.9705 0.0488 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.9711 0.0492 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.9732 0.0500 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.9725 0.0492 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.9716 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.9712 0.0506 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.9730 0.0493 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.9727 0.0473 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.9721 0.0502 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.9713 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.9733 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.9722 0.0611 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.9713 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.9708 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.9700 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.9690 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.9690 0.0577 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.9698 0.0501 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.9701 0.0499 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.9698 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.9688 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.9683 0.0482 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.9688 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.9682 0.0467 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.9677 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.9671 0.0486 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.9659 0.0492 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.9649 0.0497 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.9638 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.9630 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.9625 0.0501 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.9616 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.9609 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.9603 0.0497 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.9589 0.0510 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.9588 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.9583 0.0505 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.9579 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.9584 0.0469 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.9577 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.9580 0.0509 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.9575 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.9570 0.0507 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.9563 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.9562 0.0501 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.9559 0.0487 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.9553 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.9547 0.0469 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.9549 0.0603 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.9544 0.0578 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.9546 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.9546 0.0576 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.9543 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.9539 0.0507 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.9540 0.0490 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.9540 0.0508 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.9534 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.9530 0.0503 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.9529 0.0481 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.9529 0.0483 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.9528 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.9529 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.9525 0.0500 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.9523 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.9525 0.0499 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.9521 0.0480 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.9521 0.0481 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.9515 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.9512 0.0499 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.9507 0.0503 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.9505 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.9498 0.0469 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.9495 0.0500 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.9488 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.9483 0.0617 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.9480 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.9475 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.9470 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.9468 0.0467 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.9464 0.0484 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.9461 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.9456 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.9451 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.9447 0.0482 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.9444 0.0483 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.9442 0.0511 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.9437 0.0502 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.9432 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.9427 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.9425 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.9423 0.0485 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.9419 0.0483 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.9415 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.9412 0.0575 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.9410 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.9408 0.0494 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.9407 0.0519 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.9406 0.0487 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.9402 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.9400 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.9397 0.0480 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.9394 0.0484 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.9391 0.0574 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.9387 0.0451 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.9381 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.9378 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.9376 0.0463 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.9375 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.9373 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.9372 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.9369 0.0527 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.9366 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.9365 0.0466 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.9363 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.9358 0.0482 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.9356 0.0585 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.9355 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.9352 0.0484 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.9350 0.0467 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.9347 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.9344 0.0533 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.9342 0.0481 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.9341 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.9339 0.0487 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.9338 0.0501 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.9336 0.0475 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.9335 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.9336 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.9333 0.0489 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.9334 0.0488 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.9332 0.0480 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.9331 0.0486 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.9329 0.0492 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.9327 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.9328 0.0469 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.9326 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.9327 0.0485 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.9325 0.0602 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.9323 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.9322 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.9324 0.0469 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.9322 0.0483 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.9322 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.9320 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.9318 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.9316 0.0482 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.9314 0.0502 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.9310 0.0481 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.9311 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.9310 0.0580 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.9307 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.9306 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.9303 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.9302 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.9300 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.9298 0.0488 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.9298 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.9297 0.0480 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.9294 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.9293 0.0495 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.9292 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.9293 0.0480 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.9293 0.0582 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.9294 0.0572 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.9292 0.0481 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.9289 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.9287 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.9648 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.9175 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.9040 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.8970 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.8939 0.0500 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.8894 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.8896 0.0490 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.8903 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.8922 0.0470 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.8931 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.8909 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.8899 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.8904 0.0485 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.8927 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.8918 0.0495 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.8908 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.8904 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.8923 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.8920 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.8915 0.0510 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.8907 0.0595 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.8926 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.8915 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.8904 0.0505 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.8900 0.0495 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.8890 0.0509 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.8880 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.8879 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.8889 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.8890 0.0500 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.8887 0.0491 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.8877 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.8873 0.0471 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.8878 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.8872 0.0489 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.8868 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.8861 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.8849 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.8839 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.8828 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.8821 0.0505 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.8817 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.8809 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.8801 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.8797 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.8783 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.8782 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.8776 0.0471 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.8772 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.8778 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.8771 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.8776 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.8772 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.8768 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.8763 0.0576 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.8762 0.0488 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.8761 0.0577 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.8756 0.0576 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.8750 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.8752 0.0507 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.8748 0.0498 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.8752 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.8753 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.8751 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.8748 0.0493 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.8750 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.8751 0.0485 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.8746 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.8744 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.8742 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.8743 0.0515 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.8743 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.8745 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.8741 0.0468 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.8739 0.0495 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.8741 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.8738 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.8738 0.0478 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.8732 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.8730 0.0502 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.8725 0.0495 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.8724 0.0492 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.8718 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.8715 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.8709 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.8704 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.8702 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.8698 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.8692 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.8692 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.8688 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.8686 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.8681 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.8677 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.8673 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.8671 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.8669 0.0510 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.8664 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.8660 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.8654 0.0504 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.8653 0.0508 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.8652 0.0490 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.8648 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.8644 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.8641 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.8640 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.8638 0.0513 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.8639 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.8637 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.8635 0.0484 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.8632 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.8629 0.0501 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.8627 0.0588 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.8624 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.8621 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.8615 0.0484 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.8612 0.0528 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.8611 0.0490 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.8610 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.8608 0.0484 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.8607 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.8604 0.0509 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.8601 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.8601 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.8599 0.0509 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.8594 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.8593 0.0487 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.8592 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.8590 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.8588 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.8585 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.8582 0.0507 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.8581 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.8580 0.0496 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.8578 0.0581 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.8578 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.8577 0.0501 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.8576 0.0492 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.8578 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.8575 0.0484 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.8576 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.8575 0.0515 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.8574 0.0588 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.8573 0.0505 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.8571 0.0593 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.8572 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.8572 0.0505 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.8572 0.0484 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.8572 0.0490 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.8570 0.0489 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.8569 0.0484 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.8571 0.0502 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.8570 0.0494 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.8569 0.0505 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.8568 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.8567 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.8566 0.0495 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.8564 0.0487 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.8560 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.8562 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.8561 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.8559 0.0505 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.8558 0.0497 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.8556 0.0488 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.8555 0.0485 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.8553 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.8552 0.0498 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.8552 0.0484 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.8551 0.0494 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.8549 0.0492 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.8548 0.0688 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.8547 0.0520 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.8548 0.0487 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.8548 0.0493 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.8549 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.8547 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.8544 0.0495 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.8544 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.8982 0.0488 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.8526 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.8394 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.8326 0.0491 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.8292 0.0518 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.8233 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.8237 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.8243 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.8262 0.0499 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.8273 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.8247 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.8236 0.0491 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.8240 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.8262 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.8251 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.8240 0.0492 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.8234 0.0514 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.8256 0.0493 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.8251 0.0490 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.8249 0.0485 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.8241 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.8260 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.8248 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.8237 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.8232 0.0505 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.8221 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.8210 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.8211 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.8221 0.0509 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.8222 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.8219 0.0509 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.8208 0.0475 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.8205 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.8211 0.0506 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.8205 0.0490 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.8201 0.0579 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.8195 0.0578 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.8182 0.0512 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.8170 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.8161 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.8154 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.8153 0.0475 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.8145 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.8137 0.0495 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.8134 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.8122 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.8119 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.8113 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.8109 0.0520 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.8116 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.8110 0.0500 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.8116 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.8112 0.0493 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.8109 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.8104 0.0513 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.8103 0.0585 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.8103 0.0496 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.8098 0.0492 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.8092 0.0507 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.8096 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.8093 0.0493 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.8097 0.0475 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.8100 0.0505 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.8098 0.0515 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.8096 0.0505 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.8098 0.0492 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.8099 0.0507 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.8094 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.8093 0.0543 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.8092 0.0611 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.8094 0.0605 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.8094 0.0498 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.8096 0.0485 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.8092 0.0488 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.8090 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.8092 0.0587 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.8090 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.8090 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.8084 0.0530 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.8083 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.8078 0.0609 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.8077 0.0475 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.8071 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.8070 0.0515 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.8064 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.8059 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.8057 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.8053 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.8048 0.0511 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.8048 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.8045 0.0580 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.8043 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.8038 0.0531 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.8034 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.8031 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.8029 0.0582 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.8027 0.0577 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.8022 0.0524 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.8018 0.0579 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.8013 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.8012 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.8011 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.8007 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.8004 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.8001 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.8000 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.7999 0.0485 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.7999 0.0514 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.7998 0.0485 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.7995 0.0503 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.7993 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.7990 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.7988 0.0488 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.7986 0.0494 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.7982 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.7977 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.7975 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.7973 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.7972 0.0616 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.7971 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.7970 0.0492 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.7967 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.7964 0.0584 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.7964 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.7962 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.7957 0.0515 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.7957 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.7956 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.7954 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.7952 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.7948 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.7945 0.0492 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.7945 0.0499 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.7944 0.0485 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.7943 0.0586 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.7942 0.0581 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.7942 0.0487 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.7941 0.0549 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.7943 0.0513 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.7941 0.0509 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.7942 0.0490 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.7941 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.7940 0.0517 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.7939 0.0489 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.7938 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.7939 0.0495 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.7939 0.0489 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.7940 0.0490 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.7940 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.7938 0.0485 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.7936 0.0511 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.7938 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.7938 0.0544 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.7938 0.0492 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.7937 0.0494 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.7936 0.0485 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.7935 0.0588 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.7933 0.0494 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.7930 0.0499 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.7932 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.7932 0.0508 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.7930 0.0495 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.7929 0.0497 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.7928 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.7927 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.7925 0.0506 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.7924 0.0516 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.7926 0.0596 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.7925 0.0658 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.7923 0.0545 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.7922 0.0511 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.7921 0.0494 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.7921 0.0497 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.7922 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.7922 0.0487 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.7921 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.7918 0.0516 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.7918 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.8404 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.7963 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.7839 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.7772 0.0503 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.7730 0.0601 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.7661 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.7664 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.7668 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.7688 0.0492 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.7696 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.7669 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.7655 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.7659 0.0503 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.7681 0.0581 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.7669 0.0669 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.7656 0.0511 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.7652 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.7674 0.0497 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.7671 0.0531 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.7672 0.0492 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.7665 0.0499 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.7683 0.0483 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.7671 0.0505 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.7661 0.0484 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.7657 0.0484 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.7646 0.0506 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.7635 0.0502 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.7636 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.7647 0.0486 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.7648 0.0485 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.7646 0.0521 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.7635 0.0571 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.7633 0.0610 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.7639 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.7633 0.0573 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.7630 0.0503 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.7624 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.7611 0.0495 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.7599 0.0512 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.7590 0.0510 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.7584 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.7584 0.0488 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.7577 0.0508 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.7569 0.0488 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.7568 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.7556 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.7553 0.0599 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.7547 0.0570 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.7544 0.0545 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.7551 0.0493 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.7545 0.0488 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.7552 0.0588 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.7549 0.0537 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.7546 0.0489 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.7541 0.0519 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.7541 0.0771 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.7542 0.0516 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.7537 0.0502 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.7532 0.0504 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.7536 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.7533 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.7539 0.0529 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.7542 0.0578 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.7541 0.0492 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.7539 0.0484 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.7542 0.0509 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.7542 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.7539 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.7538 0.0511 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.7537 0.0504 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.7540 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.7541 0.0496 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.7544 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.7540 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.7538 0.0472 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.7540 0.0507 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.7538 0.0514 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.7538 0.0514 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.7533 0.0472 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.7532 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.7527 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.7527 0.0522 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.7521 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.7520 0.0509 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.7514 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.7510 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.7508 0.0500 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.7504 0.0472 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.7500 0.0483 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.7500 0.0496 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.7497 0.0580 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.7495 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.7491 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.7488 0.0607 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.7484 0.0575 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.7483 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.7481 0.0490 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.7477 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.7473 0.0507 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.7468 0.0494 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.7468 0.0484 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.7467 0.0505 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.7463 0.0485 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.7460 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.7457 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.7457 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.7456 0.0495 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.7456 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.7455 0.0503 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.7453 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.7451 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.7448 0.0507 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.7446 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.7445 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.7441 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.7437 0.0469 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.7434 0.0521 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.7433 0.0515 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.7433 0.0515 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.7431 0.0602 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.7430 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.7427 0.0539 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.7424 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.7425 0.0472 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.7423 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.7418 0.0483 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.7418 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.7418 0.0483 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.7416 0.0514 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.7414 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.7411 0.0497 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.7408 0.0497 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.7408 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.7407 0.0503 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.7406 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.7406 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.7406 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.7406 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.7408 0.0485 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.7406 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.7408 0.0491 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.7407 0.0489 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.7406 0.0489 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.7406 0.0495 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.7404 0.0483 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.7406 0.0493 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.7406 0.0504 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.7407 0.0503 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.7407 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.7405 0.0489 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.7404 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.7405 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.7405 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.7405 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.7405 0.0500 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.7404 0.0484 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.7403 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.7402 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.7399 0.0505 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.7401 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.7402 0.0483 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.7400 0.0523 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.7400 0.0528 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.7398 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.7398 0.0505 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.7396 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.7396 0.0490 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.7398 0.0520 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.7397 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.7396 0.0494 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.7395 0.0525 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.7393 0.0490 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.7394 0.0499 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.7394 0.0486 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.7395 0.0596 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.7393 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.7391 0.0485 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.7391 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.7917 0.0467 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.7493 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.7379 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.7311 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.7263 0.0516 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.7185 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.7188 0.0613 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.7187 0.0606 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.7208 0.0504 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.7211 0.0510 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.7183 0.0495 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.7167 0.0487 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.7171 0.0527 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.7193 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.7180 0.0489 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.7166 0.0491 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.7164 0.0520 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.7185 0.0493 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.7184 0.0488 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.7188 0.0513 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.7180 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.7197 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.7185 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.7177 0.0513 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.7173 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.7162 0.0530 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.7152 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.7154 0.0530 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.7165 0.0511 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.7166 0.0515 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.7164 0.0504 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.7154 0.0488 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.7154 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.7160 0.0488 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.7155 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.7152 0.0485 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.7147 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.7134 0.0512 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.7121 0.0488 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.7114 0.0489 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.7108 0.0491 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.7110 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.7103 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.7095 0.0517 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.7095 0.0489 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.7083 0.0486 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.7081 0.0508 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.7075 0.0512 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.7072 0.0495 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.7080 0.0491 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.7074 0.0504 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.7082 0.0500 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.7079 0.0488 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.7077 0.0568 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.7073 0.0590 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.7074 0.0612 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.7075 0.0501 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.7070 0.0507 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.7065 0.0505 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.7069 0.0499 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.7067 0.0497 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.7074 0.0502 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.7078 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.7077 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.7075 0.0475 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.7079 0.0512 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.7080 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.7076 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.7076 0.0719 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.7075 0.0489 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.7079 0.0505 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.7080 0.0475 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.7084 0.0475 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.7080 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.7078 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.7080 0.0511 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.7078 0.0475 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.7079 0.0506 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.7074 0.0507 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.7074 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.7069 0.0572 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.7068 0.0499 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.7063 0.0512 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.7062 0.0605 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.7057 0.0511 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.7053 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.7051 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.7048 0.0495 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.7043 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.7044 0.0509 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.7041 0.0597 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.7040 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.7036 0.0510 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.7033 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.7030 0.0502 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.7029 0.0509 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.7028 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.7024 0.0611 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.7021 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.7015 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.7015 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.7014 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.7011 0.0521 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.7009 0.0503 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.7006 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.7006 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.7005 0.0513 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.7006 0.0502 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.7005 0.0519 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.7003 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.7001 0.0511 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.6998 0.0510 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.6997 0.0496 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.6996 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.6992 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.6988 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.6986 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.6985 0.0512 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.6984 0.0498 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.6983 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.6982 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.6979 0.0487 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.6976 0.0524 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.6977 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.6975 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.6971 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.6971 0.0514 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.6971 0.0509 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.6969 0.0485 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.6967 0.0508 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.6964 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.6961 0.0471 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.6961 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.6961 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.6960 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.6960 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.6960 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.6960 0.0503 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.6962 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.6960 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.6963 0.0484 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.6962 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.6961 0.0523 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.6961 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.6960 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.6961 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.6961 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.6963 0.0583 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.6963 0.0574 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.6961 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.6960 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.6961 0.0504 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.6962 0.0506 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.6962 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.6961 0.0603 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.6961 0.0471 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.6961 0.0500 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.6960 0.0507 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.6957 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.6959 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.6960 0.0602 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.6959 0.0510 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.6959 0.0492 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.6958 0.0482 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.6958 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.6956 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.6956 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.6959 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.6958 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.6957 0.0503 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.6956 0.0482 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.6955 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.6955 0.0495 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.6956 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.6956 0.0482 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.6955 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.6953 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.6953 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.7480 0.0469 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.7095 0.0472 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.6998 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.6930 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.6876 0.0605 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.6791 0.0502 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.6794 0.0498 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.6787 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.6807 0.0512 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.6808 0.0504 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.6780 0.0512 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.6763 0.0514 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.6766 0.0603 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.6790 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.6776 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.6761 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.6761 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.6782 0.0488 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.6782 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.6788 0.0492 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.6780 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.6796 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.6784 0.0489 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.6778 0.0497 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.6775 0.0490 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.6764 0.0499 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.6754 0.0521 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.6757 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.6769 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.6770 0.0515 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.6769 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.6759 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.6760 0.0489 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.6766 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.6761 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.6758 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.6753 0.0486 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.6741 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.6728 0.0510 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.6721 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.6715 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.6718 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.6712 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.6705 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.6705 0.0495 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.6694 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.6692 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.6686 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.6683 0.0494 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.6691 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.6686 0.0503 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.6693 0.0513 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.6692 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.6690 0.0499 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.6686 0.0607 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.6687 0.0494 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.6689 0.0511 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.6684 0.1234 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.6679 0.0495 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.6684 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.6682 0.0573 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.6690 0.0524 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.6693 0.0609 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.6693 0.0490 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.6691 0.0481 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.6694 0.0572 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.6696 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.6692 0.0507 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.6692 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.6691 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.6696 0.0508 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.6697 0.0478 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.6701 0.0508 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.6698 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.6695 0.0573 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.6697 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.6696 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.6696 0.0497 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.6691 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.6691 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.6686 0.0579 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.6685 0.0503 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.6680 0.0511 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.6679 0.0577 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.6675 0.0558 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.6671 0.0549 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.6669 0.0685 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.6666 0.0488 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.6661 0.0567 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.6662 0.0495 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.6659 0.0589 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.6658 0.0625 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.6655 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.6652 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.6649 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.6648 0.0574 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.6648 0.0593 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.6643 0.0591 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.6640 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.6635 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.6635 0.0531 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.6634 0.0507 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.6631 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.6629 0.0517 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.6627 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.6627 0.0492 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.6626 0.0489 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.6627 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.6626 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.6625 0.0510 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.6623 0.0496 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.6620 0.0502 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.6619 0.0565 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.6618 0.0506 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.6614 0.0528 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.6611 0.0493 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.6609 0.0514 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.6608 0.0505 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.6608 0.0578 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.6606 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.6606 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.6603 0.0509 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.6599 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.6600 0.0488 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.6599 0.0573 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.6595 0.0614 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.6596 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.6595 0.0486 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.6593 0.0508 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.6592 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.6589 0.0493 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.6586 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.6586 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.6586 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.6585 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.6585 0.0507 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.6586 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.6586 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.6588 0.0501 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.6586 0.0591 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.6589 0.0493 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.6588 0.0502 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.6587 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.6588 0.0516 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.6586 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.6588 0.0535 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.6588 0.0506 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.6590 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.6591 0.0507 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.6589 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.6587 0.0511 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.6589 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.6589 0.0510 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.6589 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.6589 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.6589 0.0490 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.6589 0.0525 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.6589 0.0573 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.6586 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.6588 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.6589 0.0491 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.6588 0.0506 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.6589 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.6588 0.0577 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.6587 0.0493 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.6586 0.0493 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.6586 0.0486 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.6590 0.0486 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.6589 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.6588 0.0501 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.6587 0.0488 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.6586 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.6586 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.6587 0.0487 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.6587 0.0525 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.6586 0.0510 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.6584 0.0581 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.6585 0.0493 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.7113 0.0488 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.6756 0.0520 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.6678 0.0491 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.6611 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.6551 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.6458 0.0495 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.6461 0.0520 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.6451 0.0485 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.6470 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.6469 0.0501 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.6440 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.6423 0.0462 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.6425 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.6450 0.0603 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.6436 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.6420 0.0494 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.6420 0.0590 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.6439 0.0494 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.6440 0.0522 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.6447 0.0487 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.6440 0.0490 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.6455 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.6443 0.0513 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.6438 0.0584 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.6436 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.6425 0.0570 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.6416 0.0513 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.6418 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.6430 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.6432 0.0494 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.6431 0.0496 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.6422 0.0528 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.6424 0.0507 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.6430 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.6426 0.0488 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.6423 0.0495 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.6418 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.6407 0.0514 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.6393 0.0477 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.6387 0.0602 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.6381 0.0534 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.6385 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.6379 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.6372 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.6373 0.0505 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.6362 0.0577 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.6360 0.0488 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.6354 0.0579 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.6352 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.6359 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.6354 0.0578 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.6362 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.6361 0.0505 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.6360 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.6356 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.6357 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.6359 0.0488 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.6354 0.0505 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.6349 0.0473 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.6354 0.0507 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.6354 0.0596 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.6362 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.6365 0.0514 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.6365 0.0498 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.6363 0.0499 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.6366 0.0492 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.6367 0.0497 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.6364 0.0495 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.6364 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.6364 0.0495 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.6368 0.0586 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.6370 0.0579 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.6374 0.0513 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.6371 0.0495 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.6368 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.6370 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.6369 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.6369 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.6364 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.6364 0.0495 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.6359 0.0509 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.6359 0.0493 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.6353 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.6353 0.0487 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.6349 0.0525 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.6345 0.0499 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.6343 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.6340 0.0488 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.6336 0.0492 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.6337 0.0508 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.6334 0.0497 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.6333 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.6330 0.0487 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.6327 0.0507 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.6324 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.6323 0.0491 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.6323 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.6319 0.0605 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.6316 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.6311 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.6310 0.0578 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.6310 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.6307 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.6305 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.6302 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.6302 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.6302 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.6302 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.6302 0.0490 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.6301 0.0499 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.6299 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.6297 0.0508 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.6295 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.6294 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.6291 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.6287 0.0513 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.6286 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.6285 0.0488 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.6285 0.0509 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.6284 0.0521 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.6283 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.6280 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.6276 0.0503 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.6277 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.6276 0.0612 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.6272 0.0537 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.6273 0.0575 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.6273 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.6271 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.6270 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.6266 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.6264 0.0473 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.6264 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.6264 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.6264 0.0512 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.6264 0.0507 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.6265 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.6265 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.6267 0.0522 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.6265 0.0627 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.6268 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.6267 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.6267 0.0513 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.6267 0.0515 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.6266 0.0515 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.6268 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.6268 0.0505 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.6270 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.6271 0.0474 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.6269 0.0487 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.6267 0.0514 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.6269 0.0487 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.6269 0.0491 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.6269 0.0474 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.6269 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.6269 0.0506 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.6269 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.6269 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.6267 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.6268 0.0573 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.6270 0.0502 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.6269 0.0499 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.6269 0.0517 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.6268 0.0529 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.6268 0.0518 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.6267 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.6267 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.6271 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.6270 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.6269 0.0492 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.6268 0.0604 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.6267 0.0477 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.6268 0.0507 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.6268 0.0568 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.6269 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.6268 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.6266 0.0580 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.6266 0.0607 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.6797 0.0531 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.6467 0.0522 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.6396 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.6329 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.6264 0.0512 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.6167 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.6169 0.0483 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.6158 0.0514 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.6177 0.0474 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.6173 0.0483 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.6145 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.6128 0.0491 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.6129 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.6155 0.0473 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.6141 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.6125 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.6125 0.0494 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.6145 0.0578 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.6146 0.0483 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.6154 0.0521 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.6147 0.0515 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.6161 0.0518 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.6150 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.6144 0.0487 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.6144 0.0491 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.6132 0.0506 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.6123 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.6126 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.6138 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.6140 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.6139 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.6130 0.0495 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.6133 0.0516 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.6140 0.0516 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.6136 0.0505 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.6134 0.0521 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.6128 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.6118 0.0513 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.6104 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.6099 0.0580 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.6093 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.6098 0.0520 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.6092 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.6085 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.6086 0.0510 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.6076 0.0487 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.6074 0.0494 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.6068 0.0567 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.6065 0.0488 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.6073 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.6068 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.6075 0.0585 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.6074 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.6074 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.6070 0.0488 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.6072 0.0580 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.6074 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.6070 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.6065 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.6070 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.6070 0.0520 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.6078 0.0488 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.6081 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.6081 0.0490 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.6079 0.0613 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.6082 0.0497 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.6084 0.0483 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.6081 0.0492 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.6080 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.6080 0.0490 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.6085 0.0495 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.6087 0.0518 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.6091 0.0492 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.6088 0.0494 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.6086 0.0508 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.6088 0.0582 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.6087 0.0493 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.6087 0.0515 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.6082 0.0493 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.6081 0.0515 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.6076 0.0513 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.6076 0.0488 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.6070 0.0492 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.6071 0.0511 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.6067 0.0492 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.6064 0.0520 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.6061 0.0513 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.6058 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.6054 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.6055 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.6052 0.0548 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.6052 0.0489 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.6049 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.6046 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.6043 0.0518 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.6042 0.0587 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.6042 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.6038 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.6035 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.6029 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.6029 0.0493 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.6028 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.6026 0.0610 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.6024 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.6021 0.0574 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.6021 0.0513 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.6021 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.6021 0.0505 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.6020 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.6020 0.0518 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.6018 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.6016 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.6014 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.6013 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.6010 0.0511 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.6007 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.6005 0.0506 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.6005 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.6004 0.0515 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.6003 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.6003 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.5999 0.0574 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.5996 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.5997 0.0575 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.5996 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.5992 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.5993 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.5993 0.0604 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.5991 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.5989 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.5986 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.5983 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.5984 0.0504 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.5984 0.0529 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.5983 0.0525 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.5984 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.5985 0.0606 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.5985 0.0501 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.5987 0.0494 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.5986 0.0510 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.5989 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.5988 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.5988 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.5988 0.0483 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.5987 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.5989 0.0512 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.5990 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.5992 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.5992 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.5991 0.0534 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.5988 0.0602 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.5990 0.0512 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.5990 0.0512 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.5990 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.5991 0.0493 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.5991 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.5991 0.0519 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.5991 0.0503 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.5989 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.5990 0.0520 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.5991 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.5991 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.5991 0.0491 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.5990 0.0472 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.5990 0.0490 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.5989 0.0603 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.5990 0.0641 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.5993 0.0488 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.5993 0.0610 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.5992 0.0510 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.5991 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.5989 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.5990 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.5991 0.0519 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.5991 0.0483 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.5990 0.0508 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.5988 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.5989 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.6521 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.6213 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.6143 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.6079 0.0506 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.6010 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.5910 0.0501 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.5913 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.5902 0.0505 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.5919 0.0475 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.5914 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.5886 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.5870 0.0510 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.5870 0.0491 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.5896 0.0513 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.5882 0.0516 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.5866 0.0610 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.5867 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.5886 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.5888 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.5897 0.0573 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.5890 0.0507 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.5902 0.0507 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.5891 0.0490 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.5886 0.0575 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.5886 0.0494 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.5874 0.0496 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.5865 0.0583 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.5867 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.5878 0.0590 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.5881 0.0492 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.5879 0.0523 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.5871 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.5874 0.0512 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.5881 0.0495 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.5876 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.5874 0.0511 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.5869 0.0497 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.5858 0.0508 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.5844 0.0583 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.5840 0.0611 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.5834 0.0488 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.5839 0.0514 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.5834 0.0474 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.5827 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.5828 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.5818 0.0499 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.5816 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.5810 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.5808 0.0491 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.5815 0.0511 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.5810 0.0499 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.5818 0.0476 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.5817 0.0504 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.5817 0.0512 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.5813 0.0494 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.5815 0.0498 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.5817 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.5813 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.5808 0.0491 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.5814 0.0513 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.5814 0.0487 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.5822 0.0506 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.5826 0.0579 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.5825 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.5824 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.5827 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.5829 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.5825 0.0454 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.5825 0.0486 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.5825 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.5830 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.5832 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.5837 0.0490 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.5834 0.0494 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.5832 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.5834 0.0521 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.5833 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.5833 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.5828 0.0492 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.5827 0.0575 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.5822 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.5822 0.0510 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.5816 0.0494 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.5817 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.5813 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.5810 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.5808 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.5805 0.0521 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.5801 0.0500 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.5803 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.5800 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.5799 0.0486 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.5796 0.0490 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.5793 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.5790 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.5790 0.0491 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.5790 0.0473 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.5786 0.0495 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.5783 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.5777 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.5777 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.5776 0.0476 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.5774 0.0492 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.5772 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.5770 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.5770 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.5769 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.5769 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.5769 0.0572 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.5768 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.5766 0.0485 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.5764 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.5762 0.0586 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.5761 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.5758 0.0486 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.5755 0.0587 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.5754 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.5754 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.5753 0.0512 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.5752 0.0491 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.5751 0.0501 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.5748 0.0584 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.5744 0.0490 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.5745 0.0512 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.5744 0.0503 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.5740 0.0498 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.5741 0.0535 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.5741 0.0521 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.5740 0.0562 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.5738 0.0531 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.5734 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.5732 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.5733 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.5733 0.0505 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.5732 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.5733 0.0602 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.5734 0.0513 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.5735 0.0530 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.5736 0.0589 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.5735 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.5738 0.0587 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.5738 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.5737 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.5738 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.5737 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.5739 0.0530 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.5739 0.0486 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.5742 0.0509 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.5742 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.5741 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.5738 0.0523 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.5740 0.0535 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.5741 0.0513 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.5740 0.0539 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.5741 0.0570 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.5741 0.0504 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.5741 0.0601 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.5741 0.0511 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.5739 0.0515 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.5740 0.0492 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.5742 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.5742 0.0510 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.5742 0.0475 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.5741 0.0514 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.5741 0.0509 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.5740 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.5741 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.5744 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.5744 0.0501 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.5743 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.5742 0.0503 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.5741 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.5742 0.0494 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.5742 0.0501 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.5743 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.5742 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.5740 0.0501 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.5740 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.6269 0.0516 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.5980 0.0477 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.5910 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.5851 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.5780 0.0477 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.5677 0.0523 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.5679 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.5669 0.0512 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.5684 0.0476 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.5678 0.0503 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.5651 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.5636 0.0506 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.5635 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.5661 0.0518 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.5647 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.5632 0.0515 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.5633 0.0485 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.5652 0.0473 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.5654 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.5664 0.0488 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.5657 0.0538 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.5669 0.0508 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.5658 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.5654 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.5653 0.0509 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.5642 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.5632 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.5634 0.0476 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.5645 0.0502 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.5647 0.0507 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.5646 0.0501 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.5637 0.0516 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.5641 0.0608 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.5647 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.5643 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.5640 0.0517 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.5634 0.0485 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.5624 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.5610 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.5606 0.0505 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.5600 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.5606 0.0516 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.5601 0.0488 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.5594 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.5595 0.0477 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.5585 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.5583 0.0494 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.5578 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.5575 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.5582 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.5577 0.0486 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.5585 0.0491 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.5584 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.5585 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.5581 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.5583 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.5585 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.5582 0.0509 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.5577 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.5582 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.5583 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.5592 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.5595 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.5594 0.0490 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.5593 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.5596 0.0529 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.5598 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.5595 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.5594 0.0507 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.5595 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.5600 0.0487 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.5602 0.0611 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.5607 0.0513 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.5604 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.5602 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.5604 0.0485 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.5603 0.0613 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.5603 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.5598 0.0522 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.5598 0.0485 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.5593 0.0511 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.5593 0.0507 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.5587 0.0518 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.5588 0.0590 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.5584 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.5582 0.0491 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.5579 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.5577 0.0487 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.5573 0.0511 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.5574 0.0507 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.5571 0.0492 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.5571 0.0493 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.5568 0.0516 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.5565 0.0512 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.5562 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.5562 0.0514 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.5562 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.5558 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.5555 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.5550 0.0521 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.5548 0.0509 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.5548 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.5546 0.0512 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.5544 0.0491 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.5542 0.0542 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.5542 0.0499 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.5541 0.0519 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.5541 0.0581 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.5541 0.0493 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.5541 0.0509 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.5539 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.5536 0.0584 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.5535 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.5533 0.0516 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.5531 0.0520 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.5528 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.5527 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.5526 0.0492 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.5525 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.5524 0.0515 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.5524 0.0498 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.5520 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.5516 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.5517 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.5516 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.5512 0.0507 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.5513 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.5513 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.5512 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.5510 0.0617 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.5506 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.5504 0.0485 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.5505 0.0494 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.5505 0.0488 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.5505 0.0485 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.5505 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.5507 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.5507 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.5508 0.0493 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.5508 0.0519 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.5511 0.0499 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.5511 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.5510 0.0512 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.5511 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.5510 0.0515 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.5512 0.0490 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.5512 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.5515 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.5515 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.5514 0.0493 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.5511 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.5513 0.0513 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.5513 0.0488 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.5513 0.0477 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.5514 0.0516 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.5514 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.5514 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.5514 0.0505 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.5512 0.0579 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.5514 0.0490 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.5515 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.5515 0.0487 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.5515 0.0492 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.5515 0.0497 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.5515 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.5514 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.5514 0.0485 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.5518 0.0610 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.5518 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.5517 0.0511 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.5516 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.5515 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.5516 0.0477 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.5516 0.0487 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.5517 0.0516 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.5516 0.0490 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.5514 0.0598 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.5515 0.0511 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.6037 0.0513 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.5766 0.0516 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.5699 0.0503 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.5645 0.0505 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.5571 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.5467 0.0523 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.5469 0.0503 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.5458 0.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.5472 0.0624 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.5465 0.0494 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.5440 0.0540 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.5425 0.0498 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.5424 0.0494 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.5449 0.0596 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.5436 0.0590 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.5420 0.0514 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.5423 0.0534 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.5441 0.0510 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.5444 0.0532 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.5454 0.0487 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.5447 0.0538 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.5460 0.0509 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.5447 0.0505 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.5444 0.0500 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.5444 0.0508 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.5431 0.0606 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.5421 0.0506 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.5424 0.0504 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.5433 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.5436 0.0483 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.5434 0.0581 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.5424 0.0496 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.5428 0.0522 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.5433 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.5430 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.5426 0.0523 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.5420 0.0479 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.5410 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.5396 0.0527 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.5392 0.0512 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.5386 0.0474 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.5392 0.0487 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.5387 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.5380 0.0532 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.5381 0.0586 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.5371 0.0483 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.5370 0.0515 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.5364 0.0580 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.5362 0.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.5368 0.0522 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.5363 0.0488 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.5371 0.0492 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.5371 0.0502 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.5372 0.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.5368 0.0533 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.5370 0.0510 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.5372 0.0535 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.5369 0.0500 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.5365 0.0505 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.5370 0.0514 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.5371 0.0498 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.5380 0.0500 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.5383 0.0502 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.5383 0.0521 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.5381 0.0492 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.5384 0.0473 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.5386 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.5384 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.5383 0.0495 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.5384 0.0489 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.5389 0.0476 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.5391 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.5396 0.0483 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.5394 0.0493 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.5392 0.0487 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.5394 0.0504 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.5393 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.5393 0.0518 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.5388 0.0513 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.5387 0.0492 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.5382 0.0497 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.5382 0.0512 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.5377 0.0511 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.5378 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.5374 0.0487 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.5372 0.0478 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.5370 0.0508 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.5367 0.0607 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.5364 0.0511 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.5365 0.0481 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.5362 0.0491 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.5362 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.5359 0.0483 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.5357 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.5353 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.5353 0.0604 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.5354 0.0483 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.5349 0.0480 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.5346 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.5342 0.0480 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.5340 0.0521 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.5340 0.0476 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.5338 0.0490 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.5336 0.0582 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.5334 0.0499 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.5334 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.5334 0.0483 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.5334 0.0478 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.5333 0.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.5333 0.0492 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.5331 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.5329 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.5328 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.5326 0.0532 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.5324 0.0496 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.5321 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.5320 0.0481 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.5319 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.5319 0.0512 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.5317 0.0605 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.5317 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.5313 0.0607 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.5309 0.0607 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.5310 0.0490 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.5309 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.5305 0.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.5306 0.0579 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.5306 0.0636 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.5305 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.5303 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.5299 0.0505 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.5297 0.0576 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.5297 0.0505 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.5298 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.5297 0.0481 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.5298 0.0587 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.5300 0.0493 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.5300 0.0500 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.5301 0.0489 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.5300 0.0488 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.5304 0.0491 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.5304 0.0503 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.5303 0.0539 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.5304 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.5303 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.5305 0.0490 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.5305 0.0600 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.5308 0.0528 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.5308 0.0514 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.5307 0.0512 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.5305 0.0528 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.5306 0.0550 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.5307 0.0537 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.5307 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.5307 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.5307 0.0607 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.5308 0.0516 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.5308 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.5306 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.5307 0.0510 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.5309 0.0478 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.5309 0.0483 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.5309 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.5309 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.5309 0.0483 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.5308 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.5309 0.0492 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.5313 0.0503 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.5313 0.0496 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.5312 0.0492 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.5311 0.0499 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.5310 0.0589 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.5311 0.0503 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.5311 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.5312 0.0577 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.5311 0.0488 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.5309 0.0488 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.5310 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.5822 0.0477 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.5570 0.0573 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.5501 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.5452 0.0523 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.5378 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.5273 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.5276 0.0542 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.5265 0.0498 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.5278 0.0522 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.5271 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.5247 0.0595 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.5232 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.5230 0.0516 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.5255 0.0525 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.5242 0.0491 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.5226 0.0583 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.5230 0.0575 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.5246 0.0513 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.5250 0.0496 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.5260 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.5253 0.0488 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.5265 0.0497 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.5253 0.0534 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.5250 0.0521 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.5250 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.5237 0.0527 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.5227 0.0516 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.5229 0.0500 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.5237 0.0488 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.5240 0.0500 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.5238 0.0502 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.5228 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.5232 0.0499 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.5237 0.0518 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.5234 0.0668 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.5229 0.0496 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.5223 0.0536 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.5213 0.0515 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.5199 0.0488 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.5195 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.5190 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.5195 0.0628 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.5190 0.0507 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.5183 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.5185 0.0495 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.5175 0.0498 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.5173 0.0501 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.5167 0.0592 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.5166 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.5172 0.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.5167 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.5175 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.5175 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.5176 0.0600 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.5173 0.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.5174 0.0479 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.5177 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.5174 0.0515 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.5170 0.0514 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.5175 0.0485 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.5176 0.0491 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.5185 0.0518 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.5188 0.0505 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.5188 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.5186 0.0513 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.5189 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.5192 0.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.5189 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.5189 0.0493 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.5189 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.5195 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.5197 0.0594 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.5202 0.0499 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.5200 0.0497 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.5198 0.0511 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.5200 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.5199 0.0536 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.5199 0.0488 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.5194 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.5194 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.5189 0.0525 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.5189 0.0500 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.5183 0.0488 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.5184 0.0512 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.5181 0.0507 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.5179 0.0480 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.5177 0.0515 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.5175 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.5171 0.0480 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.5173 0.0504 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.5170 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.5170 0.0491 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.5167 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.5165 0.0511 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.5161 0.0510 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.5162 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.5162 0.0609 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.5158 0.0479 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.5155 0.0512 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.5150 0.0574 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.5149 0.0499 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.5149 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.5147 0.0604 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.5145 0.0567 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.5143 0.0676 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.5143 0.0519 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.5143 0.0560 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.5143 0.0876 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.5142 0.0642 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.5142 0.0598 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.5141 0.0501 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.5139 0.0513 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.5137 0.0533 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.5136 0.0615 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.5133 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.5130 0.0524 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.5130 0.0575 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.5129 0.0501 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.5128 0.0526 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.5127 0.0507 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.5127 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.5123 0.0485 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.5119 0.0480 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.5120 0.0511 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.5119 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.5115 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.5116 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.5116 0.0488 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.5114 0.0480 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.5112 0.0476 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.5109 0.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.5107 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.5107 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.5107 0.0493 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.5107 0.0605 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.5108 0.0579 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.5110 0.0519 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.5110 0.0485 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.5111 0.0521 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.5111 0.0491 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.5114 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.5114 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.5113 0.0477 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.5115 0.0605 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.5114 0.0493 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.5116 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.5116 0.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.5119 0.0485 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.5119 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.5118 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.5115 0.0513 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.5117 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.5117 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.5118 0.0509 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.5118 0.0511 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.5119 0.0477 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.5119 0.0482 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.5119 0.0518 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.5118 0.0478 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.5119 0.0485 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.5121 0.0480 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.5121 0.0476 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.5121 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.5121 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.5121 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.5120 0.0478 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.5121 0.0478 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.5126 0.0518 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.5126 0.0479 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.5125 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.5124 0.0480 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.5122 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.5124 0.0477 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.5124 0.0513 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.5125 0.0485 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.5124 0.0513 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.5122 0.0477 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.5123 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.5714 0.0480 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.5442 0.0520 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.5353 0.0606 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.5303 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.5222 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.5116 0.0501 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.5118 0.0506 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.5108 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.5118 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.5110 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.5085 0.0480 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.5069 0.0507 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.5065 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.5089 0.0480 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.5076 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.5060 0.0479 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.5063 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.5078 0.0511 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.5081 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.5091 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.5084 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.5096 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.5083 0.0478 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.5081 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.5080 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.5067 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.5056 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.5059 0.0477 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.5066 0.0478 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.5069 0.0518 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.5065 0.0499 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.5056 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.5059 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.5064 0.0517 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.5061 0.0649 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.5056 0.0480 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.5050 0.0476 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.5039 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.5026 0.0478 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.5021 0.0478 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.5016 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.5021 0.0479 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.5016 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.5009 0.0479 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.5010 0.0486 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.5000 0.0495 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.4999 0.0512 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.4993 0.0476 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.4991 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.4997 0.0476 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.4992 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.5000 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.5000 0.0500 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.5001 0.0506 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.4998 0.0497 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.5000 0.0584 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.5003 0.0605 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.5000 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.4995 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.5000 0.0486 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.5002 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.5011 0.0476 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.5014 0.0477 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.5014 0.0509 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.5012 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.5015 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.5017 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.5015 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.5014 0.0511 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.5015 0.0492 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.5020 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.5022 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.5028 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.5025 0.0520 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.5024 0.0496 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.5026 0.0513 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.5024 0.0612 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.5024 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.5019 0.0496 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.5018 0.0506 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.5014 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.5014 0.0492 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.5008 0.0486 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.5009 0.0509 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.5006 0.0486 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.5004 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.5002 0.0496 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.5000 0.0507 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.4997 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.4998 0.0507 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.4995 0.0482 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.4995 0.0557 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.4992 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.4990 0.0499 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.4987 0.0589 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.4987 0.0676 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.4987 0.0504 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.4983 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.4980 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.4976 0.0545 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.4975 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.4974 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.4972 0.0477 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.4971 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.4969 0.0514 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.4969 0.0479 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.4969 0.0576 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.4969 0.0479 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.4968 0.0496 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.4968 0.0511 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.4966 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.4965 0.0565 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.4963 0.0481 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.4962 0.0511 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.4959 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.4956 0.0505 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.4956 0.0486 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.4955 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.4955 0.0477 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.4954 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.4953 0.0582 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.4949 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.4945 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.4946 0.0480 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.4945 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.4941 0.0496 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.4942 0.0512 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.4943 0.0517 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.4941 0.0514 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.4939 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.4935 0.0477 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.4933 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.4934 0.0582 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.4934 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.4934 0.0482 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.4935 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.4937 0.0513 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.4937 0.0519 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.4938 0.0507 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.4938 0.0498 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.4941 0.0593 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.4941 0.0509 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.4941 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.4942 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.4941 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.4943 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.4944 0.0494 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.4946 0.0582 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.4947 0.0532 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.4946 0.0486 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.4943 0.0514 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.4944 0.0512 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.4945 0.0510 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.4945 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.4946 0.0479 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.4946 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.4947 0.0478 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.4947 0.0513 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.4945 0.0584 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.4947 0.0482 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.4949 0.0478 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.4949 0.0482 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.4949 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.4949 0.0512 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.4949 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.4948 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.4949 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.4954 0.0508 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.4954 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.4954 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.4952 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.4951 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.4952 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.4953 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.4953 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.4952 0.0480 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.4950 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.4951 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.5458 0.0516 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.5228 0.0492 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.5145 0.0483 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.5106 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.5028 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.4926 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.4928 0.0516 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.4919 0.0586 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.4932 0.0494 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.4925 0.0503 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.4901 0.0548 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.4888 0.0507 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.4885 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.4909 0.0483 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.4897 0.0478 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.4882 0.0523 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.4886 0.0507 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.4901 0.0479 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.4905 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.4915 0.0505 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.4909 0.0527 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.4920 0.0509 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.4908 0.0518 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.4907 0.0482 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.4907 0.0523 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.4894 0.0528 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.4884 0.0483 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.4887 0.0477 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.4893 0.0486 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.4896 0.0480 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.4892 0.0499 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.4883 0.0486 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.4887 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.4891 0.0491 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.4889 0.0607 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.4884 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.4878 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.4868 0.0475 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.4855 0.0481 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.4850 0.0526 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.4845 0.0503 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.4851 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.4846 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.4839 0.0606 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.4840 0.0514 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.4831 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.4829 0.0478 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.4823 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.4822 0.0528 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.4828 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.4823 0.0602 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.4831 0.0570 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.4831 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.4833 0.0477 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.4830 0.0513 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.4832 0.0553 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.4835 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.4832 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.4828 0.0475 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.4833 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.4834 0.0479 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.4844 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.4847 0.0512 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.4847 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.4845 0.0494 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.4848 0.0507 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.4851 0.0478 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.4848 0.0517 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.4847 0.0609 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.4848 0.0519 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.4854 0.0478 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.4856 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.4862 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.4859 0.0582 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.4857 0.0509 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.4860 0.0508 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.4858 0.0486 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.4858 0.0512 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.4853 0.0501 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.4853 0.0477 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.4848 0.0481 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.4848 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.4843 0.0586 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.4844 0.0524 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.4841 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.4839 0.0624 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.4837 0.0500 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.4835 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.4832 0.0523 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.4833 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.4830 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.4830 0.0497 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.4828 0.0535 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.4825 0.0503 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.4822 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.4822 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.4823 0.0508 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.4819 0.0482 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.4816 0.0614 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.4811 0.0481 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.4810 0.0521 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.4810 0.0494 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.4808 0.0588 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.4807 0.0498 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.4805 0.0517 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.4805 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.4805 0.0510 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.4805 0.0522 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.4804 0.0516 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.4805 0.0525 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.4803 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.4801 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.4800 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.4798 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.4796 0.0615 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.4793 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.4793 0.0520 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.4793 0.0514 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.4792 0.0492 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.4791 0.0580 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.4791 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.4787 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.4783 0.0500 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.4784 0.0509 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.4783 0.0520 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.4779 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.4780 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.4781 0.0505 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.4779 0.0562 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.4777 0.0508 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.4773 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.4771 0.0584 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.4772 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.4772 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.4773 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.4774 0.0510 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.4775 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.4776 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.4777 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.4776 0.0499 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.4780 0.0486 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.4780 0.0514 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.4780 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.4781 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.4780 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.4782 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.4783 0.0511 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.4786 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.4786 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.4785 0.0481 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.4782 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.4783 0.0511 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.4784 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.4785 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.4785 0.0476 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.4786 0.0478 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.4786 0.0515 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.4787 0.0527 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.4785 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.4786 0.0607 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.4788 0.0508 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.4788 0.0503 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.4789 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.4789 0.0480 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.4789 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.4789 0.0521 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.4790 0.0486 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.4794 0.0519 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.4794 0.0510 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.4794 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.4793 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.4791 0.0582 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.4793 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.4793 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.4794 0.0511 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.4793 0.0480 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.4791 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.4792 0.0582 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.5284 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.5070 0.0516 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.4985 0.0575 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.4951 0.0526 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.4875 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.4774 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.4778 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.4769 0.0482 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.4781 0.0549 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.4775 0.0582 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.4750 0.0514 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.4738 0.0508 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.4734 0.0577 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.4759 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.4748 0.0574 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.4732 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.4736 0.0512 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.4751 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.4755 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.4765 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.4760 0.0509 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.4772 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.4760 0.0482 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.4760 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.4759 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.4747 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.4736 0.0480 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.4739 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.4744 0.0509 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.4748 0.0584 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.4743 0.0612 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.4734 0.0580 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.4737 0.0528 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.4741 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.4740 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.4734 0.0650 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.4728 0.0519 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.4718 0.0517 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.4705 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.4700 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.4695 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.4701 0.0498 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.4696 0.0500 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.4689 0.0527 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.4690 0.0483 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.4681 0.0511 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.4679 0.0500 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.4673 0.0483 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.4672 0.0520 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.4678 0.0514 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.4673 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.4681 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.4681 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.4683 0.0480 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.4680 0.0480 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.4682 0.0509 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.4685 0.0519 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.4682 0.0481 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.4678 0.0494 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.4683 0.0494 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.4684 0.0508 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.4694 0.0489 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.4697 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.4697 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.4695 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.4698 0.0515 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.4701 0.0594 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.4698 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.4698 0.0489 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.4698 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.4704 0.0515 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.4706 0.0499 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.4712 0.0515 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.4710 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.4708 0.0479 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.4710 0.0483 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.4709 0.0579 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.4709 0.0483 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.4703 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.4703 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.4698 0.0497 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.4699 0.0510 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.4693 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.4694 0.0521 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.4692 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.4690 0.0611 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.4688 0.0507 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.4686 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.4682 0.0488 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.4684 0.0515 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.4681 0.0610 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.4681 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.4678 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.4676 0.0499 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.4673 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.4673 0.0499 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.4674 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.4669 0.0482 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.4667 0.0479 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.4662 0.0507 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.4662 0.0482 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.4661 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.4660 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.4658 0.0494 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.4656 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.4657 0.0483 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.4656 0.0509 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.4656 0.0513 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.4656 0.0489 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.4656 0.0480 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.4654 0.0482 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.4653 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.4651 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.4650 0.0510 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.4648 0.0489 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.4645 0.0511 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.4645 0.0583 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.4645 0.0478 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.4644 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.4643 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.4643 0.0514 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.4639 0.0516 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.4635 0.0510 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.4636 0.0608 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.4635 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.4631 0.0482 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.4632 0.0514 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.4633 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.4631 0.0479 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.4629 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.4626 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.4624 0.0482 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.4624 0.0516 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.4625 0.0481 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.4625 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.4626 0.0510 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.4628 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.4628 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.4629 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.4629 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.4633 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.4633 0.0579 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.4632 0.0591 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.4634 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.4633 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.4635 0.0501 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.4636 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.4639 0.0549 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.4640 0.0583 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.4639 0.0506 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.4635 0.0528 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.4636 0.0538 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.4637 0.0520 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.4637 0.0530 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.4638 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.4639 0.0507 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.4639 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.4639 0.0543 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.4638 0.0518 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.4639 0.0525 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.4641 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.4641 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.4642 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.4642 0.0483 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.4642 0.0511 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.4642 0.0518 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.4643 0.0510 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.4648 0.0482 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.4648 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.4647 0.0502 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.4647 0.0482 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.4645 0.0481 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.4646 0.0508 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.4647 0.0509 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.4648 0.0517 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.4647 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.4645 0.0581 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.4646 0.0520 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.5104 0.0482 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.4908 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.4823 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.4797 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.4721 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.4623 0.0479 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.4627 0.0549 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.4619 0.0484 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.4631 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.4626 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.4602 0.0497 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.4591 0.0531 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.4588 0.0528 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.4613 0.0595 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.4602 0.0496 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.4587 0.0499 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.4592 0.0517 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.4607 0.0505 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.4611 0.0507 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.4622 0.0506 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.4617 0.0521 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.4629 0.0505 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.4618 0.0499 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.4618 0.0497 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.4618 0.0539 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.4605 0.0585 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.4594 0.0509 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.4598 0.0573 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.4602 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.4606 0.0496 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.4601 0.0598 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.4592 0.0500 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.4596 0.0513 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.4599 0.0500 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.4598 0.0521 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.4592 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.4586 0.0510 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.4576 0.0536 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.4563 0.0502 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.4558 0.0500 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.4553 0.0510 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.4559 0.0546 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.4555 0.0508 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.4548 0.0504 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.4549 0.0589 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.4540 0.0572 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.4538 0.0488 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.4533 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.4532 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.4537 0.0600 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.4533 0.0517 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.4541 0.0613 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.4541 0.0506 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.4543 0.0504 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.4540 0.0498 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.4541 0.0514 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.4544 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.4541 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.4537 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.4543 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.4544 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.4554 0.0499 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.4557 0.0505 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.4557 0.0622 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.4555 0.0499 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.4558 0.0507 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.4561 0.0508 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.4558 0.0496 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.4557 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.4558 0.0499 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.4564 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.4566 0.0498 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.4572 0.0484 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.4569 0.0523 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.4568 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.4570 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.4569 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.4569 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.4563 0.0526 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.4563 0.0514 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.4559 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.4559 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.4554 0.0519 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.4555 0.0500 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.4552 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.4550 0.0506 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.4548 0.0588 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.4546 0.0592 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.4543 0.0508 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.4544 0.0518 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.4541 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.4541 0.0538 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.4539 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.4536 0.0540 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.4533 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.4534 0.0500 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.4535 0.0500 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.4530 0.0500 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.4528 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.4524 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.4523 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.4523 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.4521 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.4520 0.0532 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.4518 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.4518 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.4518 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.4517 0.0617 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.4517 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.4518 0.0513 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.4516 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.4514 0.0507 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.4513 0.0496 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.4512 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.4509 0.0611 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.4507 0.0509 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.4506 0.0528 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.4507 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.4506 0.0506 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.4505 0.0535 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.4505 0.0504 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.4501 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.4497 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.4498 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.4497 0.0504 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.4493 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.4494 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.4495 0.0498 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.4493 0.0514 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.4491 0.0521 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.4488 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.4486 0.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.4487 0.0544 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.4487 0.0497 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.4487 0.0516 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.4488 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.4490 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.4490 0.0554 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.4492 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.4491 0.0485 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.4495 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.4496 0.0498 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.4495 0.0502 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.4497 0.0515 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.4496 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.4498 0.0513 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.4499 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.4502 0.0530 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.4503 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.4502 0.0516 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.4498 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.4499 0.0615 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.4500 0.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.4500 0.0496 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.4501 0.0519 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.4502 0.0496 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.4502 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.4502 0.0502 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.4500 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.4502 0.0482 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.4504 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.4504 0.0502 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.4505 0.0504 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.4505 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.4505 0.0477 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.4504 0.0485 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.4506 0.0498 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.4511 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.4511 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.4510 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.4510 0.0485 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.4508 0.0525 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.4509 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.4509 0.0591 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.4511 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.4509 0.0527 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.4507 0.0517 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.4509 0.0485 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4198 0.0674 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.4115 0.0456 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.4020 0.0537 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.3896 0.0481 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3718 0.0493 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.3432 0.0506 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.2948 0.0541 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.2269 0.0540 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.1532 0.0451 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 4.0861 0.0463 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 4.0249 0.0478 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.9707 0.0465 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.9203 0.0465 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.8754 0.0448 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.8336 0.0473 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.7950 0.0486 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.7591 0.0456 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.7281 0.0486 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.6989 0.0470 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.6704 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.6457 0.0451 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.6227 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.6013 0.0456 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.5820 0.0455 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.5635 0.0459 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.5472 0.0468 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.5322 0.0455 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.5171 0.0461 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.5033 0.0476 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.4906 0.0457 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.4797 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.4683 0.0458 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.4573 0.0491 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4476 0.0471 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4379 0.0464 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.4292 0.0461 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4202 0.0455 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.4117 0.0475 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.4036 0.0475 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.3960 0.0552 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.3886 0.0523 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.3818 0.0450 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.3751 0.0455 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.3687 0.0458 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.3625 0.0473 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.3569 0.0481 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.3517 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.3468 0.0464 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.3420 0.0556 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3374 0.0466 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3328 0.0483 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3282 0.0463 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3241 0.0497 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3198 0.0487 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3159 0.0491 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3118 0.0471 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3081 0.0459 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3045 0.0450 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3009 0.0470 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.2977 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.2945 0.0471 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.2917 0.0495 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.2891 0.0451 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.2859 0.0489 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.2829 0.0456 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.2804 0.0454 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.2779 0.0512 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.2747 0.0474 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.2721 0.0478 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.2698 0.0536 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2674 0.0481 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.2653 0.0503 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.2631 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.2609 0.0478 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.2590 0.0483 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.2571 0.0478 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.2552 0.0474 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.2534 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.2514 0.0521 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2494 0.0470 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2475 0.0502 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2459 0.0556 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2443 0.0470 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2426 0.0473 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2408 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2391 0.0482 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2375 0.0472 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2359 0.0489 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2345 0.0482 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2332 0.0489 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2318 0.0512 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2304 0.0477 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2291 0.0479 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2278 0.0491 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2265 0.0507 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.2252 0.0535 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.2240 0.0697 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.2228 0.0646 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2216 0.0506 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2204 0.0553 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2193 0.0506 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2182 0.0485 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2172 0.0477 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2161 0.0667 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2151 0.0483 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2141 0.0483 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2129 0.0497 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2117 0.0508 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2108 0.0488 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2095 0.0514 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2085 0.0471 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2076 0.0454 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2065 0.0452 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2055 0.0465 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2044 0.0481 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.2035 0.0465 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2025 0.0463 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2018 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2010 0.0465 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2001 0.0506 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.1995 0.0478 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.1987 0.0465 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.1980 0.0472 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.1973 0.0476 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.1965 0.0473 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.1955 0.0455 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.1948 0.0482 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.1941 0.0458 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.1933 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.1926 0.0473 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.1920 0.0478 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1913 0.0458 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1906 0.0461 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.1899 0.0477 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.1889 0.0494 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1881 0.0539 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1873 0.0477 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1865 0.0478 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1859 0.0503 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1853 0.0492 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1846 0.0503 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1838 0.0524 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1831 0.0710 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1823 0.0477 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1817 0.0457 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1811 0.0461 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1805 0.0459 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1801 0.0458 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.1794 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.1788 0.0461 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.1783 0.0526 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.1778 0.0457 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.1772 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.1767 0.0479 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.1760 0.0465 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.1754 0.0511 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.1747 0.0462 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.1741 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.1733 0.0473 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.1726 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.1721 0.0477 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.1713 0.0473 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.1705 0.0467 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.1699 0.0446 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.1693 0.0456 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.1686 0.0464 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.1680 0.0531 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.1674 0.0457 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.1667 0.0459 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.1660 0.0460 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.1654 0.0464 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.1650 0.0500 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.1646 0.0547 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.1642 0.0451 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.1637 0.0464 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.1632 0.0458 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.1625 0.0505 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.1617 0.0480 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 3.0932 0.0454 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 3.0563 0.0460 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 3.0442 0.0498 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 3.0394 0.0489 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 3.0382 0.0460 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 3.0375 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 3.0370 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 3.0362 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 3.0338 0.0492 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 3.0323 0.0482 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 3.0287 0.0549 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 3.0271 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 3.0254 0.0470 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 3.0249 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 3.0236 0.0492 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 3.0222 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 3.0203 0.0483 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 3.0202 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 3.0188 0.0465 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 3.0158 0.0502 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 3.0139 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 3.0124 0.0468 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 3.0103 0.0481 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 3.0084 0.0480 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 3.0060 0.0497 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 3.0045 0.0492 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 3.0031 0.0471 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 3.0008 0.0514 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.9988 0.0469 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.9971 0.0569 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.9961 0.0666 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.9939 0.0474 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.9915 0.0497 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.9896 0.0498 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.9872 0.0518 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.9855 0.0458 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.9829 0.0451 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.9803 0.0445 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.9776 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.9751 0.0496 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.9724 0.0564 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.9698 0.0503 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.9671 0.0468 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.9645 0.0483 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.9620 0.0470 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.9599 0.0515 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.9576 0.0498 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.9556 0.0502 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.9533 0.0569 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.9514 0.0491 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.9489 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.9464 0.0575 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.9440 0.0468 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.9414 0.0460 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.9391 0.0460 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.9364 0.0459 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.9340 0.0463 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.9314 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.9288 0.0456 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.9264 0.0458 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.9239 0.0570 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.9218 0.0476 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.9198 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.9171 0.0451 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.9144 0.0469 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.9122 0.0485 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.9098 0.0479 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.9068 0.0459 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.9040 0.0530 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.9017 0.0482 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.8993 0.0474 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.8971 0.0503 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.8946 0.0565 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.8921 0.0482 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.8899 0.0488 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.8877 0.0478 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.8854 0.0502 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.8831 0.0474 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.8806 0.0488 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.8780 0.0478 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.8757 0.0574 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.8735 0.0566 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.8712 0.0463 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.8687 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.8661 0.0484 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.8636 0.0533 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.8612 0.0455 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.8588 0.0474 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.8566 0.0498 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.8544 0.0493 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.8522 0.0496 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.8500 0.0500 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.8478 0.0477 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.8454 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.8430 0.0476 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.8406 0.0457 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.8386 0.0459 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.8363 0.0475 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.8342 0.0480 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.8320 0.0481 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.8300 0.0520 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.8279 0.0481 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.8257 0.0489 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.8235 0.0611 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.8214 0.0515 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.8194 0.0509 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.8171 0.0482 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.8151 0.0551 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.8131 0.0515 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.8107 0.0596 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.8087 0.0481 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.8069 0.0573 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.8048 0.0585 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.8026 0.0515 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.8006 0.0687 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.7984 0.0474 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.7964 0.0498 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.7946 0.0529 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.7929 0.0468 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.7909 0.0480 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.7893 0.0513 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.7875 0.0486 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.7857 0.0467 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.7841 0.0456 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.7823 0.0470 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.7803 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.7787 0.0458 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.7771 0.0428 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.7753 0.0450 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.7737 0.0489 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.7721 0.0457 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.7703 0.0456 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.7687 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.7671 0.0442 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.7653 0.0480 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.7635 0.0505 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.7619 0.0484 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.7603 0.0483 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.7588 0.0600 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.7572 0.0598 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.7557 0.0530 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.7540 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.7525 0.0487 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.7509 0.0478 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.7495 0.0513 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.7481 0.0481 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.7467 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.7454 0.0462 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.7438 0.0449 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.7423 0.0467 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.7412 0.0468 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.7400 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.7387 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.7373 0.0462 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.7359 0.0452 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.7344 0.0475 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.7329 0.0499 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.7315 0.0459 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.7300 0.0498 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.7286 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.7273 0.0469 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.7257 0.0523 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.7242 0.0483 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.7229 0.0488 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.7216 0.0454 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.7202 0.0470 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.7189 0.0459 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.7176 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.7163 0.0490 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.7149 0.0455 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.7137 0.0489 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.7125 0.0494 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.7115 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.7105 0.0568 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.7095 0.0450 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.7082 0.0570 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.7069 0.0464 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.7056 0.0454 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.5676 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.5069 0.0593 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.4924 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.4870 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.4832 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.4801 0.0451 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.4792 0.0495 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.4792 0.0473 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.4796 0.0474 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.4782 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.4760 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.4758 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.4752 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.4766 0.0504 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.4763 0.0455 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.4762 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.4754 0.0454 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.4768 0.0454 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.4763 0.0498 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.4744 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.4736 0.0453 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.4745 0.0462 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.4738 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.4727 0.0461 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.4717 0.0483 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.4711 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.4705 0.0451 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.4696 0.0456 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.4693 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.4690 0.0504 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.4690 0.0480 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.4681 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.4670 0.0496 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.4664 0.0539 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.4654 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.4651 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.4641 0.0463 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.4627 0.0456 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.4617 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.4606 0.0503 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.4595 0.0598 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.4584 0.0454 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.4573 0.0450 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.4563 0.0577 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.4553 0.0496 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.4539 0.0488 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.4535 0.0456 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.4529 0.0453 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.4521 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.4519 0.0504 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.4511 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.4504 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.4495 0.0451 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.4487 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.4480 0.0456 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.4473 0.0490 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.4466 0.0453 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.4458 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.4451 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.4447 0.0490 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.4440 0.0574 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.4438 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.4434 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.4427 0.0472 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.4418 0.0470 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.4416 0.0504 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.4410 0.0470 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.4400 0.0452 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.4392 0.0493 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.4388 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.4383 0.0491 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.4380 0.0534 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.4375 0.0656 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.4368 0.0615 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.4363 0.0752 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.4363 0.0676 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.4357 0.0594 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.4354 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.4348 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.4342 0.0479 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.4337 0.0463 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.4333 0.0483 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.4327 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.4320 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.4311 0.0467 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.4305 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.4299 0.0572 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.4294 0.0472 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.4288 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.4284 0.0485 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.4279 0.0474 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.4275 0.0469 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.4269 0.0462 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.4262 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.4255 0.0469 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.4249 0.0463 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.4244 0.0462 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.4238 0.0462 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.4233 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.4227 0.0456 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.4224 0.0495 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.4219 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.4212 0.0456 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.4206 0.0451 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.4202 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.4197 0.0496 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.4191 0.0486 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.4188 0.0492 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.4184 0.0466 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.4177 0.0461 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.4173 0.0476 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.4170 0.0504 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.4164 0.0506 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.4158 0.0565 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.4153 0.1015 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.4146 0.0502 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.4141 0.0540 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.4137 0.0505 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.4135 0.0463 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.4131 0.0485 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.4128 0.0468 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.4124 0.0469 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.4120 0.0462 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.4116 0.0507 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.4112 0.0479 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.4106 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.4102 0.0659 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.4100 0.0601 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.4095 0.0493 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.4091 0.0676 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.4087 0.0506 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.4081 0.0477 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.4078 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.4075 0.0485 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.4069 0.0658 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.4065 0.0644 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.4061 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.4057 0.0491 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.4055 0.0505 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.4050 0.0474 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.4047 0.0467 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.4042 0.0466 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.4039 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.4035 0.0477 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.4030 0.0478 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.4029 0.0461 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.4025 0.0468 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.4023 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.4019 0.0523 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.4014 0.0469 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.4013 0.0461 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.4013 0.0487 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.4010 0.0480 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.4007 0.0478 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.4003 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.3999 0.0502 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.3994 0.0468 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.3990 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.3985 0.0605 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.3983 0.0466 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.3979 0.0486 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.3974 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.3969 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.3966 0.0574 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.3962 0.0463 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.3959 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.3955 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.3952 0.0491 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.3949 0.0491 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.3944 0.0480 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.3941 0.0464 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.3939 0.0465 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.3938 0.0480 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.3937 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.3936 0.0578 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.3933 0.0467 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.3928 0.0498 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.3923 0.0460 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.4110 0.0452 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.3515 0.0507 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.3347 0.0459 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.3309 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.3280 0.0486 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.3249 0.0463 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.3242 0.0508 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.3254 0.0579 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.3262 0.0577 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.3256 0.0613 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.3236 0.0607 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.3233 0.0560 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.3228 0.0481 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.3246 0.0457 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.3241 0.0461 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.3241 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.3236 0.0508 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.3252 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.3250 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.3233 0.0479 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.3228 0.0462 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.3242 0.0498 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.3236 0.0486 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.3225 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.3216 0.0476 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.3211 0.0495 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.3204 0.0531 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.3199 0.0519 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.3199 0.0606 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.3197 0.0518 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.3199 0.0662 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.3191 0.0679 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.3182 0.0669 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.3181 0.0503 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.3174 0.0490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.3172 0.0501 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.3166 0.0482 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.3154 0.0496 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.3145 0.0472 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.3135 0.0477 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.3126 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.3118 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.3108 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.3100 0.0467 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.3092 0.0463 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.3078 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.3077 0.0467 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.3072 0.0472 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.3066 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.3066 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.3059 0.0480 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.3057 0.0472 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.3050 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.3044 0.0462 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.3038 0.0599 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.3034 0.0601 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.3030 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.3024 0.0466 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.3019 0.0462 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.3019 0.0462 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.3015 0.0514 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.3013 0.0479 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.3011 0.0489 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.3005 0.0487 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.3000 0.0489 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.2999 0.0498 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.2995 0.0519 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.2987 0.0500 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.2981 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.2979 0.0601 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.2976 0.0512 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.2975 0.0585 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.2973 0.0510 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.2967 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.2963 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.2965 0.0484 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.2961 0.0592 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.2960 0.0504 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.2954 0.0526 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.2950 0.0491 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.2944 0.0665 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.2942 0.0657 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.2937 0.0478 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.2932 0.0590 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.2924 0.0488 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.2919 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.2915 0.0467 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.2911 0.0462 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.2906 0.0484 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.2904 0.0491 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.2900 0.0463 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.2897 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 2.2892 0.0459 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 2.2887 0.0573 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 2.2881 0.0612 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 2.2877 0.0464 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 2.2873 0.0461 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 2.2870 0.0460 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 2.2866 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 2.2861 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 2.2859 0.0466 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 2.2856 0.0495 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 2.2850 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 2.2847 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 2.2842 0.0466 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 2.2839 0.0477 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 2.2835 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 2.2834 0.0479 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 2.2832 0.0487 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 2.2826 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 2.2823 0.0490 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 2.2822 0.0467 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 2.2818 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 2.2814 0.0466 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 2.2811 0.0501 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 2.2805 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 2.2802 0.0481 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 2.2800 0.0467 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 2.2799 0.0466 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 2.2796 0.0467 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 2.2795 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 2.2793 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 2.2789 0.0603 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 2.2788 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 2.2784 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 2.2780 0.0491 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 2.2778 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 2.2776 0.0466 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 2.2773 0.0477 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 2.2770 0.0491 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 2.2767 0.0520 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 2.2763 0.0481 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 2.2760 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 2.2759 0.0528 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 2.2754 0.0483 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 2.2752 0.0559 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 2.2749 0.0492 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 2.2747 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 2.2746 0.0588 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 2.2743 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 2.2742 0.0472 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 2.2738 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 2.2736 0.0467 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 2.2733 0.0544 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 2.2730 0.0601 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 2.2730 0.0492 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 2.2727 0.0577 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 2.2727 0.0489 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 2.2724 0.0482 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 2.2721 0.0484 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 2.2721 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 2.2722 0.0498 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 2.2720 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 2.2718 0.0473 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 689/3560 Training loss: 2.2715 0.0466 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 2.2713 0.0493 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 2.2710 0.0464 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 2.2706 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 2.2702 0.0503 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 2.2702 0.0465 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 2.2700 0.0575 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 2.2696 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 2.2693 0.0467 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 2.2690 0.0495 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 2.2688 0.0492 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 2.2686 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 2.2684 0.0502 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 2.2683 0.0569 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 2.2680 0.0474 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 2.2677 0.0466 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 2.2675 0.0501 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 2.2674 0.0501 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 2.2674 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 2.2675 0.0493 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 2.2675 0.0499 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 2.2673 0.0489 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 2.2669 0.0468 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 2.2666 0.0463 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.3025 0.0464 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 2.2431 0.0463 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 2.2251 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 2.2204 0.0493 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 2.2186 0.0466 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 2.2156 0.0508 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 2.2151 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 2.2163 0.0500 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 2.2176 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 2.2176 0.0467 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 2.2153 0.0504 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 2.2146 0.0600 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 2.2144 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 2.2163 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 2.2160 0.0480 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 2.2157 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 2.2154 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 2.2171 0.0488 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 2.2170 0.0498 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 2.2156 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 2.2151 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 2.2171 0.0596 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 2.2165 0.0612 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 2.2156 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 2.2147 0.0469 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 2.2143 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 2.2136 0.0572 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 2.2134 0.0671 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 2.2138 0.0498 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 2.2138 0.0719 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 2.2140 0.0586 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 2.2133 0.0481 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 2.2126 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 2.2128 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 2.2122 0.0578 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 2.2121 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 2.2116 0.0500 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 2.2105 0.0579 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 2.2096 0.0496 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 2.2087 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 2.2080 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 2.2073 0.0505 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 2.2064 0.0467 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 2.2057 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 2.2049 0.0488 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 2.2036 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 2.2035 0.0535 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 2.2030 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 2.2026 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 2.2028 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 2.2022 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 2.2021 0.0490 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 2.2015 0.0508 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 2.2010 0.0581 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 2.2005 0.0467 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 2.2003 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 2.2001 0.0508 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 2.1995 0.0508 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 2.1991 0.0486 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 2.1993 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 2.1990 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 2.1990 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 2.1989 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 2.1984 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 2.1980 0.0498 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 2.1980 0.0579 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 2.1978 0.0483 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 2.1971 0.0467 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 2.1967 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 2.1965 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 2.1964 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 2.1963 0.0585 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 2.1963 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 2.1958 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 2.1955 0.0467 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 2.1958 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 2.1954 0.0523 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 2.1954 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 2.1949 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 2.1945 0.0572 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 2.1939 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 2.1938 0.0606 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 2.1933 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 2.1928 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 2.1921 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 2.1916 0.0580 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 2.1913 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 2.1909 0.0502 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 2.1905 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 2.1904 0.0606 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 2.1900 0.0491 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 2.1898 0.0611 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 2.1894 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 2.1889 0.0572 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 2.1884 0.0486 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 2.1880 0.0475 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 809/3560 Training loss: 2.1877 0.0487 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 2.1874 0.0469 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 2.1871 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 2.1866 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 2.1865 0.0577 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 2.1863 0.0485 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 2.1858 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 2.1855 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 2.1851 0.0481 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 2.1849 0.0488 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 2.1845 0.0519 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 2.1845 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 2.1844 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 2.1839 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 2.1837 0.0575 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 2.1836 0.0587 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 2.1834 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 2.1831 0.0465 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 2.1828 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 2.1823 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 2.1821 0.0490 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 2.1819 0.0465 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 2.1819 0.0506 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 2.1817 0.0445 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 2.1817 0.0463 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 2.1815 0.0503 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 2.1812 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 2.1811 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 2.1808 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 2.1804 0.0505 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 2.1803 0.0499 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 2.1801 0.0480 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 2.1799 0.0469 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 2.1797 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 2.1795 0.0483 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 2.1791 0.0490 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 2.1789 0.0523 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 2.1788 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 2.1785 0.0503 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 2.1783 0.0500 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 2.1781 0.0515 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 2.1779 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 2.1780 0.0481 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 2.1777 0.0469 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 2.1777 0.0466 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 2.1774 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 2.1773 0.0489 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 2.1770 0.0467 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 2.1768 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 2.1769 0.0473 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 2.1767 0.0489 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 2.1768 0.0595 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 2.1765 0.0546 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 2.1762 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 2.1763 0.0469 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 2.1765 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 2.1764 0.0495 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 2.1763 0.0579 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 2.1761 0.0609 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 2.1759 0.0466 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 2.1756 0.0487 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 2.1754 0.0501 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 2.1751 0.0573 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 2.1752 0.0489 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 2.1751 0.0487 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 2.1748 0.0495 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 2.1745 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 2.1744 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 2.1743 0.0613 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 2.1741 0.0480 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 2.1740 0.0480 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 2.1740 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 2.1738 0.0472 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 2.1736 0.0504 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 2.1735 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 2.1734 0.0490 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 2.1736 0.0477 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 2.1737 0.0479 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 2.1739 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 2.1737 0.0471 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 2.1734 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 2.1732 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 2.2209 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 2.1642 0.0490 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 2.1463 0.0471 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 2.1402 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 2.1386 0.0574 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 2.1357 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 2.1352 0.0490 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 2.1363 0.0605 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 2.1379 0.0496 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 2.1380 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 2.1356 0.0470 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 2.1347 0.0471 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 2.1344 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 2.1363 0.0539 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 2.1361 0.0513 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 2.1354 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 2.1351 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 2.1369 0.0470 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 2.1367 0.0502 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 2.1355 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 2.1349 0.0467 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 2.1372 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 2.1364 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 2.1355 0.0470 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 2.1346 0.0490 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 2.1341 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 2.1334 0.0598 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 2.1333 0.0489 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 2.1339 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 2.1341 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 2.1343 0.0468 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 2.1335 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 2.1329 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 2.1333 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 2.1327 0.0490 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 2.1325 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 2.1322 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 2.1311 0.0473 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 929/3560 Training loss: 2.1302 0.0546 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 2.1293 0.0496 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 2.1286 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 2.1280 0.0572 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 2.1271 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 2.1264 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 2.1257 0.0519 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 2.1243 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 2.1243 0.0466 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 2.1238 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 2.1234 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 2.1237 0.0588 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 2.1231 0.0495 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 2.1232 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 2.1227 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 2.1222 0.0485 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 2.1217 0.0494 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 2.1216 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 2.1215 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 2.1209 0.0467 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 2.1205 0.0576 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 2.1209 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 2.1205 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 2.1206 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 2.1206 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 2.1202 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 2.1199 0.0508 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 2.1200 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 2.1197 0.0468 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 2.1191 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 2.1188 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 2.1187 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 2.1185 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 2.1185 0.0497 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 2.1186 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 2.1181 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 2.1179 0.0504 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 2.1183 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 2.1179 0.0485 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 2.1180 0.0468 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 2.1175 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 2.1171 0.0513 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 2.1165 0.0509 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 2.1164 0.0470 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 2.1159 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 2.1155 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 2.1148 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 2.1144 0.0485 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 2.1141 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 2.1137 0.0501 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 2.1133 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 2.1132 0.0465 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 2.1129 0.0509 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 2.1127 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 2.1123 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 2.1119 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 2.1114 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 2.1111 0.0520 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 2.1109 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 2.1106 0.0586 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 2.1102 0.0489 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 2.1098 0.0493 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 2.1097 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 2.1095 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 2.1091 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 2.1089 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 2.1085 0.0468 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 2.1084 0.0500 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 2.1081 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 2.1081 0.0502 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 2.1080 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 2.1076 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 2.1074 0.0485 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 2.1073 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 2.1072 0.0499 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 2.1069 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 2.1067 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 2.1062 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 2.1060 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 2.1059 0.0472 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 2.1059 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 2.1058 0.0493 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 2.1057 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 2.1055 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 2.1053 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 2.1052 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 2.1050 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 2.1047 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 2.1046 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 2.1045 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 2.1043 0.0601 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 2.1042 0.0506 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 2.1040 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 2.1036 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 2.1035 0.0503 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 2.1034 0.0494 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 2.1032 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 2.1031 0.0513 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 2.1029 0.0542 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 2.1028 0.0491 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 2.1029 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 2.1027 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 2.1027 0.0492 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 2.1025 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 2.1024 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 2.1023 0.0471 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 2.1020 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 2.1022 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 2.1021 0.0490 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 2.1021 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 2.1020 0.0500 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 2.1018 0.0583 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 2.1018 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 2.1021 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 2.1020 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 2.1019 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 2.1018 0.0609 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1046/3560 Training loss: 2.1017 0.0487 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 2.1015 0.0501 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 2.1013 0.0513 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 2.1010 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 2.1012 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 2.1011 0.0507 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 2.1009 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 2.1007 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 2.1006 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 2.1006 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 2.1004 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 2.1004 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 2.1004 0.0499 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 2.1003 0.0585 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 2.1002 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 2.1001 0.0489 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 2.1001 0.0475 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 2.1003 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 2.1005 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 2.1007 0.0482 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 2.1006 0.0583 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 2.1004 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 2.1002 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 2.1564 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 2.0998 0.0500 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 2.0826 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 2.0752 0.0490 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 2.0736 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 2.0703 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 2.0700 0.0468 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 2.0709 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 2.0726 0.0508 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 2.0728 0.0490 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 2.0706 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 2.0695 0.0470 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 2.0694 0.0501 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 2.0716 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 2.0715 0.0593 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 2.0707 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 2.0704 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 2.0724 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 2.0722 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 2.0713 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 2.0707 0.0512 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 2.0733 0.0475 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 2.0724 0.0475 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 2.0715 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 2.0707 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 2.0701 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 2.0693 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 2.0693 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 2.0701 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 2.0703 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 2.0704 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 2.0697 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 2.0692 0.0498 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 2.0697 0.0489 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 2.0691 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 2.0689 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 2.0686 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 2.0676 0.0468 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 2.0667 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 2.0658 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 2.0652 0.0505 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 2.0647 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 2.0639 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 2.0633 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 2.0626 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 2.0611 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 2.0611 0.0467 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 2.0606 0.0574 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 2.0603 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 2.0607 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 2.0601 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 2.0603 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 2.0598 0.0471 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 2.0593 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 2.0589 0.0524 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 2.0589 0.0471 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 2.0588 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 2.0583 0.0488 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 2.0578 0.0487 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 2.0583 0.0518 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 2.0579 0.0469 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 2.0582 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 2.0582 0.0469 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 2.0579 0.0579 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 2.0575 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 2.0577 0.0501 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 2.0576 0.0469 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 2.0570 0.0467 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 2.0567 0.0509 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 2.0566 0.0490 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 2.0565 0.0500 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 2.0565 0.0491 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 2.0567 0.0582 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 2.0563 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 2.0561 0.0514 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 2.0565 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 2.0562 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 2.0562 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 2.0558 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 2.0555 0.0485 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 2.0549 0.0506 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 2.0548 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 2.0543 0.0488 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 2.0539 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 2.0533 0.0518 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 2.0529 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 2.0526 0.0471 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 2.0523 0.0488 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 2.0519 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 2.0518 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 2.0515 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 2.0513 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 2.0509 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 2.0506 0.0525 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 2.0502 0.0509 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 2.0499 0.0485 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1165/3560 Training loss: 2.0497 0.0471 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 2.0494 0.0488 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 2.0491 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 2.0486 0.0486 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 2.0486 0.0475 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 2.0484 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 2.0481 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 2.0479 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 2.0476 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 2.0475 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 2.0472 0.0589 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 2.0472 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 2.0472 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 2.0468 0.0509 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 2.0467 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 2.0466 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 2.0465 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 2.0463 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 2.0460 0.0508 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 2.0456 0.0466 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 2.0454 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 2.0453 0.0578 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 2.0453 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 2.0452 0.0489 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 2.0452 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 2.0450 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 2.0448 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 2.0448 0.0608 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 2.0446 0.0490 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 2.0443 0.0468 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 2.0442 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 2.0442 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 2.0440 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 2.0440 0.0498 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 2.0438 0.0490 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 2.0435 0.0507 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 2.0434 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 2.0433 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 2.0431 0.0488 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 2.0431 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 2.0430 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 2.0429 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 2.0430 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 2.0428 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 2.0429 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 2.0427 0.0470 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 2.0427 0.0598 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 2.0426 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 2.0424 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 2.0425 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 2.0425 0.0504 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 2.0426 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 2.0424 0.0507 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 2.0422 0.0505 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 2.0423 0.0547 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 2.0426 0.0471 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 2.0426 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 2.0425 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 2.0424 0.0467 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 2.0423 0.0530 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 2.0422 0.0469 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 2.0421 0.0487 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 2.0418 0.0472 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 2.0421 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 2.0420 0.0496 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 2.0419 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 2.0417 0.0490 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 2.0417 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 2.0416 0.0469 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 2.0415 0.0508 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 2.0415 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 2.0416 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 2.0415 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 2.0414 0.0495 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 2.0414 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 2.0414 0.0473 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 2.0417 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 2.0420 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 2.0422 0.0492 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 2.0421 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 2.0419 0.0471 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 2.0418 0.0485 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 2.1029 0.0501 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 2.0490 0.0483 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 2.0316 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 2.0238 0.0610 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 2.0220 0.0584 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 2.0185 0.0509 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 2.0181 0.0571 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 2.0191 0.0498 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 2.0207 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 2.0211 0.0572 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 2.0189 0.0502 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 2.0180 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 2.0179 0.0492 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 2.0202 0.0466 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 2.0200 0.0469 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 2.0190 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 2.0188 0.0483 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 2.0209 0.0505 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 2.0207 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 2.0198 0.0472 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 2.0194 0.0496 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 2.0220 0.0487 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 2.0211 0.0616 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 2.0200 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 2.0192 0.0586 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 2.0185 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 2.0177 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 2.0177 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 2.0185 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 2.0188 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 2.0188 0.0508 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 2.0181 0.0468 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 2.0176 0.0488 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 2.0181 0.0470 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1281/3560 Training loss: 2.0176 0.0552 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 2.0174 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 2.0171 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 2.0159 0.0528 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 2.0150 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 2.0142 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 2.0136 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 2.0131 0.0509 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 2.0123 0.0528 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 2.0117 0.0498 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 2.0111 0.0501 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 2.0096 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 2.0096 0.0470 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 2.0091 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 2.0088 0.0577 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 2.0092 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 2.0087 0.0484 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 2.0090 0.0500 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 2.0086 0.0485 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 2.0081 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 2.0077 0.0508 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 2.0077 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 2.0076 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 2.0072 0.0502 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 2.0067 0.0483 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 2.0072 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 2.0069 0.0486 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 2.0071 0.0492 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 2.0073 0.0492 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 2.0071 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 2.0066 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 2.0069 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 2.0068 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 2.0063 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 2.0060 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 2.0058 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 2.0058 0.0570 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 2.0058 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 2.0060 0.0587 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 2.0057 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 2.0055 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 2.0059 0.0505 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 2.0056 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 2.0057 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 2.0052 0.0498 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 2.0050 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 2.0044 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 2.0044 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 2.0039 0.0531 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 2.0035 0.0607 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 2.0029 0.0491 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 2.0025 0.0608 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 2.0023 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 2.0019 0.0509 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 2.0016 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 2.0016 0.0496 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 2.0013 0.0509 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 2.0010 0.0605 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 2.0007 0.0493 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 2.0003 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.9999 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.9997 0.0583 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.9995 0.0495 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.9992 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.9989 0.0505 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.9984 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.9984 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.9983 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.9979 0.0490 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.9977 0.0488 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.9975 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.9974 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.9972 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.9973 0.0493 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.9973 0.0495 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.9969 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.9968 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.9967 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.9966 0.0499 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.9964 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.9962 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.9957 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.9956 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.9955 0.0472 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.9954 0.0485 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.9953 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.9953 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.9951 0.0488 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.9949 0.0507 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.9949 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.9948 0.0503 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.9945 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.9944 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.9944 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.9943 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.9942 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.9941 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.9938 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.9937 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.9937 0.0505 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.9936 0.0470 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.9936 0.0506 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.9935 0.0476 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.9934 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.9936 0.0516 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.9934 0.0501 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.9936 0.0507 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.9934 0.0506 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.9934 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.9933 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.9931 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.9933 0.0506 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.9932 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.9933 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.9933 0.0504 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.9931 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.9932 0.0507 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.9935 0.0609 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.9935 0.0492 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.9934 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.9933 0.0493 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.9932 0.0490 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.9931 0.0532 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.9930 0.0488 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.9928 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.9930 0.0472 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.9930 0.0475 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.9929 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.9928 0.0484 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.9927 0.0587 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.9927 0.0477 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.9925 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.9926 0.0581 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.9927 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.9926 0.0529 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.9925 0.0575 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.9925 0.0608 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.9926 0.0504 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.9929 0.0497 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.9932 0.0474 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.9934 0.0509 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.9933 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.9931 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.9930 0.0502 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 2.0552 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 2.0033 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.9860 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.9779 0.0499 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.9760 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.9723 0.0605 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.9720 0.0501 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.9729 0.0475 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.9745 0.0482 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.9751 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.9731 0.0502 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.9722 0.0485 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.9722 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.9746 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.9744 0.0490 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.9733 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.9732 0.0487 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.9754 0.0500 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.9752 0.0469 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.9746 0.0494 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.9742 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.9770 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.9761 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.9751 0.0496 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.9744 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.9736 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.9727 0.0507 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.9727 0.0581 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.9736 0.0491 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.9739 0.0482 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.9739 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.9731 0.0502 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.9728 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.9734 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.9728 0.0505 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.9727 0.0506 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.9723 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.9712 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.9702 0.0486 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.9694 0.0484 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.9688 0.0485 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.9684 0.0475 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.9677 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.9670 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.9665 0.0491 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.9650 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.9650 0.0471 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.9645 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.9642 0.0605 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.9647 0.0484 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.9642 0.0504 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.9646 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.9642 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.9637 0.0498 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.9634 0.0504 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.9634 0.0499 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.9634 0.0471 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.9629 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.9624 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.9630 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.9626 0.0489 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.9630 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.9632 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.9630 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.9626 0.0580 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.9629 0.0482 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.9629 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.9624 0.0567 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.9621 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.9620 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.9620 0.0497 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.9620 0.0508 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.9622 0.0475 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.9619 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.9617 0.0540 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.9621 0.0491 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.9618 0.0467 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.9620 0.0484 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.9615 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.9613 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.9608 0.0496 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.9608 0.0471 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.9603 0.0502 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.9599 0.0485 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.9594 0.0489 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.9590 0.0499 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.9588 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.9585 0.0485 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.9581 0.0610 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.9581 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.9579 0.0484 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.9576 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.9573 0.0485 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.9569 0.0618 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.9566 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.9563 0.0503 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.9561 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.9559 0.0600 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.9555 0.0602 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.9551 0.0511 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.9550 0.0579 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.9549 0.0508 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.9546 0.0470 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.9545 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.9542 0.0533 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.9542 0.0505 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.9540 0.0471 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.9541 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.9540 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.9538 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.9537 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.9535 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.9534 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.9532 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.9530 0.0500 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.9526 0.0584 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.9524 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.9523 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.9523 0.0471 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.9522 0.0487 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.9522 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.9520 0.0490 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.9518 0.0493 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.9518 0.0511 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.9517 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.9514 0.0573 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.9514 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.9514 0.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.9512 0.0470 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.9512 0.0508 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.9510 0.0493 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.9507 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.9507 0.0518 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.9507 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.9505 0.0495 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.9506 0.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.9505 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.9505 0.0494 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.9506 0.0475 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.9505 0.0483 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.9507 0.0495 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.9505 0.0577 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.9505 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.9504 0.0489 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.9503 0.0476 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.9504 0.0486 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.9504 0.0471 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.9505 0.0485 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.9505 0.0578 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.9503 0.0502 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.9503 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.9507 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.9507 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.9507 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.9506 0.0475 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.9505 0.0527 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.9504 0.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.9503 0.0580 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.9501 0.0486 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.9503 0.0470 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.9504 0.0588 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.9502 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.9501 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.9500 0.0580 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.9500 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.9499 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.9500 0.0505 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.9501 0.0501 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.9501 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.9500 0.0597 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.9500 0.0486 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.9501 0.0473 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.9504 0.0577 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.9507 0.0478 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.9510 0.0512 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.9509 0.0482 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.9507 0.0482 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.9506 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 2.0143 0.0472 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.9645 0.0511 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.9474 0.0492 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.9390 0.0487 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.9369 0.0584 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.9326 0.0503 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.9324 0.0497 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.9332 0.0487 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.9346 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.9352 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.9333 0.0489 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.9323 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.9324 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.9348 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.9345 0.0472 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.9334 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.9333 0.0496 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.9355 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.9354 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.9349 0.0490 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.9345 0.0481 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.9373 0.0508 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.9364 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.9356 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.9348 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.9339 0.0481 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.9330 0.0650 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.9331 0.0727 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.9339 0.0494 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.9343 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.9342 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.9334 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.9331 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.9338 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.9332 0.0601 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.9331 0.0504 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.9327 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.9316 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.9306 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.9298 0.0481 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.9292 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.9289 0.0503 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.9281 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.9275 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.9270 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.9255 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.9255 0.0489 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.9250 0.0481 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.9247 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.9253 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.9248 0.0510 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.9253 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.9249 0.0487 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.9245 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.9242 0.0481 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.9242 0.0506 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.9242 0.0586 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.9238 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.9233 0.0504 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.9239 0.0489 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.9235 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.9240 0.0507 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.9242 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.9241 0.0500 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.9237 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.9241 0.0488 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.9241 0.0519 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.9235 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.9233 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.9232 0.0588 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.9232 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.9232 0.0496 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.9235 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.9232 0.0472 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.9230 0.0502 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.9234 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.9231 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.9233 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.9229 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.9227 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.9221 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.9222 0.0581 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.9217 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.9214 0.0472 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.9208 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.9205 0.0486 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.9204 0.0516 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.9200 0.0504 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.9196 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.9196 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.9194 0.0613 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.9192 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.9188 0.0481 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.9185 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.9181 0.0487 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.9179 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.9177 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.9175 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.9171 0.0472 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.9167 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.9166 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.9166 0.0500 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.9163 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.9161 0.0471 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.9158 0.0471 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.9158 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.9157 0.0586 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.9158 0.0472 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.9157 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.9155 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.9154 0.0470 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.9152 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.9151 0.0551 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.9150 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.9147 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.9143 0.0494 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.9142 0.0470 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.9141 0.0530 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.9140 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.9140 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.9139 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.9137 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.9136 0.0489 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.9136 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.9135 0.0577 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.9131 0.0503 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.9132 0.0510 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.9131 0.0633 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.9130 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.9130 0.0487 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.9128 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.9125 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.9125 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.9125 0.0481 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.9124 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.9124 0.0470 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.9124 0.0599 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.9124 0.0511 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.9125 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.9124 0.0504 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.9126 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.9125 0.0478 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.9125 0.0491 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.9124 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.9123 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.9124 0.0479 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.9124 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.9125 0.0634 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.9125 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.9123 0.0587 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.9124 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.9127 0.0580 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.9127 0.0603 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.9127 0.0580 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.9126 0.0543 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.9126 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.9125 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.9124 0.0510 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.9122 0.0510 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.9125 0.0513 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.9125 0.0606 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.9124 0.0478 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.9123 0.0474 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.9122 0.0469 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.9122 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.9121 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.9122 0.0595 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.9124 0.0486 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.9123 0.0475 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.9122 0.0472 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.9122 0.0476 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.9123 0.0498 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.9127 0.0489 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.9130 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.9133 0.0473 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.9132 0.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.9130 0.0506 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.9129 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.9770 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.9287 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.9123 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.9037 0.0493 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.9013 0.0493 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.8965 0.0519 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.8963 0.0507 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.8970 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.8985 0.0493 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.8990 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.8972 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.8961 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.8964 0.0473 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.8987 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.8984 0.0482 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.8973 0.0477 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.8972 0.0477 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.8995 0.0506 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.8994 0.0596 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.8990 0.0581 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.8987 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.9017 0.0471 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.9008 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.9000 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.8993 0.0506 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.8984 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.8974 0.0488 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.8975 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.8983 0.0524 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.8987 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.8986 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.8978 0.0470 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.8976 0.0498 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.8983 0.0505 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.8977 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.8977 0.0504 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.8973 0.0496 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.8961 0.0503 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.8951 0.0518 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.8943 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.8936 0.0474 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.8934 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.8927 0.0505 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.8921 0.0516 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.8917 0.0501 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.8902 0.0477 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.8902 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.8897 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.8894 0.0510 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.8901 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.8896 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.8901 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.8899 0.0578 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.8895 0.0508 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.8891 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.8892 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.8892 0.0495 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.8888 0.0504 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.8883 0.0607 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.8889 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.8887 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.8891 0.0500 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.8894 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.8893 0.0493 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.8890 0.0509 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.8894 0.0507 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.8894 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.8889 0.0594 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.8887 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.8886 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.8887 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.8887 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.8890 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.8887 0.0512 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.8885 0.0477 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.8889 0.0482 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.8886 0.0488 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.8888 0.0478 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.8884 0.0492 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.8882 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.8877 0.0482 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.8878 0.0479 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.8873 0.0501 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.8871 0.0503 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.8865 0.0473 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.8862 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.8861 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.8857 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.8853 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.8854 0.0474 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.8851 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.8849 0.0506 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.8845 0.0509 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.8842 0.0499 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.8839 0.0501 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.8837 0.0601 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.8835 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.8833 0.0472 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.8829 0.0506 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.8825 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.8825 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.8824 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.8821 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.8819 0.0584 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.8817 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.8817 0.0473 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.8815 0.0604 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.8817 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.8816 0.0510 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.8814 0.0472 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.8813 0.0472 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.8811 0.0500 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.8810 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.8809 0.0505 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.8807 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.8802 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.8801 0.0482 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.8800 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.8800 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.8799 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.8799 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.8797 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.8795 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.8795 0.0511 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.8794 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.8791 0.0507 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.8791 0.0493 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.8791 0.0529 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.8790 0.0503 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.8790 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.8788 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.8785 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.8785 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.8785 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.8784 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.8785 0.0472 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.8785 0.0583 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.8785 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.8786 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.8786 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.8787 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.8787 0.0606 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.8786 0.0509 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.8786 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.8785 0.0483 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.8787 0.0474 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.8786 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.8788 0.0475 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.8788 0.0513 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.8786 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.8786 0.0473 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.8790 0.0583 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.8790 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.8790 0.0486 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.8789 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.8789 0.0473 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.8788 0.0485 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.8788 0.0491 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.8785 0.0492 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.8788 0.0482 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.8789 0.0502 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.8787 0.0514 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.8787 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.8786 0.0492 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.8786 0.0477 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.8785 0.0474 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.8786 0.0491 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.8788 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.8788 0.0481 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.8787 0.0489 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.8787 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.8788 0.0477 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.8791 0.0501 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.8795 0.0578 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.8798 0.0476 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.8797 0.0508 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.8795 0.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.8795 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.9429 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.8970 0.0498 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.8812 0.0505 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.8724 0.0506 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.8698 0.0483 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.8645 0.0497 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.8643 0.0470 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.8649 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.8665 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.8669 0.0569 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.8652 0.0513 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.8641 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.8643 0.0608 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.8668 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.8664 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.8653 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.8652 0.0610 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.8675 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.8674 0.0481 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.8671 0.0608 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.8668 0.0516 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.8698 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.8689 0.0490 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.8681 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.8675 0.0494 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.8666 0.0535 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.8656 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.8656 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.8665 0.0491 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.8668 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.8667 0.0490 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.8659 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.8657 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.8665 0.0513 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.8660 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.8659 0.0492 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.8655 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.8643 0.0605 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.8633 0.0503 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.8625 0.0473 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.8618 0.0502 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.8617 0.0474 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.8609 0.0490 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.8603 0.0583 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.8600 0.0583 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.8586 0.0512 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.8586 0.0473 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.8580 0.0492 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.8578 0.0510 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.8585 0.0473 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.8580 0.0487 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.8586 0.0509 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.8584 0.0576 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.8581 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.8577 0.0604 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.8578 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.8579 0.0509 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.8575 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.8569 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.8576 0.0501 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.8574 0.0504 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.8579 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.8582 0.0582 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.8581 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.8578 0.0588 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.8583 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.8584 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.8579 0.0507 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.8577 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.8576 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.8577 0.0488 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.8578 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.8581 0.0578 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.8578 0.0483 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.8576 0.0488 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.8579 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.8577 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.8579 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.8575 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.8574 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.8568 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.8569 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.8565 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.8563 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.8558 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.8555 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.8553 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.8550 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.8546 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.8546 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.8544 0.0496 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.8542 0.0490 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.8539 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.8536 0.0507 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.8532 0.0614 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.8530 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.8529 0.0508 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.8526 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.8523 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.8518 0.0510 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.8518 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.8518 0.0504 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.8515 0.0509 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.8513 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.8511 0.0505 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.8511 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.8510 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.8511 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.8511 0.0491 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.8509 0.0473 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.8508 0.0613 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.8506 0.0506 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.8505 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.8504 0.0500 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.8501 0.0517 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.8497 0.0473 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.8496 0.0473 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.8495 0.0474 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.8495 0.0478 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.8494 0.0497 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.8493 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.8491 0.0483 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.8489 0.0585 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.8490 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.8489 0.0487 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.8485 0.0505 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.8486 0.0473 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.8486 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.8485 0.0505 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.8485 0.0503 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.8483 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.8480 0.0510 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.8480 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.8480 0.0470 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.8480 0.0582 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.8480 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.8480 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.8480 0.0472 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.8482 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.8482 0.0514 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.8484 0.0502 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.8483 0.0512 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.8483 0.0512 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.8483 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.8481 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.8483 0.0502 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.8483 0.0587 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.8485 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.8485 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.8483 0.0509 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.8484 0.0493 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.8487 0.0497 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.8487 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.8487 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.8487 0.0479 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.8486 0.0475 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.8486 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.8485 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.8483 0.0483 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.8486 0.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.8487 0.0500 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.8486 0.0491 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.8485 0.0487 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.8484 0.0485 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.8485 0.0453 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.8483 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.8484 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.8487 0.0486 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.8486 0.0488 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.8485 0.0487 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.8486 0.0491 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.8487 0.0490 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.8490 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.8494 0.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.8497 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.8496 0.0508 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.8494 0.0480 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.8494 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.9115 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.8678 0.0515 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.8526 0.0492 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.8439 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.8411 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.8354 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.8352 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.8356 0.0498 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.8372 0.0508 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.8377 0.0582 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.8359 0.0496 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.8348 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.8350 0.0488 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.8376 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.8371 0.0578 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.8360 0.0487 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.8359 0.0474 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.8382 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.8381 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.8379 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.8376 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.8406 0.0494 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.8397 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.8389 0.0488 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.8384 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.8374 0.0476 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.8364 0.0473 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.8365 0.0503 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.8373 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.8377 0.0492 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.8376 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.8367 0.0473 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.8366 0.0476 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.8374 0.0515 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.8369 0.0475 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.8369 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.8365 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.8352 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.8342 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.8334 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.8327 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.8327 0.0492 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.8319 0.0494 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.8313 0.0471 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.8310 0.0575 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.8296 0.0494 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.8296 0.0486 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.8291 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.8289 0.0473 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.8297 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.8291 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.8298 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.8297 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.8294 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.8290 0.0507 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.8291 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.8292 0.0578 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.8288 0.0512 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.8283 0.0475 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.8289 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.8288 0.0475 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.8293 0.0500 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.8297 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.8296 0.0504 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.8294 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.8298 0.0494 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.8300 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.8295 0.0607 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.8293 0.0509 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.8292 0.0511 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.8294 0.0519 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.8294 0.0478 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.8298 0.0476 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.8295 0.0509 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.8293 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.8296 0.0508 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.8294 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.8296 0.0503 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.8292 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.8291 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.8286 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.8287 0.0492 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.8283 0.0488 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.8282 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.8276 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.8273 0.0617 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.8272 0.0474 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.8269 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.8264 0.0573 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.8266 0.0504 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.8263 0.0490 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.8261 0.0699 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.8258 0.0490 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.8255 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.8252 0.0487 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.8250 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.8249 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.8246 0.0507 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.8243 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.8238 0.0504 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.8238 0.0504 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.8238 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.8235 0.0494 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.8234 0.0490 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.8231 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.8232 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.8231 0.0509 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.8232 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.8232 0.0502 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.8230 0.0476 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.8229 0.0558 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.8227 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.8226 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.8224 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.8222 0.0572 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.8218 0.0510 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.8217 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.8216 0.0497 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.8215 0.0476 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.8215 0.0581 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.8214 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.8212 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.8210 0.0532 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.8211 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.8209 0.0488 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.8206 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.8207 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.8207 0.0519 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.8206 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.8205 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.8203 0.0514 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.8201 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.8201 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.8201 0.0491 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.8201 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.8202 0.0492 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.8202 0.0506 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.8202 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.8204 0.0515 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.8203 0.0483 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.8205 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.8205 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.8205 0.0509 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.8205 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.8203 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.8205 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.8205 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.8207 0.0513 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.8207 0.0578 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.8206 0.0610 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.8206 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.8209 0.0491 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.8209 0.0511 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.8209 0.0504 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.8209 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.8209 0.0507 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.8209 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.8208 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.8206 0.0488 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.8209 0.0498 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.8210 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.8209 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.8208 0.0493 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.8208 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.8208 0.0568 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.8207 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.8208 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.8211 0.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.8210 0.0474 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.8210 0.0481 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.8210 0.0486 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.8211 0.0472 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.8215 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.8218 0.0478 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.8221 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.8220 0.0489 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.8218 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.8218 0.0501 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.8823 0.0469 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.8411 0.0501 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.8265 0.0473 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.8180 0.0501 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.8148 0.0512 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.8090 0.0485 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.8088 0.0493 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.8093 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.8109 0.0476 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.8113 0.0501 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.8095 0.0475 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.8083 0.0501 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.8085 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.8111 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.8105 0.0504 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.8095 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.8093 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.8115 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.8114 0.0476 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.8113 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.8109 0.0502 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.8137 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.8128 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.8120 0.0490 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.8115 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.8106 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.8096 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.8097 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.8105 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.8109 0.0515 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.8108 0.0508 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.8099 0.0476 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.8099 0.0488 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.8106 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.8102 0.0541 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.8102 0.0496 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.8098 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.8086 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.8074 0.0476 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.8067 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.8060 0.0494 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.8060 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.8053 0.0492 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.8047 0.0604 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.8045 0.0488 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.8031 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.8031 0.0499 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.8026 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.8024 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.8032 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.8026 0.0485 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.8034 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.8033 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.8030 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.8026 0.0488 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.8027 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.8028 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.8024 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.8019 0.0515 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.8026 0.0630 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.8025 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.8031 0.0491 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.8034 0.0508 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.8034 0.0490 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.8032 0.0500 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.8037 0.0474 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.8038 0.0604 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.8034 0.0606 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.8032 0.0474 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.8031 0.0514 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.8034 0.0503 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.8034 0.0580 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.8038 0.0471 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.8035 0.0477 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.8033 0.0488 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.8036 0.0503 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.8035 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.8037 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.8032 0.0577 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.8032 0.0487 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.8026 0.0471 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.8028 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.8023 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.8023 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.8017 0.0513 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.8015 0.0488 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.8013 0.0579 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.8010 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.8006 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.8007 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.8005 0.0501 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.8003 0.0471 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.8000 0.0476 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.7997 0.0473 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.7994 0.0523 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.7993 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.7992 0.0471 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.7989 0.0509 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.7986 0.0558 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.7981 0.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.7982 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.7981 0.0485 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.7979 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.7977 0.0473 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.7975 0.0494 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.7976 0.0477 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.7975 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.7976 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.7976 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.7974 0.0506 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.7973 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.7971 0.0476 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.7970 0.0501 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.7969 0.0476 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.7966 0.0502 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.7962 0.0507 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.7961 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.7960 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.7960 0.0506 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.7959 0.0500 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.7959 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.7956 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.7954 0.0474 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.7955 0.0506 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.7954 0.0491 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.7950 0.0471 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.7951 0.0506 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.7951 0.0474 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.7950 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.7950 0.0480 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.7948 0.0507 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.7946 0.0501 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.7946 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.7947 0.0490 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.7946 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.7947 0.0477 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.7947 0.0469 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.7947 0.0470 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.7949 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.7949 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.7951 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.7951 0.0481 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.7950 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.7951 0.0474 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.7950 0.0470 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.7952 0.0606 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.7951 0.0477 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.7953 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.7953 0.0509 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.7952 0.0581 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.7952 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.7955 0.0471 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.7956 0.0516 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.7956 0.0534 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.7956 0.0487 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.7956 0.0572 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.7956 0.0487 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.7955 0.0509 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.7954 0.0475 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.7956 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.7958 0.0493 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.7957 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.7957 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.7957 0.0504 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.7957 0.0579 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.7956 0.0477 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.7957 0.0473 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.7960 0.0483 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.7960 0.0482 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.7959 0.0595 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.7960 0.0493 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.7961 0.0486 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.7964 0.0515 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.7968 0.0574 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.7971 0.0534 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.7970 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.7968 0.0617 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.7969 0.0497 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.8558 0.0481 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.8153 0.0611 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.8019 0.0489 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.7930 0.0481 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.7898 0.0491 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.7835 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.7836 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.7837 0.0476 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.7855 0.0469 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.7854 0.0470 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.7837 0.0470 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.7823 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.7826 0.0575 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.7853 0.0487 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.7847 0.0489 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.7836 0.0499 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.7834 0.0491 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.7858 0.0617 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.7858 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.7859 0.0492 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.7855 0.0473 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.7883 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.7874 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.7868 0.0473 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.7864 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.7855 0.0502 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.7844 0.0481 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.7846 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.7855 0.0487 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.7859 0.0491 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.7858 0.0483 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.7850 0.0503 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.7851 0.0488 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.7858 0.0476 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.7855 0.0475 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.7855 0.0474 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.7850 0.0487 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.7838 0.0480 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.7827 0.0480 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.7819 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.7813 0.0493 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.7813 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.7806 0.0494 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.7800 0.0473 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.7798 0.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.7785 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.7785 0.0490 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.7781 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.7778 0.0491 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.7786 0.0504 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.7781 0.0474 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.7789 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.7788 0.0483 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.7786 0.0571 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.7782 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.7783 0.0470 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.7784 0.0479 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.7780 0.0571 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.7775 0.0504 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.7782 0.0479 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.7781 0.0540 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.7788 0.0608 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.7792 0.0495 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.7792 0.0599 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.7790 0.0512 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.7795 0.0624 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.7797 0.0528 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.7792 0.0571 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.7790 0.0475 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.7790 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.7793 0.0496 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.7793 0.0510 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.7797 0.0500 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.7794 0.0496 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.7792 0.0480 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.7795 0.0537 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.7793 0.0516 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.7795 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.7791 0.0499 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.7790 0.0494 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.7784 0.0489 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.7786 0.0499 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.7782 0.0479 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.7781 0.0522 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.7776 0.0509 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.7773 0.0560 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.7772 0.0572 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.7769 0.0521 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.7765 0.0584 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.7766 0.0513 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.7763 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.7762 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.7759 0.0488 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.7756 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.7753 0.0476 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.7752 0.0503 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.7751 0.0493 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.7748 0.0546 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.7745 0.0491 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.7741 0.0495 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.7741 0.0517 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.7741 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.7738 0.0584 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.7737 0.0476 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.7736 0.0488 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.7736 0.0477 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.7736 0.0511 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.7737 0.0487 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.7737 0.0509 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.7735 0.0519 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.7734 0.0685 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.7732 0.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.7731 0.0497 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.7730 0.0516 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.7728 0.0499 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.7724 0.0543 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.7722 0.0527 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.7722 0.0492 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.7721 0.0501 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.7721 0.0527 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.7720 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.7718 0.0498 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.7716 0.0497 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.7716 0.0582 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.7715 0.0509 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.7712 0.0494 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.7713 0.0488 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.7714 0.0496 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.7713 0.0608 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.7713 0.0514 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.7711 0.0497 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.7709 0.0493 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.7709 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.7710 0.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.7709 0.0503 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.7711 0.0505 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.7711 0.0495 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.7711 0.0484 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.7713 0.0511 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.7713 0.0497 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.7715 0.0490 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.7715 0.0592 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.7715 0.0489 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.7715 0.0561 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.7714 0.0498 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.7716 0.0497 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.7716 0.0493 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.7718 0.0495 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.7719 0.0588 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.7718 0.0490 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.7717 0.0518 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.7720 0.0480 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.7721 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.7721 0.0515 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.7721 0.0496 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.7721 0.0531 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.7721 0.0505 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.7720 0.0495 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.7719 0.0559 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.7721 0.0490 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.7723 0.0506 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.7722 0.0500 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.7722 0.0501 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.7721 0.0505 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.7722 0.0632 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.7720 0.0598 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.7722 0.0489 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.7725 0.0501 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.7725 0.0501 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.7724 0.0489 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.7724 0.0493 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.7726 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.7729 0.0542 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.7733 0.0522 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.7736 0.0492 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.7735 0.0582 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.7734 0.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.7734 0.0493 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.8318 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.7929 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.7803 0.0483 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.7717 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.7682 0.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.7617 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.7618 0.0480 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.7620 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.7636 0.0493 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.7637 0.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.7618 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.7604 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.7607 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.7633 0.0518 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.7626 0.0499 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.7615 0.0507 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.7613 0.0495 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.7635 0.0479 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.7635 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.7637 0.0528 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.7633 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.7659 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.7650 0.0515 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.7643 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.7640 0.0505 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.7631 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.7620 0.0485 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.7622 0.0520 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.7630 0.0512 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.7635 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.7634 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.7626 0.0495 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.7627 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.7634 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.7631 0.0491 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.7631 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.7626 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.7615 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.7603 0.0488 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.7595 0.0509 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.7589 0.0584 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.7590 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.7583 0.0493 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.7576 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.7575 0.0507 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.7562 0.0501 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.7563 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.7558 0.0505 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.7555 0.0480 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.7564 0.0546 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.7558 0.0497 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.7567 0.0493 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.7566 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.7564 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.7561 0.0502 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.7562 0.0579 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.7563 0.0506 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.7560 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.7554 0.0488 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.7562 0.0497 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.7561 0.0497 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.7568 0.0476 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.7572 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.7572 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.7570 0.0491 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.7576 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.7577 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.7573 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.7572 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.7571 0.0506 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.7574 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.7575 0.0500 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.7579 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.7576 0.0488 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.7574 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.7577 0.0485 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.7575 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.7576 0.0510 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.7572 0.0497 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.7571 0.0513 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.7566 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.7568 0.0491 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.7564 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.7563 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.7558 0.0502 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.7556 0.0478 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.7554 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.7551 0.0507 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.7547 0.0474 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.7549 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.7546 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.7544 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.7542 0.0485 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.7539 0.0524 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.7536 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.7535 0.0602 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.7534 0.0511 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.7531 0.0512 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.7528 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.7523 0.0515 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.7524 0.0485 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.7524 0.0511 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.7521 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.7520 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.7518 0.0497 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.7519 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.7519 0.0583 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.7520 0.0479 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.7520 0.0478 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.7519 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.7517 0.0517 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.7515 0.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.7514 0.0479 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.7513 0.0591 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.7511 0.0736 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.7507 0.0631 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.7506 0.0506 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.7505 0.0499 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.7505 0.0514 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.7504 0.0490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.7504 0.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.7501 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.7499 0.0491 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.7499 0.0602 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.7498 0.0477 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.7495 0.0475 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.7496 0.0521 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.7497 0.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.7496 0.0508 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.7496 0.0454 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.7494 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.7491 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.7492 0.0475 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.7492 0.0528 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.7492 0.0697 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.7493 0.0527 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.7494 0.0589 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.7494 0.0534 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.7496 0.0491 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.7495 0.0611 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.7498 0.0495 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.7497 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.7497 0.0534 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.7498 0.0526 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.7496 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.7498 0.0497 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.7498 0.0493 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.7500 0.0597 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.7501 0.0532 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.7500 0.0673 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.7499 0.0501 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.7502 0.0517 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.7503 0.0486 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.7503 0.0507 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.7503 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.7503 0.0484 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.7503 0.0512 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.7502 0.0606 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.7500 0.0496 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.7503 0.0502 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.7504 0.0496 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.7504 0.0553 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.7504 0.0526 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.7503 0.0507 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.7504 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.7502 0.0481 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.7504 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.7507 0.0479 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.7507 0.0499 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.7506 0.0610 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.7507 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.7508 0.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.7512 0.0611 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.7515 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.7518 0.0510 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.7518 0.0518 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.7516 0.0494 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.7516 0.0512 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.8087 0.0474 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.7716 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.7596 0.0511 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.7511 0.0500 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.7476 0.0501 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.7409 0.0497 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.7411 0.0497 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.7413 0.0534 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.7429 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.7428 0.0518 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.7408 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.7394 0.0497 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.7396 0.1380 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.7423 0.0479 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.7415 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.7403 0.0515 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.7402 0.0498 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.7425 0.0503 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.7425 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.7427 0.0494 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.7423 0.0523 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.7449 0.0603 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.7439 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.7433 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.7430 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.7420 0.0510 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.7410 0.0503 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.7412 0.0528 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.7421 0.0579 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.7425 0.0684 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.7425 0.0499 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.7416 0.0496 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.7418 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.7425 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.7423 0.0481 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.7423 0.0499 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.7418 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.7406 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.7394 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.7387 0.0527 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.7381 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.7382 0.0479 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.7375 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.7369 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.7368 0.0480 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.7356 0.0483 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.7356 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.7351 0.0479 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.7348 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.7358 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.7352 0.0504 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.7361 0.0601 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.7361 0.0510 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.7359 0.0499 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.7356 0.0538 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.7357 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.7358 0.0490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.7355 0.0517 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.7350 0.0507 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.7357 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.7357 0.0528 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.7364 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.7368 0.0513 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.7369 0.0500 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.7367 0.0508 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.7372 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.7374 0.0503 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.7370 0.0503 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.7369 0.0499 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.7369 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.7372 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.7373 0.0519 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.7377 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.7374 0.0482 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.7372 0.0623 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.7374 0.0500 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.7373 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.7374 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.7370 0.0485 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.7369 0.0518 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.7364 0.0481 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.7366 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.7362 0.0512 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.7361 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.7357 0.0529 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.7354 0.0497 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.7353 0.0502 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.7350 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.7346 0.0494 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.7347 0.0497 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.7345 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.7343 0.0496 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.7340 0.0523 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.7338 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.7335 0.0501 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.7334 0.0521 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.7333 0.0495 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.7330 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.7327 0.0514 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.7322 0.0511 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.7323 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.7322 0.0595 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.7320 0.0492 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.7319 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.7317 0.0494 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.7318 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.7317 0.0510 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.7319 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.7318 0.0514 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.7317 0.0494 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.7316 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.7314 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.7313 0.0493 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.7312 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.7310 0.0492 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.7306 0.0502 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.7305 0.0495 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.7304 0.0486 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.7303 0.0495 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.7303 0.0515 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.7302 0.0494 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.7299 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.7297 0.0494 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.7297 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.7296 0.0492 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.7293 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.7294 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.7295 0.0501 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.7294 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.7294 0.0522 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.7292 0.0510 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.7289 0.0514 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.7290 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.7291 0.0521 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.7290 0.0472 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.7291 0.0586 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.7292 0.0497 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.7292 0.0591 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.7294 0.0507 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.7294 0.0511 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.7296 0.0469 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.7296 0.0486 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.7295 0.0499 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.7296 0.0500 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.7295 0.0532 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.7297 0.0514 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.7297 0.0492 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.7299 0.0506 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.7300 0.0528 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.7299 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.7298 0.0496 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.7301 0.0492 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.7301 0.0509 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.7302 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.7302 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.7301 0.0509 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.7302 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.7301 0.0488 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.7299 0.0599 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.7302 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.7303 0.0491 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.7303 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.7303 0.0492 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.7303 0.0507 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.7303 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.7302 0.0627 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.7303 0.0502 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.7307 0.0534 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.7307 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.7306 0.0490 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.7306 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.7308 0.0495 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.7311 0.0511 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.7315 0.0501 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.7318 0.0487 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.7318 0.0494 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.7316 0.0517 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.7316 0.0520 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.7871 0.0483 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.7518 0.0505 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.7404 0.0479 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.7321 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.7285 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.7217 0.0483 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.7219 0.0501 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.7220 0.0498 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.7236 0.0497 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.7234 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.7213 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.7199 0.0529 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.7201 0.0653 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.7228 0.0659 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.7220 0.0533 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.7208 0.0494 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.7207 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.7229 0.0483 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.7230 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.7233 0.0515 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.7229 0.0589 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.7254 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.7244 0.0594 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.7238 0.0512 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.7235 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.7225 0.0500 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.7214 0.0492 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.7217 0.0540 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.7226 0.0499 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.7230 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.7230 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.7222 0.0513 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.7224 0.0482 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.7231 0.0522 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.7229 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.7229 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.7224 0.0497 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.7213 0.0626 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.7200 0.0540 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.7193 0.0527 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.7187 0.0501 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.7188 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.7182 0.0506 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.7175 0.0499 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.7175 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.7163 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.7164 0.0518 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.7159 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.7156 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.7165 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.7160 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.7170 0.0506 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.7169 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.7168 0.0515 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.7164 0.0596 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.7165 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.7167 0.0519 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.7164 0.0503 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.7159 0.0520 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.7166 0.0503 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.7166 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.7174 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.7178 0.0499 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.7179 0.0483 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.7177 0.0494 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.7182 0.0492 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.7184 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.7180 0.0553 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.7179 0.0499 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.7179 0.0504 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.7182 0.0483 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.7184 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.7188 0.0535 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.7185 0.0598 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.7183 0.0517 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.7185 0.0486 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.7184 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.7185 0.0514 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.7181 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.7180 0.0600 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.7175 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.7177 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.7173 0.0499 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.7172 0.0501 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.7168 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.7165 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.7164 0.0620 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.7161 0.0494 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.7157 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.7158 0.0504 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.7156 0.0500 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.7154 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.7152 0.0519 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.7149 0.0580 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.7146 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.7145 0.0482 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.7145 0.0497 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.7141 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.7138 0.0497 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.7134 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.7134 0.0509 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.7134 0.0512 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.7131 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.7130 0.0498 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.7128 0.0520 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.7129 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.7128 0.0515 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.7130 0.0481 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.7129 0.0497 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.7128 0.0494 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.7127 0.0490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.7125 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.7124 0.0504 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.7123 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.7121 0.0482 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.7117 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.7115 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.7115 0.0482 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.7114 0.0481 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.7113 0.0491 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.7113 0.0498 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.7110 0.0509 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.7107 0.0483 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.7108 0.0521 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.7107 0.0497 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.7103 0.0589 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.7105 0.0560 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.7105 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.7104 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.7104 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.7102 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.7100 0.0517 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.7101 0.0531 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.7101 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.7101 0.0508 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.7101 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.7102 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.7102 0.0499 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.7105 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.7104 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.7106 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.7106 0.0503 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.7106 0.0498 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.7107 0.0494 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.7105 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.7107 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.7107 0.0602 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.7110 0.0497 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.7110 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.7110 0.0490 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.7109 0.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.7112 0.0506 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.7112 0.0499 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.7112 0.0484 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.7112 0.0494 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.7112 0.0485 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.7112 0.0506 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.7112 0.0498 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.7110 0.0483 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.7113 0.0496 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.7114 0.0482 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.7114 0.0542 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.7114 0.0492 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.7114 0.0568 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.7114 0.0509 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.7113 0.0513 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.7115 0.0492 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.7118 0.0507 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.7118 0.0493 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.7118 0.0595 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.7118 0.0498 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.7119 0.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.7123 0.0588 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.7127 0.0585 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.7130 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.7129 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.7128 0.0487 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.7128 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.7664 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.7330 0.0497 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.7222 0.0524 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.7140 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.7103 0.0489 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.7035 0.0455 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.7037 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.7039 0.0520 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.7055 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.7051 0.0577 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.7030 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.7016 0.0546 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.7018 0.0505 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.7045 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.7036 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.7024 0.0514 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.7023 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.7045 0.0500 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.7047 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.7050 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.7046 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.7070 0.0517 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.7060 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.7054 0.0518 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.7051 0.0512 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.7041 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.7030 0.0586 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.7032 0.0502 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.7042 0.0509 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.7046 0.0518 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.7046 0.0513 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.7037 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.7040 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.7047 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.7045 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.7046 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.7040 0.0612 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.7029 0.0494 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.7016 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.7009 0.0589 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.7003 0.0557 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.7004 0.0498 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.6998 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.6991 0.0590 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.6992 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.6981 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.6981 0.0494 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.6976 0.0519 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.6973 0.0487 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.6983 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.6978 0.0468 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.6987 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.6987 0.0483 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.6986 0.0590 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.6982 0.0503 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.6984 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.6986 0.0618 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.6983 0.0504 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.6977 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.6985 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.6985 0.0500 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.6993 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.6997 0.0513 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.6998 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.6997 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.7002 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.7004 0.0510 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.7000 0.0499 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.6999 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.6999 0.0601 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.7002 0.0507 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.7004 0.0511 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.7008 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.7005 0.0489 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.7003 0.0503 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.7006 0.0508 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.7004 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.7006 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.7001 0.0501 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.7001 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.6996 0.0513 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.6997 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.6993 0.0516 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.6993 0.0504 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.6989 0.0618 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.6986 0.0497 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.6985 0.0519 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.6981 0.0512 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.6978 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.6979 0.0504 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.6977 0.0516 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.6975 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.6973 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.6970 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.6967 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.6966 0.0546 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.6966 0.0516 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.6962 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.6959 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.6954 0.0503 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.6955 0.0494 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.6954 0.0507 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.6952 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.6951 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.6949 0.0589 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.6950 0.0602 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.6949 0.0497 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.6950 0.0494 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.6950 0.0499 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.6949 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.6948 0.0516 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.6945 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.6944 0.0504 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.6943 0.0506 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.6941 0.0506 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.6937 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.6936 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.6935 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.6934 0.0515 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.6934 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.6933 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.6930 0.0502 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.6927 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.6928 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.6927 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.6923 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.6925 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.6925 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.6924 0.0614 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.6924 0.0503 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.6921 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.6919 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.6920 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.6921 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.6920 0.0502 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.6921 0.0513 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.6922 0.0499 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.6922 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.6924 0.0502 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.6924 0.0585 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.6926 0.0489 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.6926 0.0499 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.6926 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.6927 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.6925 0.0483 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.6927 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.6928 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.6930 0.0505 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.6931 0.0483 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.6930 0.0505 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.6929 0.0496 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.6932 0.0503 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.6932 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.6933 0.0499 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.6933 0.0519 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.6933 0.0591 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.6933 0.0499 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.6933 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.6931 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.6933 0.0492 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.6935 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.6935 0.0488 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.6935 0.0485 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.6935 0.0490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.6935 0.0508 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.6934 0.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.6936 0.0498 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.6940 0.0486 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.6939 0.0497 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.6939 0.0509 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.6939 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.6941 0.0506 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.6945 0.0500 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.6948 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.6952 0.0499 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.6951 0.0491 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.6949 0.0509 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.6950 0.0508 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.7469 0.0505 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.7155 0.0540 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.7051 0.0504 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.6969 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.6932 0.0499 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.6862 0.0496 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.6865 0.0515 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.6867 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.6884 0.0497 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.6879 0.0483 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.6857 0.0514 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.6843 0.0503 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.6845 0.0500 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.6872 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.6863 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.6851 0.0520 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.6850 0.0529 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.6872 0.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.6874 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.6878 0.0488 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.6874 0.0506 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.6897 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.6886 0.0535 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.6881 0.0509 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.6877 0.0585 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.6867 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.6856 0.0612 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.6859 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.6868 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.6873 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.6872 0.0505 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.6864 0.0590 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.6867 0.0488 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.6874 0.0488 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.6872 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.6873 0.0544 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.6867 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.6856 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.6843 0.0510 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.6836 0.0517 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.6830 0.0619 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.6832 0.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.6826 0.0496 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.6819 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.6820 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.6809 0.0509 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.6809 0.0484 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.6804 0.0524 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.6801 0.0519 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.6811 0.0524 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.6806 0.0505 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.6816 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.6815 0.0519 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.6815 0.0507 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.6811 0.0526 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.6813 0.0497 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.6815 0.0587 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.6812 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.6807 0.0488 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.6814 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.6815 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.6824 0.0505 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.6828 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.6828 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.6827 0.0514 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.6832 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.6834 0.0514 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.6831 0.0521 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.6830 0.0616 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.6830 0.0620 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.6834 0.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.6835 0.0581 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.6840 0.0510 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.6837 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.6835 0.0511 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.6837 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.6836 0.0496 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.6837 0.0485 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.6833 0.0498 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.6832 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.6827 0.0509 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.6829 0.0500 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.6825 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.6824 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.6820 0.0497 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.6817 0.0483 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.6816 0.0488 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.6813 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.6810 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.6811 0.0519 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.6809 0.0484 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.6807 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.6805 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.6802 0.0524 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.6799 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.6798 0.0520 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.6798 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.6794 0.0578 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.6791 0.0487 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.6786 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.6787 0.0514 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.6786 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.6784 0.0526 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.6783 0.0514 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.6781 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.6782 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.6781 0.0483 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.6782 0.0478 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.6782 0.0515 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.6781 0.0482 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.6779 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.6777 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.6776 0.0595 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.6775 0.0590 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.6773 0.0523 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.6769 0.0497 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.6768 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.6767 0.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.6766 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.6765 0.0498 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.6765 0.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.6762 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.6759 0.0488 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.6759 0.0513 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.6758 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.6754 0.0498 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.6756 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.6756 0.0620 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.6755 0.0497 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.6755 0.0497 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.6753 0.0488 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.6750 0.0485 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.6751 0.0533 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.6752 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.6752 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.6753 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.6754 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.6754 0.0500 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.6756 0.0493 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.6755 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.6758 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.6758 0.0501 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.6758 0.0482 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.6759 0.0524 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.6757 0.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.6759 0.0498 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.6760 0.0518 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.6762 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.6763 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.6762 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.6761 0.0514 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.6764 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.6764 0.0578 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.6765 0.0503 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.6765 0.0485 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.6765 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.6765 0.0504 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.6765 0.0506 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.6763 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.6765 0.0588 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.6767 0.0484 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.6767 0.0491 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.6768 0.0616 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.6767 0.0514 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.6768 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.6767 0.0496 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.6769 0.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.6773 0.0494 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.6772 0.0497 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.6772 0.0512 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.6772 0.0489 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.6774 0.0499 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.6778 0.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.6781 0.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.6785 0.0606 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.6784 0.0486 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.6783 0.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.6783 0.0601 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4170 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3923 0.0321 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3442 0.0316 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.1689 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.0179 0.0331 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 3.8954 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 3.7994 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 3.7243 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.6622 0.0323 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.6121 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.5671 0.0341 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.5297 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.4977 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.4718 0.0321 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.4487 0.0321 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.4277 0.0317 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.4082 0.0316 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.3927 0.0328 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.3777 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.3623 0.0334 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.3492 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.3374 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.3262 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.3159 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.3061 0.0314 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.2976 0.0324 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.2898 0.0324 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.2816 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.2743 0.0309 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.2676 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.2619 0.0357 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.2556 0.0316 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.2493 0.0316 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.2440 0.0314 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.2383 0.0331 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.2334 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.2280 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.2228 0.0316 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.2177 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.2131 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.2084 0.0309 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.2041 0.0313 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.1997 0.0316 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.1955 0.0335 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.1913 0.0311 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.1876 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.1841 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.1808 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.1775 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.1743 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.1709 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.1675 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.1643 0.0339 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.1609 0.0323 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.1579 0.0337 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.1545 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.1514 0.0330 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.1484 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.1452 0.0319 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.1423 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.1393 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.1367 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.1342 0.0318 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.1310 0.0314 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.1279 0.0314 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.1252 0.0361 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.1225 0.0311 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.1190 0.0311 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.1158 0.0365 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.1130 0.0345 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.1101 0.0324 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.1074 0.0328 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.1045 0.0326 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.1015 0.0326 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.0986 0.0334 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.0959 0.0345 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.0929 0.0346 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.0899 0.0352 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.0868 0.0342 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.0836 0.0346 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.0804 0.0353 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.0773 0.0366 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.0742 0.0351 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.0710 0.0372 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.0675 0.0351 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.0640 0.0352 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.0606 0.0355 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.0572 0.0353 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.0539 0.0356 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.0505 0.0374 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.0472 0.0376 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.0437 0.0421 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.0403 0.0381 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.0366 0.0375 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.0330 0.0371 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.0293 0.0370 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.0259 0.0388 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.0223 0.0374 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.0190 0.0370 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.0154 0.0380 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.0120 0.0380 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.0085 0.0371 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.0048 0.0386 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.0012 0.0386 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 2.9975 0.0380 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 2.9939 0.0387 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 2.9902 0.0391 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 2.9866 0.0376 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 2.9831 0.0441 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 2.9792 0.0452 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 2.9756 0.0410 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 2.9721 0.0375 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 2.9684 0.0389 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 2.9646 0.0387 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 2.9609 0.0378 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 2.9571 0.0391 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 2.9534 0.0384 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 2.9499 0.0382 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 2.9465 0.0390 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 2.9429 0.0412 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 2.9396 0.0397 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 2.9361 0.0397 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 2.9326 0.0445 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 2.9292 0.0392 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 2.9257 0.0384 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 2.9221 0.0388 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 2.9187 0.0389 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 2.9154 0.0393 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 2.9119 0.0464 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 2.9086 0.0395 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 2.9052 0.0382 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 2.9017 0.0393 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 2.8984 0.0389 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 2.8951 0.0389 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 2.8916 0.0427 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 2.8882 0.0391 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 2.8849 0.0389 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 2.8816 0.0385 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 2.8785 0.0385 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 2.8753 0.0390 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 2.8728 0.0418 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 2.8698 0.0409 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 2.8669 0.0390 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 2.8638 0.0382 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 2.8609 0.0392 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 2.8581 0.0383 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 2.8552 0.0388 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 2.8524 0.0400 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 2.8494 0.0387 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 2.8464 0.0399 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 2.8437 0.0388 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 2.8411 0.0401 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 2.8384 0.0384 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 2.8356 0.0425 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 2.8327 0.0386 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 2.8299 0.0401 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 2.8270 0.0456 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 2.8241 0.0387 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 2.8211 0.0398 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 2.8184 0.0419 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 2.8157 0.0386 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 2.8128 0.0390 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 2.8100 0.0388 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 164/3560 Training loss: 2.8072 0.0396 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 2.8046 0.0445 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 2.8019 0.0437 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 2.7992 0.0391 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 2.7966 0.0399 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 2.7940 0.0401 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 2.7913 0.0395 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 2.7887 0.0388 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 2.7862 0.0409 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 2.7839 0.0402 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 2.7816 0.0399 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 2.7792 0.0440 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 2.7767 0.0411 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 2.7741 0.0442 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 2.7715 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.3802 0.0393 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.3314 0.0446 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.3183 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.3142 0.0393 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.3119 0.0393 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.3090 0.0399 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.3086 0.0404 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.3088 0.0448 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.3092 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.3077 0.0466 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.3053 0.0443 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.3045 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.3035 0.0397 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.3051 0.0393 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.3054 0.0421 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.3055 0.0453 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.3048 0.0397 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.3062 0.0462 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.3055 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.3038 0.0404 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.3024 0.0399 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.3031 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.3017 0.0406 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.3001 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.2987 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.2975 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.2960 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.2951 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.2948 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.2942 0.0445 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.2936 0.0409 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.2923 0.0429 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.2908 0.0410 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.2902 0.0403 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.2890 0.0404 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.2880 0.0433 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.2868 0.0430 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.2848 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.2833 0.0426 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.2818 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.2803 0.0411 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.2790 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.2774 0.0404 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.2759 0.0416 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.2745 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.2725 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.2718 0.0449 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.2704 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.2694 0.0445 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.2687 0.0411 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.2672 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.2665 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.2652 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.2640 0.0499 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.2628 0.0410 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.2620 0.0399 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.2611 0.0415 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.2599 0.0456 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.2588 0.0444 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.2584 0.0416 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.2573 0.0409 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.2566 0.0425 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.2560 0.0422 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.2552 0.0419 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.2540 0.0431 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.2533 0.0454 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.2524 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.2511 0.0399 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.2500 0.0409 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.2492 0.0421 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.2485 0.0444 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.2478 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.2470 0.0417 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.2459 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.2449 0.0425 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.2445 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.2435 0.0426 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.2429 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.2417 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.2407 0.0425 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.2395 0.0425 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.2388 0.0470 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.2377 0.0429 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.2366 0.0428 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.2352 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.2341 0.0410 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.2331 0.0406 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.2322 0.0416 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.2311 0.0460 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.2304 0.0410 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.2294 0.0421 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.2286 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.2275 0.0406 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.2264 0.0406 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.2253 0.0414 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.2244 0.0421 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.2236 0.0411 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.2228 0.0406 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.2219 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.2209 0.0481 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.2203 0.0413 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.2195 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.2186 0.0450 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.2177 0.0428 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.2167 0.0409 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.2159 0.0427 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.2151 0.0404 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.2146 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.2139 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.2130 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.2122 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.2115 0.0403 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.2107 0.0439 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.2099 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.2091 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.2080 0.0413 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.2073 0.0413 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.2065 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.2059 0.0423 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.2052 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.2046 0.0423 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.2038 0.0461 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.2029 0.0411 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.2024 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.2017 0.0438 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.2009 0.0450 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.2003 0.0409 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.1997 0.0419 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.1990 0.0397 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.1984 0.0426 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.1976 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.1967 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.1960 0.0403 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.1954 0.0457 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.1947 0.0403 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.1941 0.0462 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.1934 0.0422 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.1927 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.1923 0.0409 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.1915 0.0409 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.1910 0.0427 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.1902 0.0442 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.1895 0.0411 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.1889 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.1881 0.0416 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.1876 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.1869 0.0465 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.1864 0.0423 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.1857 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.1849 0.0416 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.1843 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.1840 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.1835 0.0411 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.1828 0.0414 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.1821 0.0402 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.1814 0.0419 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.1807 0.0410 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.1800 0.0417 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.1791 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.1787 0.0416 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.1782 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.1774 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.1768 0.0409 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.1761 0.0413 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.1755 0.0416 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.1748 0.0433 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.1742 0.0407 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.1737 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.1731 0.0403 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.1723 0.0406 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.1717 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.1712 0.0422 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.1708 0.0414 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.1704 0.0404 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.1699 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.1694 0.0458 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.1687 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.1680 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.1270 0.0416 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.0775 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.0620 0.0430 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.0553 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.0513 0.0434 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.0468 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.0458 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.0464 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.0476 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.0469 0.0428 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.0441 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.0422 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.0417 0.0418 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.0437 0.0424 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.0432 0.0421 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.0419 0.0413 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.0416 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.0430 0.0480 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.0426 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.0417 0.0430 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.0409 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.0427 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.0416 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.0403 0.0437 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.0395 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.0384 0.0420 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.0373 0.0431 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.0371 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.0377 0.0429 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.0375 0.0437 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.0370 0.0410 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.0359 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.0351 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.0353 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.0345 0.0446 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.0337 0.0406 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.0329 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.0314 0.0406 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.0303 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.0289 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.0281 0.0412 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.0275 0.0418 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.0265 0.0431 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.0253 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.0247 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.0229 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.0225 0.0423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.0214 0.0449 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.0207 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.0208 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.0198 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.0199 0.0420 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.0192 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.0184 0.0411 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.0179 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.0175 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.0171 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.0163 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.0155 0.0466 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.0156 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.0149 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.0149 0.0412 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.0148 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.0144 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.0137 0.0419 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.0135 0.0406 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.0130 0.0419 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.0122 0.0410 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.0116 0.0473 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.0110 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.0108 0.0422 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.0105 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.0102 0.0412 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.0094 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.0089 0.0405 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.0088 0.0410 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.0082 0.0421 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.0079 0.0412 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.0071 0.0411 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.0065 0.0406 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.0056 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.0053 0.0411 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.0043 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.0038 0.0420 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.0028 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.0020 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.0015 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.0009 0.0438 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.0001 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 1.9998 0.0449 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 1.9991 0.0419 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 1.9986 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 1.9977 0.0421 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 1.9970 0.0405 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 1.9963 0.0434 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 1.9958 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 1.9953 0.0460 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 1.9946 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 1.9939 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 1.9931 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 1.9927 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 1.9922 0.0413 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 1.9915 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 1.9909 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 1.9902 0.0419 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 1.9898 0.0475 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 1.9893 0.0410 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 1.9890 0.0413 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 1.9886 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 1.9880 0.0419 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 1.9874 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 1.9869 0.0432 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 1.9864 0.0435 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 1.9859 0.0421 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 1.9853 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 1.9845 0.0412 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 1.9840 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 1.9836 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 1.9832 0.0416 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 1.9828 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 1.9824 0.0412 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 1.9818 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 1.9812 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 1.9810 0.0416 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 1.9807 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 1.9800 0.0456 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 1.9797 0.0406 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 1.9795 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 1.9790 0.0474 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 1.9787 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 1.9781 0.0428 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 1.9774 0.0429 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 1.9771 0.0410 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 1.9767 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 1.9764 0.0413 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 1.9760 0.0411 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 1.9757 0.0412 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 1.9753 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 1.9751 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 1.9746 0.0413 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 1.9744 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 1.9740 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 1.9735 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 1.9732 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 1.9727 0.0410 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 1.9725 0.0420 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 1.9721 0.0457 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 1.9719 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 1.9715 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 1.9710 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 1.9706 0.0426 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 1.9704 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 1.9701 0.0410 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 1.9698 0.0434 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 1.9693 0.0421 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 1.9689 0.0405 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 1.9685 0.0416 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 1.9681 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 1.9675 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 1.9674 0.0462 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 1.9672 0.0418 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 1.9667 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 1.9664 0.0428 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 1.9660 0.0418 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 1.9656 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 1.9652 0.0447 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 1.9649 0.0410 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 524/3560 Training loss: 1.9648 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 1.9644 0.0412 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 1.9640 0.0433 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 1.9636 0.0413 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 1.9633 0.0463 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 1.9631 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 1.9628 0.0426 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 1.9626 0.0422 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 1.9623 0.0411 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 1.9618 0.0424 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 1.9615 0.0437 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 1.9658 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.9199 0.0453 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.9062 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.8989 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.8948 0.0436 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.8884 0.0487 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.8876 0.0544 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.8877 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.8899 0.0487 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.8898 0.0421 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.8874 0.0452 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.8851 0.0426 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.8849 0.0459 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.8872 0.0422 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.8863 0.0424 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.8845 0.0419 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.8837 0.0434 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.8857 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.8852 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.8850 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.8845 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.8863 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.8856 0.0426 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.8846 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.8843 0.0417 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.8830 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.8818 0.0418 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.8819 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.8826 0.0450 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.8827 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.8823 0.0428 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.8812 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.8808 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.8812 0.0426 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.8806 0.0461 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.8799 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.8792 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.8778 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.8765 0.0426 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.8754 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.8748 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.8745 0.0455 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.8736 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.8727 0.0415 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.8724 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.8707 0.0424 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.8703 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.8694 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.8689 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.8694 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.8685 0.0415 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.8689 0.0444 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.8684 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.8679 0.0418 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.8674 0.0417 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.8673 0.0463 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.8671 0.0466 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.8664 0.0423 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.8658 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.8660 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.8655 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.8658 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.8659 0.0439 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.8658 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.8653 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.8654 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.8653 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.8647 0.0456 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.8643 0.0418 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.8640 0.0440 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.8641 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.8640 0.0420 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.8639 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.8633 0.0431 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.8630 0.0432 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.8631 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.8627 0.0483 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.8626 0.0475 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.8620 0.0486 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.8617 0.0499 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.8610 0.0420 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.8608 0.0425 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.8601 0.0414 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.8597 0.0417 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.8589 0.0470 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.8584 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.8580 0.0415 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.8574 0.0419 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.8567 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.8565 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.8560 0.0465 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.8556 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.8548 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.8543 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.8537 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.8534 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.8530 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.8525 0.0428 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.8519 0.0447 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.8511 0.0424 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.8509 0.0456 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.8506 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.8501 0.0428 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.8497 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.8492 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.8490 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.8486 0.0425 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.8485 0.0406 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.8482 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.8478 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.8474 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.8470 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.8467 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.8464 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.8459 0.0422 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.8453 0.0429 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.8450 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.8446 0.0430 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.8444 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.8441 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.8439 0.0420 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.8434 0.0491 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.8430 0.0447 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.8429 0.0465 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.8427 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.8421 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.8420 0.0432 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.8418 0.0414 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.8415 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.8412 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.8407 0.0462 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.8402 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.8400 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.8398 0.0419 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.8395 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.8393 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.8391 0.0426 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.8389 0.0453 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.8388 0.0422 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.8384 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.8384 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.8381 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.8378 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.8376 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.8373 0.0435 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.8372 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.8370 0.0417 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.8369 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.8367 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.8363 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.8360 0.0434 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.8359 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.8357 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.8355 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.8352 0.0415 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.8349 0.0418 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.8347 0.0439 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.8344 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.8340 0.0420 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.8340 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.8339 0.0433 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.8336 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.8334 0.0460 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.8332 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.8329 0.0425 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.8326 0.0414 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.8325 0.0414 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.8326 0.0426 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.8323 0.0435 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.8320 0.0461 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.8317 0.0413 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.8315 0.0431 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.8313 0.0422 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.8312 0.0491 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.8310 0.0418 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.8308 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.8304 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.8303 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.8652 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.8198 0.0428 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.8063 0.0430 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.7984 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.7941 0.0423 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.7857 0.0429 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.7848 0.0413 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.7842 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.7860 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.7861 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.7840 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.7818 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.7816 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.7836 0.0410 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.7826 0.0461 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.7808 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.7803 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.7823 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.7818 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.7821 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.7813 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.7829 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.7820 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.7813 0.0420 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.7811 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.7796 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.7783 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.7784 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.7791 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.7791 0.0424 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.7789 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.7775 0.0421 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.7774 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.7779 0.0429 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.7774 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.7768 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.7760 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.7747 0.0424 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.7734 0.0422 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.7724 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.7718 0.0437 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.7716 0.0413 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.7710 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.7701 0.0464 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.7700 0.0456 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.7684 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.7681 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.7672 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.7669 0.0410 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.7674 0.0410 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.7665 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.7671 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.7667 0.0413 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.7664 0.0475 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.7661 0.0468 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.7660 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.7661 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.7656 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.7648 0.0462 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.7651 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.7647 0.0480 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.7652 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.7653 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.7653 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.7649 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.7650 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.7650 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.7645 0.0450 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.7642 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.7640 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.7642 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.7642 0.0453 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.7642 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.7637 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.7634 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.7636 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.7632 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.7631 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.7625 0.0442 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.7622 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.7616 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.7614 0.0436 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.7607 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.7606 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.7598 0.0441 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.7594 0.0432 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.7590 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.7585 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.7578 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.7578 0.0437 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.7573 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.7570 0.0459 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.7564 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.7559 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.7554 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.7552 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.7549 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.7544 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.7538 0.0413 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.7532 0.0410 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.7531 0.0429 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.7529 0.0424 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.7524 0.0425 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.7522 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.7518 0.0425 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.7516 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.7514 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.7513 0.0483 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.7511 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.7509 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.7506 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.7503 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.7500 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.7497 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.7494 0.0502 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.7488 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.7486 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.7483 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.7481 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.7479 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.7477 0.0425 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.7473 0.0423 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.7469 0.0410 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.7469 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.7466 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.7461 0.0438 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.7460 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.7459 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.7457 0.0453 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.7454 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.7450 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.7445 0.0436 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.7443 0.0413 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.7442 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.7440 0.0435 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.7438 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.7438 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.7436 0.0428 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.7436 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.7433 0.0410 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.7434 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.7431 0.0459 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.7429 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.7428 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.7425 0.0456 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.7425 0.0438 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.7424 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.7424 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.7422 0.0432 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.7419 0.0427 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.7416 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.7415 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.7414 0.0459 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.7413 0.0458 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.7411 0.0438 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.7409 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.7408 0.0426 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.7406 0.0418 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.7402 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.7403 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.7402 0.0413 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.7400 0.0421 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.7399 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.7397 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.7396 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.7394 0.0425 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.7393 0.0423 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.7394 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.7392 0.0437 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.7390 0.0409 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.7388 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.7385 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.7384 0.0420 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.7383 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.7382 0.0426 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.7380 0.0435 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.7377 0.0413 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.7376 0.0424 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.7923 0.0427 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.7438 0.0458 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.7309 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.7227 0.0433 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.7182 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.7082 0.0443 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.7065 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.7056 0.0431 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.7070 0.0434 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.7070 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.7043 0.0427 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.7026 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.7023 0.0441 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.7044 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.7032 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.7015 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.7012 0.0468 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.7030 0.0427 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.7026 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.7029 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.7020 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.7029 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.7017 0.0440 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.7010 0.0426 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.7007 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.6991 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.6977 0.0432 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.6978 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.6986 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.6986 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.6984 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.6971 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.6971 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.6977 0.0488 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.6974 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.6968 0.0484 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.6960 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.6948 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.6935 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.6927 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.6921 0.0467 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.6921 0.0424 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.6915 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.6907 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.6907 0.0437 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.6892 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.6889 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.6881 0.0460 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.6878 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.6883 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.6875 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.6883 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.6880 0.0428 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.6878 0.0498 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.6874 0.0464 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.6875 0.0461 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.6878 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.6872 0.0417 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.6865 0.0461 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.6869 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.6866 0.0461 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.6872 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.6873 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.6875 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.6871 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.6873 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.6873 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.6868 0.0423 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.6867 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.6865 0.0429 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.6868 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.6869 0.0460 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.6871 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.6867 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.6864 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.6867 0.0456 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.6863 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.6863 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.6857 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.6855 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.6849 0.0468 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.6848 0.0422 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.6842 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.6841 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.6835 0.0435 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.6831 0.0433 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.6828 0.0431 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.6824 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.6817 0.0417 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.6818 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.6813 0.0436 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.6811 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.6805 0.0422 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.6801 0.0417 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.6797 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.6795 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.6793 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.6788 0.0431 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.6784 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.6778 0.0465 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.6777 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.6774 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.6771 0.0437 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.6768 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.6764 0.0432 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.6762 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.6760 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.6758 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.6757 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.6756 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.6753 0.0427 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.6750 0.0466 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.6747 0.0424 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.6746 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.6742 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.6737 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.6734 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.6733 0.0457 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.6731 0.0419 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.6729 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.6727 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.6723 0.0460 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.6719 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.6719 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.6717 0.0459 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.6712 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.6712 0.0451 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.6711 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.6709 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.6707 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.6702 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.6698 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.6697 0.0438 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.6696 0.0445 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.6695 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.6694 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.6694 0.0438 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.6693 0.0480 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.6693 0.0422 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.6690 0.0437 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.6692 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.6690 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.6688 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.6689 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.6686 0.0451 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.6686 0.0423 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.6686 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.6687 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.6686 0.0464 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.6683 0.0417 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.6680 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.6679 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.6678 0.0444 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.6678 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.6677 0.0417 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.6676 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.6675 0.0437 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.6673 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.6670 0.0423 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.6670 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.6671 0.0437 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.6669 0.0424 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.6669 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.6667 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.6666 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.6664 0.0457 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.6664 0.0463 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.6666 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.6665 0.0481 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.6663 0.0466 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.6661 0.0436 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.6658 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.6658 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.6657 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.6656 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.6654 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.6651 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.6651 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.7350 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.6847 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.6712 0.0470 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.6638 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.6589 0.0430 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.6476 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.6460 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.6448 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.6462 0.0436 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.6458 0.0412 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.6429 0.0423 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.6412 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.6409 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.6430 0.0470 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.6418 0.0421 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.6402 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.6400 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.6417 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.6415 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.6420 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.6410 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.6416 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.6404 0.0427 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.6397 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.6395 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.6378 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.6364 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.6367 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.6374 0.0434 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.6374 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.6372 0.0421 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.6360 0.0465 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.6360 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.6366 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.6364 0.0426 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.6359 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.6351 0.0421 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.6339 0.0464 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.6327 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.6319 0.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.6313 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.6315 0.0433 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.6309 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.6302 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.6302 0.0423 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.6289 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.6285 0.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.6278 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.6275 0.0465 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.6280 0.0441 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.6272 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.6281 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.6278 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.6277 0.0423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.6274 0.0457 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.6274 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.6278 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.6274 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.6267 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.6271 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.6269 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.6276 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.6279 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.6280 0.0431 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.6278 0.0421 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.6279 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.6280 0.0467 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.6276 0.0465 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.6274 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.6273 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.6276 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.6278 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.6280 0.0431 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.6276 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.6274 0.0464 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.6276 0.0408 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.6272 0.0466 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.6272 0.0412 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.6266 0.0435 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.6264 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.6258 0.0461 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.6257 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.6251 0.0423 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.6250 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.6245 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.6241 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.6238 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.6234 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.6228 0.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.6228 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.6224 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.6221 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.6216 0.0436 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.6212 0.0423 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.6208 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.6207 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.6205 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.6200 0.0438 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.6196 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.6190 0.0412 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.6189 0.0465 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.6187 0.0436 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.6183 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.6180 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.6177 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.6175 0.0423 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.6173 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.6171 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.6170 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.6169 0.0419 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.6166 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.6164 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.6161 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.6160 0.0423 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.6157 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.6152 0.0463 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.6150 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.6148 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.6147 0.0433 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.6145 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.6144 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.6140 0.0462 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.6136 0.0421 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.6136 0.0495 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.6134 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.6129 0.0443 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.6129 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.6129 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.6127 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.6125 0.0430 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.6120 0.0442 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.6117 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.6116 0.0419 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.6116 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.6115 0.0419 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.6115 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.6115 0.0432 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.6114 0.0441 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.6114 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.6113 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.6115 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.6113 0.0447 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.6112 0.0437 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.6113 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.6111 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.6111 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.6111 0.0412 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.6113 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.6113 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.6111 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.6107 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.6106 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.6106 0.0419 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.6106 0.0436 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.6106 0.0429 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.6105 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.6104 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.6103 0.0435 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.6100 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.6101 0.0414 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.6101 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.6100 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.6100 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.6099 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.6098 0.0459 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.6096 0.0443 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.6096 0.0426 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.6099 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.6098 0.0458 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.6096 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.6094 0.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.6092 0.0499 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.6092 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.6091 0.0433 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.6091 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.6089 0.0427 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.6086 0.0429 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.6087 0.0461 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.6814 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.6330 0.0463 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.6189 0.0456 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.6120 0.0414 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.6064 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.5949 0.0429 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.5937 0.0453 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.5924 0.0421 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.5937 0.0425 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.5929 0.0446 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.5900 0.0461 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.5884 0.0501 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.5879 0.0496 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.5901 0.0503 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.5889 0.0427 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.5876 0.0430 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.5876 0.0435 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.5894 0.0428 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.5893 0.0455 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.5901 0.0435 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.5892 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.5898 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.5886 0.0410 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.5879 0.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.5877 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.5860 0.0417 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.5846 0.0485 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.5849 0.0426 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.5855 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.5856 0.0441 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.5853 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.5842 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.5842 0.0502 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.5849 0.0456 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.5848 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.5843 0.0468 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.5834 0.0435 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.5824 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.5811 0.0427 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.5804 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.5798 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.5801 0.0461 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.5795 0.0417 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.5788 0.0484 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.5788 0.0480 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.5776 0.0434 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.5772 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.5766 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.5763 0.0462 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.5768 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.5760 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.5768 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.5766 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.5765 0.0466 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.5762 0.0427 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.5763 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.5768 0.0426 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.5763 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.5757 0.0481 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.5762 0.0458 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.5760 0.0470 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.5768 0.0414 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.5771 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.5773 0.0417 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.5771 0.0452 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.5773 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.5774 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.5770 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.5769 0.0412 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.5768 0.0451 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.5772 0.0410 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.5774 0.0405 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.5776 0.0436 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.5773 0.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.5771 0.0464 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.5773 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.5770 0.0439 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.5770 0.0414 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.5764 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.5762 0.0413 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.5757 0.0672 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.5756 0.0464 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.5750 0.0495 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.5750 0.0486 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.5745 0.0409 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.5742 0.0507 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.5739 0.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.5735 0.0435 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.5729 0.0490 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.5729 0.0459 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.5726 0.0435 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.5723 0.0427 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.5719 0.0513 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.5714 0.0413 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.5710 0.0424 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.5710 0.0419 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.5708 0.0446 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.5703 0.0432 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.5699 0.0457 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.5694 0.0410 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.5693 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.5691 0.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.5688 0.0426 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.5685 0.0412 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.5682 0.0418 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.5680 0.0412 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.5679 0.0462 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.5677 0.0464 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.5676 0.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.5675 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.5673 0.0433 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.5670 0.0428 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.5669 0.0433 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.5668 0.0418 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.5664 0.0440 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.5660 0.0409 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.5658 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.5657 0.0426 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.5656 0.0441 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.5654 0.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.5652 0.0432 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.5649 0.0419 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.5645 0.0468 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.5644 0.0450 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.5643 0.0425 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.5638 0.0412 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.5638 0.0456 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.5638 0.0434 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.5636 0.0423 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.5634 0.0421 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.5630 0.0404 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.5626 0.0425 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.5625 0.0469 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.5625 0.0463 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.5625 0.0425 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.5625 0.0459 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.5625 0.0491 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.5625 0.0460 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.5625 0.1058 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.5624 0.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.5626 0.0460 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.5625 0.0457 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.5624 0.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.5625 0.0420 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.5623 0.0444 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.5624 0.0425 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.5624 0.0408 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.5626 0.0414 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.5626 0.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.5624 0.0422 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.5620 0.0471 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.5620 0.0485 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.5620 0.0419 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.5620 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.5620 0.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.5619 0.0431 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.5619 0.0410 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.5618 0.0430 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.5616 0.0410 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.5616 0.0418 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.5617 0.0430 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.5616 0.0417 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.5616 0.0431 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.5615 0.0406 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.5615 0.0432 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.5613 0.0410 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.5613 0.0435 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.5617 0.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.5616 0.0415 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.5615 0.0440 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.5613 0.0425 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.5611 0.0432 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.5611 0.0409 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.5610 0.0418 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.5610 0.0413 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.5608 0.0465 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.5606 0.0434 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.5607 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.6339 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.5876 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.5729 0.0430 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.5661 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.5596 0.0410 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.5484 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.5476 0.0413 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.5463 0.0420 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.5475 0.0428 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.5466 0.0455 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.5435 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.5421 0.0432 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.5416 0.0464 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.5438 0.0415 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.5427 0.0426 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.5416 0.0415 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.5416 0.0468 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.5434 0.0468 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.5434 0.0420 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.5443 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.5435 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.5442 0.0410 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.5430 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.5424 0.0423 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.5423 0.0441 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.5405 0.0415 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.5392 0.0461 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.5395 0.0421 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.5401 0.0431 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.5403 0.0427 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.5400 0.0460 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.5389 0.0433 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.5390 0.0430 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.5397 0.0430 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.5396 0.0426 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.5391 0.0438 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.5383 0.0436 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.5372 0.0422 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.5359 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.5353 0.0458 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.5347 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.5351 0.0462 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.5346 0.0441 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.5339 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.5340 0.0428 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.5328 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.5324 0.0408 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.5318 0.0470 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.5316 0.0410 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.5320 0.0459 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.5313 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.5321 0.0410 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.5319 0.0425 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.5319 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.5316 0.0465 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.5317 0.0429 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.5322 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.5318 0.0428 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.5312 0.0428 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.5317 0.0457 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.5317 0.0413 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.5325 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.5329 0.0426 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.5330 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.5329 0.0422 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.5331 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.5332 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.5328 0.0470 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.5328 0.0430 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.5327 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.5331 0.0429 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.5334 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.5337 0.0422 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.5334 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.5333 0.0458 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.5334 0.0489 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.5331 0.0437 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.5331 0.0426 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.5325 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.5324 0.0435 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.5318 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.5318 0.0457 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.5312 0.0474 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.5311 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.5307 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.5305 0.0435 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.5302 0.0441 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.5298 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.5292 0.0437 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.5293 0.0432 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.5289 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.5287 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.5283 0.0419 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.5279 0.0438 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.5275 0.0492 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.5275 0.0432 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.5274 0.0426 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.5269 0.0438 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.5266 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.5261 0.0413 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.5260 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.5258 0.0456 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.5255 0.0421 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.5253 0.0438 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.5249 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.5248 0.0423 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.5247 0.0413 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.5245 0.0434 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.5244 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.5244 0.0440 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.5241 0.0496 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.5239 0.0434 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.5238 0.0407 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.5237 0.0467 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.5234 0.0423 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.5229 0.0455 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.5228 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.5227 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.5226 0.0408 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.5225 0.0429 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.5223 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.5220 0.0410 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.5216 0.0426 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.5215 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.5214 0.0459 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.5209 0.0418 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.5210 0.0411 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.5209 0.0415 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.5208 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.5205 0.0408 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.5201 0.0463 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.5197 0.0460 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.5197 0.0454 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.5197 0.0426 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.5197 0.0413 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.5197 0.0410 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.5198 0.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.5198 0.0419 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.5198 0.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.5197 0.0462 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.5200 0.0427 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.5199 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.5198 0.0435 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.5199 0.0421 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.5197 0.0467 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.5199 0.0462 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.5199 0.0432 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.5201 0.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.5202 0.0433 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.5200 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.5196 0.0422 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.5196 0.0472 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.5196 0.0438 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.5196 0.0427 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.5196 0.0442 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.5195 0.0432 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.5196 0.0425 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.5195 0.0459 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.5193 0.0437 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.5193 0.0506 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.5195 0.0477 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.5194 0.0418 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.5194 0.0421 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.5193 0.0460 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.5193 0.0416 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.5191 0.0422 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.5191 0.0423 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.5196 0.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.5195 0.0424 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.5194 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.5192 0.0421 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.5190 0.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.5190 0.0412 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.5190 0.0443 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.5190 0.0420 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.5188 0.0425 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.5186 0.0490 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.5187 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.5950 0.0403 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.5497 0.0417 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.5347 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.5281 0.0460 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.5207 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.5096 0.0424 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.5092 0.0414 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.5077 0.0409 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.5087 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.5078 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.5043 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.5031 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.5025 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.5046 0.0417 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.5036 0.0414 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.5025 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.5025 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.5042 0.0414 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.5043 0.0431 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.5052 0.0464 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.5045 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.5050 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.5039 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.5035 0.0436 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.5034 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.5016 0.0480 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.5003 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.5006 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.5011 0.0434 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.5014 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.5011 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.4999 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.5001 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.5007 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.5007 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.5002 0.0415 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.4994 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.4984 0.0424 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.4970 0.0460 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.4965 0.0414 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.4959 0.0455 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.4963 0.0429 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.4959 0.0487 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.4952 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.4953 0.0415 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.4942 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.4939 0.0415 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.4932 0.0423 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.4930 0.0428 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.4935 0.0460 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.4928 0.0423 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.4936 0.0426 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.4934 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.4935 0.0415 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.4932 0.0434 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.4933 0.0424 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.4938 0.0429 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.4934 0.0436 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.4928 0.0438 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.4934 0.0443 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.4934 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.4942 0.0486 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.4946 0.0437 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.4948 0.0442 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.4947 0.0420 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.4948 0.0463 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.4950 0.0485 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.4946 0.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.4946 0.0415 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.4945 0.0436 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.4950 0.0421 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.4953 0.0427 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.4956 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.4953 0.0426 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.4952 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.4953 0.0470 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.4950 0.0428 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.4950 0.0409 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.4945 0.0425 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.4943 0.0445 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.4938 0.0409 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.4937 0.0431 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.4931 0.0439 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.4931 0.0420 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.4927 0.0428 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.4925 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.4922 0.0426 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.4918 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.4912 0.0462 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.4913 0.0443 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.4910 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.4908 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.4904 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.4900 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.4897 0.0471 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.4897 0.0422 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.4896 0.0430 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.4892 0.0424 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.4888 0.0444 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.4883 0.1578 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.4883 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.4881 0.0444 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.4878 0.0446 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.4876 0.0470 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.4873 0.0425 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.4871 0.0439 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.4870 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.4869 0.0420 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.4868 0.0446 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.4868 0.0425 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.4865 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.4863 0.0414 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.4862 0.0415 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.4861 0.0431 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.4858 0.0432 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.4853 0.0414 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.4852 0.0419 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.4852 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.4851 0.0434 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.4850 0.1806 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.4848 0.0482 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.4845 0.0430 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.4841 0.0417 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.4841 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.4839 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.4835 0.0466 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.4835 0.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.4835 0.0429 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.4833 0.0409 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.4831 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.4826 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.4823 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.4823 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.4823 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.4823 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.4823 0.0421 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.4824 0.0409 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.4824 0.0414 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.4825 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.4824 0.0426 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.4827 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.4826 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.4825 0.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.4827 0.0421 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.4825 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.4827 0.0411 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.4827 0.0440 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.4830 0.0433 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.4830 0.0428 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.4828 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.4825 0.0415 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.4824 0.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.4824 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.4824 0.0405 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.4824 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.4824 0.0433 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.4824 0.0425 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.4824 0.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.4822 0.0417 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.4822 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.4824 0.0423 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.4823 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.4823 0.0416 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.4822 0.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.4822 0.0431 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.4821 0.0420 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.4821 0.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.4826 0.0430 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.4825 0.0467 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.4825 0.0468 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.4823 0.0414 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.4821 0.0427 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.4821 0.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.4821 0.0467 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.4821 0.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.4819 0.0409 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.4817 0.0429 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.4818 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.5601 0.0455 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.5162 0.0420 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.5010 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.4943 0.0432 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.4862 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.4755 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.4753 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.4737 0.0418 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.4746 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.4736 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.4700 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.4688 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.4682 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.4703 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.4692 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.4680 0.0434 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.4681 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.4696 0.0459 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.4697 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.4708 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.4701 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.4705 0.0418 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.4694 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.4690 0.0416 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.4690 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.4672 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.4659 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.4662 0.0463 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.4668 0.0420 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.4670 0.0465 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.4667 0.0461 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.4655 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.4657 0.0419 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.4662 0.0437 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.4663 0.0431 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.4658 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.4649 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.4639 0.0423 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.4625 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.4620 0.0432 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.4614 0.0409 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.4620 0.0433 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.4616 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.4610 0.0428 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.4611 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.4600 0.0462 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.4597 0.0418 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.4591 0.0459 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.4589 0.0459 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.4593 0.0416 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.4587 0.0417 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.4595 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.4593 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.4595 0.0459 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.4592 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.4593 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.4598 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.4594 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.4588 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.4594 0.0435 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.4595 0.0422 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.4604 0.0418 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.4608 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.4610 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.4609 0.0416 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.4610 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.4612 0.0431 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.4608 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.4608 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.4608 0.0461 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.4613 0.0432 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.4615 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.4619 0.0425 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.4616 0.0427 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.4615 0.0427 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.4617 0.0463 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.4614 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.4613 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.4608 0.0484 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.4607 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.4602 0.0472 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.4601 0.0463 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.4595 0.0416 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.4595 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.4591 0.0423 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.4589 0.0409 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.4586 0.0426 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.4582 0.0416 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.4577 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.4578 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.4575 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.4573 0.0464 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.4569 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.4565 0.0459 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.4563 0.0421 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.4563 0.0437 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.4562 0.0422 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.4558 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.4555 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.4550 0.0404 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.4550 0.0445 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.4547 0.0459 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.4545 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.4543 0.0418 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.4540 0.0457 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.4539 0.0460 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.4538 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.4537 0.0419 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.4536 0.0428 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.4536 0.0466 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.4533 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.4531 0.0419 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.4530 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.4529 0.0412 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.4526 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.4522 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.4521 0.0460 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.4521 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.4520 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.4519 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.4517 0.0458 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.4514 0.0409 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.4510 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.4510 0.0460 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.4509 0.0402 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.4505 0.0464 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.4505 0.0439 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.4505 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.4503 0.0420 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.4501 0.0436 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.4496 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.4493 0.0405 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.4493 0.0451 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.4493 0.0422 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.4493 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.4494 0.0418 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.4495 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.4495 0.0417 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.4495 0.0465 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.4494 0.0436 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.4497 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.4497 0.0447 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.4496 0.0428 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.4498 0.0419 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.4496 0.0409 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.4498 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.4499 0.0414 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.4501 0.0435 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.4502 0.0416 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.4500 0.0466 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.4497 0.0410 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.4496 0.0419 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.4496 0.0480 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.4496 0.0407 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.4496 0.0409 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.4496 0.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.4496 0.0463 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.4496 0.0427 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.4494 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.4495 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.4496 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.4496 0.0419 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.4496 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.4495 0.0462 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.4495 0.0427 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.4494 0.0449 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.4495 0.0436 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.4499 0.0418 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.4499 0.0408 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.4498 0.0411 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.4497 0.0459 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.4495 0.0421 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.4495 0.0463 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.4495 0.0466 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.4495 0.0424 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.4493 0.0413 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.4492 0.0415 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.4493 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.5291 0.0405 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.4871 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.4716 0.0419 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.4651 0.0412 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.4566 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.4456 0.0425 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.4456 0.0417 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.4440 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.4445 0.0419 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.4434 0.0419 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.4397 0.0420 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.4386 0.0499 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.4379 0.0493 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.4400 0.0476 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.4389 0.1397 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.4376 0.0439 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.4377 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.4392 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.4393 0.0425 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.4405 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.4398 0.0481 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.4402 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.4391 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.4388 0.0420 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.4388 0.0417 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.4370 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.4357 0.0459 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.4361 0.0460 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.4366 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.4368 0.0459 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.4365 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.4352 0.0430 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.4355 0.0412 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.4360 0.0416 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.4361 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.4356 0.0420 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.4348 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.4337 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.4323 0.0445 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.4319 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.4313 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.4319 0.0436 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.4315 0.0417 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.4310 0.0433 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.4312 0.0416 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.4301 0.0412 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.4298 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.4292 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.4290 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.4294 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.4288 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.4296 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.4295 0.0462 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.4296 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.4294 0.0426 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.4295 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.4299 0.0433 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.4295 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.4290 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.4296 0.0457 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.4297 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.4306 0.0430 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.4310 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.4312 0.0428 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.4312 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.4314 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.4315 0.0419 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.4312 0.0431 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.4312 0.0460 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.4311 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.4316 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.4319 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.4323 0.0404 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.4320 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.4318 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.4320 0.0423 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.4317 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.4316 0.0489 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.4311 0.0422 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.4310 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.4305 0.0436 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.4304 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.4298 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.4298 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.4295 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.4293 0.0406 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.4290 0.0425 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.4286 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.4281 0.0434 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.4283 0.0458 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.4280 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.4278 0.0427 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.4275 0.0439 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.4271 0.0436 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.4268 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.4269 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.4269 0.0417 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.4265 0.0461 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.4262 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.4257 0.0464 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.4256 0.0416 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.4254 0.0419 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.4252 0.0425 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.4250 0.0419 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.4247 0.0430 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.4246 0.0430 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.4245 0.0417 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.4245 0.0416 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.4244 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.4244 0.0439 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.4240 0.0412 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.4239 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.4238 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.4237 0.0426 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.4234 0.0407 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.4230 0.0433 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.4230 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.4229 0.0416 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.4229 0.0457 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.4227 0.0416 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.4226 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.4223 0.0432 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.4219 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.4219 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.4218 0.0421 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.4214 0.0469 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.4214 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.4214 0.0416 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.4213 0.0466 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.4210 0.0412 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.4205 0.0463 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.4202 0.0433 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.4203 0.0436 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.4203 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.4203 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.4203 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.4205 0.0463 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.4205 0.0430 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.4205 0.0434 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.4205 0.0408 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.4208 0.0415 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.4207 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.4206 0.0484 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.4208 0.0427 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.4207 0.0422 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.4209 0.0416 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.4210 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.4213 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.4213 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.4212 0.0419 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.4208 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.4208 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.4208 0.0411 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.4208 0.0418 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.4208 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.4208 0.0438 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.4208 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.4208 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.4206 0.0421 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.4207 0.0434 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.4208 0.0430 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.4208 0.0409 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.4208 0.0414 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.4208 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.4208 0.0410 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.4207 0.0460 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.4208 0.0436 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.4212 0.0435 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.4212 0.0413 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.4212 0.0420 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.4210 0.0422 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.4208 0.0438 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.4209 0.0440 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.4209 0.0426 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.4209 0.0435 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.4207 0.0420 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.4205 0.0440 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.4206 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.5012 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.4617 0.0432 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.4456 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.4391 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.4302 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.4190 0.0433 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.4191 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.4175 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.4178 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.4166 0.0427 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.4128 0.0433 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.4119 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.4112 0.0488 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.4132 0.0428 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.4122 0.0433 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.4108 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.4108 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.4122 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.4124 0.0446 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.4137 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.4130 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.4133 0.0458 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.4122 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.4120 0.0437 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.4121 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.4103 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.4091 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.4095 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.4099 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.4101 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.4098 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.4085 0.0408 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.4088 0.0463 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.4094 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.4094 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.4090 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.4081 0.0434 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.4071 0.0420 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.4057 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.4053 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.4048 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.4054 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.4050 0.0414 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.4045 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.4047 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.4037 0.0434 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.4034 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.4028 0.0428 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.4026 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.4029 0.0440 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.4024 0.0426 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.4032 0.0405 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.4030 0.0463 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.4032 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.4030 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.4031 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.4035 0.0484 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.4032 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.4027 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.4033 0.0458 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.4034 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.4043 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.4048 0.0453 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.4050 0.0480 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.4050 0.0432 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.4051 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.4053 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.4049 0.0423 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.4049 0.0433 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.4048 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.4053 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.4056 0.0437 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.4060 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.4057 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.4056 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.4057 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.4055 0.0435 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.4054 0.0406 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.4049 0.0457 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.4048 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.4043 0.0407 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.4042 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.4036 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.4036 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.4033 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.4031 0.0460 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.4028 0.0463 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.4025 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.4020 0.0431 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.4022 0.0457 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.4019 0.0482 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.4017 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.4014 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.4011 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.4008 0.0421 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.4010 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.4009 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.4005 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.4002 0.0404 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.3998 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.3997 0.0423 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.3995 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.3993 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.3991 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.3988 0.0438 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.3988 0.0474 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.3987 0.0475 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.3987 0.0408 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.3986 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.3986 0.0420 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.3983 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.3982 0.0457 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.3980 0.0430 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.3980 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.3977 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.3973 0.0409 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.3973 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.3972 0.0434 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.3972 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.3971 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.3970 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.3967 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.3963 0.0462 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.3963 0.0433 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.3962 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.3958 0.0422 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.3958 0.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.3958 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.3956 0.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.3954 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.3949 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.3946 0.0425 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.3947 0.0431 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.3947 0.0431 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.3947 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.3948 0.0429 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.3949 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.3949 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.3950 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.3949 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.3952 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.3952 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.3951 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.3953 0.0463 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.3952 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.3954 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.3955 0.0443 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.3958 0.0415 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.3958 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.3957 0.0433 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.3954 0.0435 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.3953 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.3954 0.0412 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.3953 0.0460 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.3954 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.3954 0.0450 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.3954 0.0411 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.3954 0.0411 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.3952 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.3953 0.0421 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.3954 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.3954 0.0434 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.3954 0.0442 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.3954 0.0414 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.3954 0.0425 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.3953 0.0435 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.3954 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.3959 0.0405 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.3958 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.3958 0.0417 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.3957 0.0416 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.3955 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.3956 0.0485 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.3956 0.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.3956 0.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.3954 0.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.3952 0.0419 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.3954 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.4758 0.0432 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.4385 0.0458 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.4224 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.4159 0.0436 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.4067 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.3954 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.3956 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.3941 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.3943 0.0431 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.3931 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.3892 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.3886 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.3878 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.3898 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.3887 0.0461 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.3873 0.0487 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.3873 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.3886 0.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.3889 0.0430 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.3903 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.3895 0.0435 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.3898 0.0425 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.3888 0.0489 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.3887 0.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.3887 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.3870 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.3859 0.0487 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.3862 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.3866 0.0444 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.3868 0.0466 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.3865 0.0464 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.3853 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.3855 0.0426 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.3861 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.3862 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.3857 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.3849 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.3838 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.3825 0.0429 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.3822 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.3816 0.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.3823 0.0424 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.3819 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.3814 0.0433 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.3817 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.3806 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.3803 0.0459 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.3797 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.3795 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.3798 0.0422 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.3793 0.0434 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.3801 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.3800 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.3802 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.3800 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.3801 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.3805 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.3802 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.3797 0.0405 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.3803 0.0484 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.3805 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.3814 0.0466 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.3819 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.3821 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.3821 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.3822 0.0437 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.3824 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.3820 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.3820 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.3819 0.0407 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.3824 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.3827 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.3831 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.3828 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.3826 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.3828 0.0407 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.3825 0.0422 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.3824 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.3819 0.0429 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.3818 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.3814 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.3812 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.3806 0.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.3806 0.0432 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.3803 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.3801 0.0427 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.3798 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.3796 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.3790 0.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.3792 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.3790 0.0432 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.3789 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.3786 0.0433 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.3782 0.0468 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.3780 0.0434 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.3781 0.0464 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.3781 0.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.3777 0.0436 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.3774 0.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.3770 0.0424 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.3770 0.0464 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.3768 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.3766 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.3764 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.3761 0.0430 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.3761 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.3761 0.0467 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.3761 0.0408 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.3760 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.3760 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.3757 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.3756 0.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.3755 0.0433 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.3754 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.3751 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.3748 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.3747 0.0436 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.3747 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.3746 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.3745 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.3744 0.0430 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.3741 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.3738 0.0428 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.3738 0.0441 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.3736 0.0423 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.3733 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.3733 0.0424 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.3733 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.3731 0.0462 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.3728 0.0416 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.3724 0.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.3721 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.3722 0.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.3722 0.0432 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.3722 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.3723 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.3724 0.0417 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.3725 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.3725 0.0432 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.3725 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.3728 0.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.3728 0.0436 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.3727 0.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.3729 0.0431 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.3728 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.3730 0.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.3731 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.3734 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.3735 0.0406 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.3734 0.0466 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.3730 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.3729 0.0479 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.3730 0.0423 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.3730 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.3730 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.3730 0.0414 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.3731 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.3730 0.0442 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.3729 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.3730 0.0433 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.3731 0.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.3731 0.0422 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.3731 0.0432 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.3731 0.0420 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.3731 0.0450 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.3730 0.0410 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.3731 0.0428 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.3736 0.0478 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.3736 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.3736 0.0411 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.3735 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.3732 0.0464 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.3733 0.0415 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.3734 0.0468 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.3734 0.0413 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.3732 0.0412 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.3730 0.0433 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.3732 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.4538 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.4169 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.4012 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.3950 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.3854 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.3741 0.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.3743 0.0430 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.3729 0.0432 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.3732 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.3720 0.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.3681 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.3676 0.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.3668 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.3686 0.0406 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.3676 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.3661 0.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.3662 0.0421 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.3675 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.3678 0.0428 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.3692 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.3686 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.3688 0.0480 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.3678 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.3678 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.3679 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.3662 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.3650 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.3654 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.3658 0.0421 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.3660 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.3656 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.3644 0.0409 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.3647 0.0424 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.3653 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.3654 0.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.3649 0.0425 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.3641 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.3630 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.3617 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.3614 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.3609 0.0436 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.3617 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.3613 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.3609 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.3612 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.3602 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.3599 0.0426 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.3593 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.3591 0.0432 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.3594 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.3589 0.0403 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.3597 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.3596 0.0429 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.3598 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.3596 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.3597 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.3601 0.0452 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.3599 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.3594 0.0430 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.3601 0.0466 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.3603 0.0464 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.3612 0.0466 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.3616 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.3618 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.3618 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.3620 0.0460 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.3621 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.3617 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.3618 0.0458 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.3616 0.0460 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.3621 0.0440 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.3624 0.0432 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.3628 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.3625 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.3623 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.3625 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.3622 0.0423 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.3621 0.0467 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.3616 0.0434 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.3615 0.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.3611 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.3610 0.0462 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.3604 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.3604 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.3600 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.3598 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.3596 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.3593 0.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.3588 0.0459 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.3590 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.3587 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.3587 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.3584 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.3581 0.0437 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.3578 0.0463 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.3580 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.3580 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.3576 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.3573 0.0463 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.3569 0.0460 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.3569 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.3567 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.3566 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.3564 0.0432 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.3562 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.3561 0.0486 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.3562 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.3562 0.0423 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.3561 0.0421 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.3561 0.0436 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.3558 0.0422 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.3557 0.0446 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.3556 0.0411 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.3555 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.3552 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.3549 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.3548 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.3548 0.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.3548 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.3547 0.0421 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.3546 0.0434 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.3543 0.0422 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.3539 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.3539 0.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.3538 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.3534 0.0431 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.3535 0.0428 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.3535 0.0445 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.3533 0.0432 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.3530 0.0422 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.3526 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.3524 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.3524 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.3525 0.0423 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.3525 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.3526 0.0424 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.3527 0.0459 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.3527 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.3528 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.3527 0.0461 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.3531 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.3531 0.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.3530 0.0462 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.3532 0.0439 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.3531 0.0409 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.3533 0.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.3534 0.0443 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.3538 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.3538 0.0442 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.3537 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.3534 0.0410 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.3533 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.3533 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.3533 0.0419 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.3534 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.3533 0.0408 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.3534 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.3534 0.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.3532 0.0443 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.3534 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.3535 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.3535 0.0423 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.3535 0.0416 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.3535 0.0420 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.3535 0.0407 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.3534 0.0434 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.3536 0.0461 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.3541 0.0412 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.3541 0.0438 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.3541 0.0442 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.3540 0.0466 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.3537 0.0421 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.3538 0.0479 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.3539 0.0463 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.3539 0.0447 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.3537 0.0418 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.3536 0.0414 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.3537 0.0464 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.4333 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.3966 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.3816 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.3755 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.3653 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.3540 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.3544 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.3529 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.3533 0.0442 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.3521 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.3483 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.3479 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.3471 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.3488 0.0439 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.3478 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.3463 0.0483 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.3465 0.0424 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.3478 0.0429 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.3481 0.0430 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.3495 0.0461 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.3491 0.0432 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.3493 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.3483 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.3484 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.3485 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.3468 0.0425 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.3457 0.0429 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.3462 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.3465 0.0425 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.3468 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.3463 0.0467 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.3453 0.0427 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.3456 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.3462 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.3462 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.3458 0.0479 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.3451 0.0435 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.3441 0.0427 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.3428 0.0425 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.3425 0.0487 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.3421 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.3429 0.0464 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.3426 0.0428 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.3422 0.0449 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.3425 0.0433 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.3415 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.3413 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.3407 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.3405 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.3408 0.0445 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.3403 0.0458 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.3411 0.0432 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.3410 0.0439 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.3412 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.3411 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.3412 0.0465 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.3416 0.0438 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.3414 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.3409 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.3416 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.3418 0.0436 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.3427 0.0472 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.3431 0.0420 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.3433 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.3433 0.0428 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.3435 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.3436 0.0459 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.3433 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.3433 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.3432 0.0434 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.3437 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.3440 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.3444 0.0426 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.3441 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.3439 0.0431 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.3441 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.3439 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.3438 0.0436 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.3432 0.0429 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.3432 0.0463 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.3428 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.3426 0.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.3420 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.3421 0.0413 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.3417 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.3415 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.3413 0.0447 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.3410 0.0434 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.3406 0.0419 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.3408 0.0444 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.3405 0.0407 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.3405 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.3402 0.0406 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.3399 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.3397 0.0419 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.3399 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.3398 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.3395 0.0420 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.3392 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.3388 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.3388 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.3387 0.0469 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.3385 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.3384 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.3382 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.3382 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.3382 0.0422 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.3382 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.3381 0.0419 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.3381 0.0434 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.3378 0.0463 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.3377 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.3376 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.3375 0.0432 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.3372 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.3369 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.3369 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.3369 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.3368 0.0434 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.3367 0.0480 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.3366 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.3363 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.3359 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.3359 0.0433 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.3358 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.3355 0.0428 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.3355 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.3355 0.0420 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.3354 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.3351 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.3346 0.0442 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.3344 0.0425 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.3345 0.0489 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.3345 0.0449 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.3346 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.3346 0.0424 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.3348 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.3348 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.3348 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.3348 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.3351 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.3351 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.3351 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.3353 0.0432 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.3352 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.3354 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.3355 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.3359 0.0435 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.3360 0.0434 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.3359 0.0442 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.3356 0.0412 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.3354 0.0424 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.3355 0.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.3355 0.0465 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.3355 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.3355 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.3356 0.0440 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.3356 0.0459 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.3354 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.3355 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.3356 0.0468 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.3356 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.3356 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.3356 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.3356 0.0410 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.3355 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.3357 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.3362 0.0421 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.3362 0.0414 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.3362 0.0413 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.3361 0.0409 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.3358 0.0418 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.3359 0.0411 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.3360 0.0415 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.3360 0.0408 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.3358 0.0417 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.3357 0.0435 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.3358 0.0444 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.4151 0.0408 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.3788 0.0425 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.3640 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.3582 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.3476 0.0480 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.3366 0.0435 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.3369 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.3356 0.0432 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.3359 0.0419 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.3348 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.3310 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.3308 0.0425 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.3300 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.3315 0.0431 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.3305 0.0423 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.3290 0.0460 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.3293 0.0434 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.3305 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.3308 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.3322 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.3318 0.0412 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.3320 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.3310 0.0457 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.3313 0.0461 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.3313 0.0422 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.3296 0.0458 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.3286 0.0462 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.3291 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.3295 0.0430 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.3298 0.0404 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.3293 0.0419 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.3283 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.3286 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.3293 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.3293 0.0478 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.3289 0.0431 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.3282 0.0425 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.3272 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.3260 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.3257 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.3253 0.0426 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.3261 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.3258 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.3254 0.0435 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.3257 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.3248 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.3245 0.0472 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.3239 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.3238 0.0438 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.3240 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.3235 0.0419 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.3243 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.3242 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.3244 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.3242 0.0406 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.3243 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.3247 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.3245 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.3240 0.0439 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.3246 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.3248 0.0469 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.3257 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.3261 0.0419 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.3263 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.3263 0.0466 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.3264 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.3266 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.3262 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.3263 0.0464 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.3262 0.0429 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.3267 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.3271 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.3275 0.0419 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.3272 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.3270 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.3271 0.0442 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.3269 0.0422 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.3268 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.3263 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.3262 0.0427 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.3258 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.3257 0.0449 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.3251 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.3251 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.3248 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.3247 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.3244 0.0443 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.3242 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.3237 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.3239 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.3237 0.0423 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.3237 0.0461 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.3234 0.0440 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.3231 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.3229 0.0474 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.3231 0.0496 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.3230 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.3227 0.0419 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.3225 0.0429 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.3221 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.3221 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.3219 0.0462 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.3218 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.3217 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.3215 0.0447 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.3215 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.3215 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.3215 0.0431 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.3214 0.0419 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.3214 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.3211 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.3210 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.3209 0.0458 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.3208 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.3205 0.0440 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.3202 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.3202 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.3202 0.0450 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.3201 0.0437 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.3201 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.3199 0.0404 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.3196 0.0441 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.3192 0.0411 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.3192 0.0423 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.3192 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.3188 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.3189 0.0426 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.3189 0.0429 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.3187 0.0436 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.3184 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.3179 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.3177 0.0419 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.3178 0.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.3179 0.0443 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.3179 0.0419 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.3179 0.0417 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.3181 0.0489 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.3181 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.3181 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.3181 0.0471 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.3184 0.0439 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.3184 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.3184 0.0408 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.3186 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.3185 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.3187 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.3189 0.0422 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.3192 0.0424 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.3193 0.0462 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.3192 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.3189 0.0425 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.3187 0.0409 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.3188 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.3188 0.0410 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.3189 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.3188 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.3189 0.0437 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.3188 0.0420 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.3187 0.0412 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.3188 0.0416 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.3189 0.0433 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.3189 0.0428 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.3189 0.0442 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.3189 0.0424 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.3189 0.0443 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.3188 0.0467 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.3190 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.3195 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.3195 0.0441 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.3195 0.0417 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.3194 0.0418 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.3191 0.0421 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.3192 0.0413 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.3193 0.0484 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.3193 0.0463 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.3191 0.0460 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.3190 0.0414 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.3191 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.3969 0.0438 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.3620 0.0465 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.3470 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.3418 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.3310 0.0421 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.3201 0.0467 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.3204 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.3193 0.0453 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.3196 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.3185 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.3149 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.3148 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.3140 0.0467 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.3154 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.3143 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.3129 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.3131 0.0429 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.3145 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.3148 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.3161 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.3158 0.0407 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.3160 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.3150 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.3153 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.3153 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.3136 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.3127 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.3133 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.3136 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.3140 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.3134 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.3124 0.0422 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.3128 0.0425 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.3135 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.3135 0.0430 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.3131 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.3124 0.0432 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.3114 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.3102 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.3100 0.0468 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.3095 0.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.3104 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.3101 0.0492 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.3097 0.0437 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.3101 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.3092 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.3089 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.3083 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.3082 0.0441 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.3084 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.3080 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.3088 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.3086 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.3088 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.3086 0.0450 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.3088 0.0422 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.3090 0.0464 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.3089 0.0435 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.3084 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.3090 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.3092 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.3101 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.3105 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.3107 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.3106 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.3108 0.0437 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.3111 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.3107 0.0436 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.3108 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.3107 0.0409 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.3112 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.3115 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.3120 0.0418 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.3117 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.3115 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.3116 0.0435 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.3114 0.0443 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.3113 0.0471 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.3108 0.0463 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.3107 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.3104 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.3103 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.3097 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.3097 0.0441 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.3094 0.0472 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.3092 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.3090 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.3088 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.3083 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.3085 0.0430 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.3083 0.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.3083 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.3080 0.0438 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.3077 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.3075 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.3077 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.3077 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.3073 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.3071 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.3067 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.3067 0.0442 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.3066 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.3065 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.3064 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.3062 0.0439 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.3062 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.3062 0.0440 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.3062 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.3061 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.3062 0.0407 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.3059 0.0463 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.3057 0.0432 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.3057 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.3056 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.3053 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.3050 0.0441 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.3049 0.0414 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.3049 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.3049 0.0447 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.3048 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.3047 0.0426 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.3044 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.3040 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.3040 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.3039 0.0451 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.3035 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.3036 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.3036 0.0442 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.3034 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.3031 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.3026 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.3024 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.3025 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.3025 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.3026 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.3026 0.0464 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.3028 0.0425 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.3028 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.3028 0.0422 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.3028 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.3031 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.3031 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.3031 0.0426 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.3033 0.0421 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.3032 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.3034 0.0423 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.3036 0.0411 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.3039 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.3040 0.0424 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.3039 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.3036 0.0463 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.3034 0.0410 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.3035 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.3035 0.0445 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.3036 0.0427 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.3035 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.3036 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.3036 0.0427 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.3034 0.0417 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.3035 0.0430 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.3037 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.3036 0.0428 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.3036 0.0457 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.3036 0.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.3036 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.3036 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.3038 0.0419 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.3042 0.0431 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.3042 0.0427 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.3042 0.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.3041 0.0416 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.3039 0.0427 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.3040 0.0421 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.3040 0.0437 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.3040 0.0412 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.3039 0.0418 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.3037 0.0413 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.3039 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.3809 0.0403 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.3471 0.0427 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.3324 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.3276 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.3166 0.0440 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.3057 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.3059 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.3050 0.0467 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.3053 0.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.3043 0.0414 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.3007 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.3007 0.0433 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.2999 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.3013 0.0461 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.3001 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.2987 0.0437 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.2990 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.3003 0.0426 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.3006 0.0433 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.3019 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.3016 0.0463 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.3018 0.0431 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.3008 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.3011 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.3012 0.0417 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.2995 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.2986 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.2991 0.0437 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.2995 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.2998 0.0465 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.2992 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.2982 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.2986 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.2993 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.2993 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.2989 0.0408 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.2981 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.2971 0.0417 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.2960 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.2958 0.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.2953 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.2962 0.0417 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.2960 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.2955 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.2958 0.0427 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.2950 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.2947 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.2941 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.2940 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.2942 0.0473 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.2938 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.2946 0.0420 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.2944 0.0426 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.2946 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.2944 0.0417 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.2945 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.2948 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.2947 0.0431 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.2942 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.2948 0.0432 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.2950 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.2959 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.2963 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.2965 0.0431 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.2964 0.0444 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.2966 0.0433 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.2969 0.0452 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.2965 0.0462 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.2966 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.2966 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.2971 0.0466 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.2974 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.2978 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.2975 0.0420 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.2973 0.0493 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.2975 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.2973 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.2972 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.2967 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.2967 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.2963 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.2962 0.0417 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.2956 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.2956 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.2954 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.2952 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.2950 0.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.2948 0.0484 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.2944 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.2945 0.0421 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.2943 0.0408 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.2944 0.0441 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.2941 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.2938 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.2936 0.0438 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.2938 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.2937 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.2934 0.0455 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.2932 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.2928 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.2928 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.2927 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.2926 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.2925 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.2923 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.2923 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.2923 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.2924 0.0426 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.2923 0.0428 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.2923 0.0427 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.2920 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.2919 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.2918 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.2917 0.0439 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.2914 0.0437 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.2911 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.2911 0.0431 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.2911 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.2910 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.2909 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.2908 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.2905 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.2901 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.2901 0.0464 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.2900 0.0418 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.2896 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.2897 0.0448 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.2897 0.0420 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.2895 0.0439 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.2892 0.0487 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.2886 0.0412 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.2885 0.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.2886 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.2886 0.0448 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.2886 0.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.2887 0.0464 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.2888 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.2888 0.0444 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.2888 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.2888 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.2891 0.0440 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.2892 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.2891 0.0437 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.2894 0.0465 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.2893 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.2895 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.2896 0.0466 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.2900 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.2900 0.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.2899 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.2897 0.0410 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.2895 0.0414 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.2896 0.0464 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.2895 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.2896 0.0465 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.2896 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.2896 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.2896 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.2895 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.2896 0.0462 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.2897 0.0445 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.2897 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.2897 0.0419 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.2897 0.0460 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.2897 0.0461 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.2897 0.0422 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.2898 0.0411 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.2903 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.2903 0.0464 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.2903 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.2902 0.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.2900 0.0415 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.2901 0.0427 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.2901 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.2902 0.0427 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.2900 0.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.2899 0.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.2900 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.3658 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.3333 0.0422 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.3187 0.0521 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.3143 0.0427 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.3033 0.0464 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.2925 0.0447 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.2926 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.2918 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.2922 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.2911 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.2876 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.2877 0.0420 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.2870 0.0420 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.2882 0.0445 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.2871 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.2856 0.0423 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.2859 0.0480 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.2872 0.0436 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.2875 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.2888 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.2885 0.0422 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.2887 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.2878 0.0406 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.2881 0.0437 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.2881 0.0429 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.2864 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.2856 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.2862 0.0406 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.2865 0.0438 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.2869 0.0425 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.2862 0.0433 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.2853 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.2856 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.2863 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.2863 0.0420 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.2859 0.0443 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.2852 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.2842 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.2830 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.2829 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.2823 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.2833 0.0426 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.2830 0.0482 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.2825 0.0431 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.2829 0.0465 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.2821 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.2818 0.0424 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.2812 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.2811 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.2813 0.0420 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.2809 0.0422 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.2817 0.0408 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.2815 0.0462 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.2818 0.0437 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.2816 0.0463 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.2817 0.0426 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.2820 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.2818 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.2813 0.0434 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.2820 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.2821 0.0478 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.2830 0.0423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.2834 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.2836 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.2836 0.0440 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.2837 0.0423 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.2840 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.2837 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.2838 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.2837 0.0445 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.2842 0.0427 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.2846 0.0461 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.2850 0.0436 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.2847 0.0431 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.2845 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.2846 0.0436 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.2845 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.2844 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.2839 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.2839 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.2835 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.2834 0.0443 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.2829 0.0422 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.2829 0.0432 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.2826 0.0462 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.2825 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.2823 0.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.2821 0.0422 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.2816 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.2818 0.0437 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.2816 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.2817 0.0482 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.2814 0.0427 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.2811 0.0479 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.2809 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.2811 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.2810 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.2807 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.2805 0.0411 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.2801 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.2801 0.0410 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.2800 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.2799 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.2798 0.0465 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.2796 0.0464 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.2796 0.0431 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.2797 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.2797 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.2796 0.0488 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.2796 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.2793 0.0473 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.2792 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.2791 0.0438 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.2790 0.0435 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.2788 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.2784 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.2784 0.0468 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.2784 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.2783 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.2783 0.0466 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.2781 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.2778 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.2774 0.0434 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.2774 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.2773 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.2770 0.0462 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.2770 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.2770 0.0452 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.2768 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.2764 0.0461 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.2759 0.0420 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.2757 0.0426 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.2758 0.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.2759 0.0472 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.2759 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.2759 0.0463 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.2761 0.0464 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.2761 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.2761 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.2761 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.2764 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.2764 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.2764 0.0429 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.2766 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.2765 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.2768 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.2769 0.0467 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.2772 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.2773 0.0422 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.2772 0.0422 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.2769 0.0467 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.2768 0.0436 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.2768 0.0469 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.2768 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.2769 0.0414 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.2769 0.0484 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.2769 0.0421 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.2769 0.0440 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.2768 0.0412 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.2769 0.0467 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.2770 0.0435 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.2770 0.0423 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.2769 0.0439 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.2769 0.0415 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.2770 0.0440 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.2769 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.2771 0.0413 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.2776 0.0440 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.2776 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.2776 0.0417 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.2775 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.2773 0.0419 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.2774 0.0437 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.2775 0.0462 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.2775 0.0434 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.2773 0.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.2772 0.0426 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.2773 0.0469 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4252 0.0477 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.4122 0.0311 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3973 0.0304 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.3758 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3309 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.2357 0.0305 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.1496 0.0326 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.0714 0.0325 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.9952 0.0332 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.9264 0.0301 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.8648 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.8115 0.0332 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.7650 0.0317 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.7238 0.0305 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.6862 0.0329 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.6525 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.6218 0.0317 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.5964 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.5723 0.0306 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.5485 0.0304 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.5278 0.0330 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.5088 0.0311 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.4913 0.0306 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.4752 0.0323 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.4600 0.0354 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.4467 0.0357 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.4345 0.0304 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.4221 0.0306 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.4108 0.0328 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.4006 0.0311 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.3917 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.3823 0.0311 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.3731 0.0330 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.3651 0.0341 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.3569 0.0320 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.3497 0.0337 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.3421 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.3349 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.3280 0.0316 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.3216 0.0302 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.3153 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.3094 0.0347 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.3036 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.2981 0.0303 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.2927 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.2879 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.2834 0.0318 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.2792 0.0311 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.2750 0.0321 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.2709 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.2669 0.0332 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.2628 0.0301 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.2591 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.2552 0.0328 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.2517 0.0318 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.2479 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.2445 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.2412 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.2378 0.0314 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.2348 0.0328 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.2317 0.0314 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.2292 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.2267 0.0305 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.2236 0.0311 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.2207 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.2182 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.2157 0.0388 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.2126 0.0328 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.2099 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.2075 0.0334 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2051 0.0312 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.2030 0.0325 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.2006 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.1983 0.0309 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.1962 0.0335 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.1943 0.0309 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.1922 0.0316 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.1901 0.0353 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.1880 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.1857 0.0329 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.1836 0.0334 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.1817 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.1798 0.0309 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.1778 0.0305 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.1757 0.0363 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.1737 0.0308 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.1717 0.0302 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.1697 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.1679 0.0304 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.1662 0.0323 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.1644 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.1626 0.0313 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.1608 0.0302 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.1590 0.0304 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.1572 0.0305 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.1553 0.0314 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.1537 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.1519 0.0302 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.1502 0.0330 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.1484 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.1468 0.0349 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.1451 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.1434 0.0310 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.1417 0.0309 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.1399 0.0306 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.1383 0.0328 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.1364 0.0306 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.1346 0.0328 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.1330 0.0329 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.1309 0.0307 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.1292 0.0331 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.1275 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.1257 0.0315 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.1238 0.0371 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.1220 0.0330 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.1201 0.0331 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.1183 0.0344 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.1167 0.0327 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.1151 0.0336 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.1132 0.0319 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.1117 0.0360 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.1099 0.0329 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.1082 0.0351 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.1065 0.0417 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.1047 0.0329 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.1027 0.0356 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.1009 0.0331 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.0992 0.0386 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.0973 0.0338 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.0955 0.0338 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.0938 0.0349 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.0918 0.0353 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.0900 0.0343 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.0881 0.0344 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.0859 0.0339 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.0839 0.0349 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.0819 0.0343 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.0798 0.0406 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.0780 0.0349 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.0760 0.0408 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.0740 0.0367 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.0719 0.0347 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.0698 0.0353 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.0677 0.0360 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.0657 0.0353 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.0637 0.0429 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.0618 0.0356 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.0599 0.0351 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.0577 0.0377 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.0556 0.0356 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.0537 0.0382 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.0519 0.0368 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.0499 0.0354 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.0479 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.0457 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.0436 0.0371 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.0414 0.0360 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.0392 0.0373 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.0369 0.0362 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.0347 0.0363 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.0327 0.0386 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.0304 0.0365 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.0281 0.0367 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.0259 0.0374 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.0238 0.0394 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.0216 0.0367 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.0195 0.0415 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.0174 0.0373 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.0153 0.0373 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.0130 0.0363 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.0109 0.0376 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.0089 0.0372 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.0070 0.0368 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.0051 0.0372 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.0031 0.0378 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.0010 0.0375 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 2.9988 0.0376 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 2.9965 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.6920 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.6325 0.0371 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.6146 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.6081 0.0371 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.6044 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.6060 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.6103 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.6075 0.0378 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.6075 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.6054 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.6017 0.0372 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.6005 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.5983 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.5981 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.5967 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.5953 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.5938 0.0435 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.5938 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.5924 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.5895 0.0373 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.5872 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.5860 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.5840 0.0425 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.5817 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.5794 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.5779 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.5761 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.5741 0.0377 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.5725 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.5709 0.0391 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.5700 0.0458 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.5680 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.5658 0.0431 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.5643 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.5623 0.0377 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.5609 0.0433 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.5590 0.0376 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.5568 0.0410 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.5548 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.5529 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.5507 0.0394 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.5487 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.5465 0.0380 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.5446 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.5426 0.0449 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.5404 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.5391 0.0376 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.5374 0.0455 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.5358 0.0406 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.5346 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.5329 0.0391 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.5314 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.5297 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.5279 0.0408 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.5263 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.5248 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.5234 0.0391 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.5217 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.5201 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.5189 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.5175 0.0403 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.5162 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.5151 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.5138 0.0379 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.5122 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.5112 0.0440 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.5099 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.5080 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.5064 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.5053 0.0430 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.5041 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.5030 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.5017 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.5003 0.0391 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.4991 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.4983 0.0394 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4970 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4959 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4946 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4933 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4919 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.4909 0.0412 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.4896 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.4882 0.0383 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.4866 0.0394 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.4852 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.4839 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.4827 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.4814 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.4803 0.0436 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.4791 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.4780 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.4768 0.0382 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.4755 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.4741 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.4730 0.0439 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.4720 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.4709 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.4698 0.0381 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.4686 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.4677 0.0468 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.4666 0.0399 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.4653 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.4642 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.4631 0.0386 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.4620 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.4608 0.0426 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.4599 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.4590 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.4577 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.4567 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.4558 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.4547 0.0439 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.4536 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.4525 0.0409 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.4512 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.4502 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.4492 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.4483 0.0409 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.4473 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.4465 0.0417 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.4455 0.0393 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.4445 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.4436 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.4426 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.4415 0.0435 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.4407 0.0384 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.4398 0.0411 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.4390 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.4381 0.0393 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.4372 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.4361 0.0393 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.4352 0.0410 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.4344 0.0391 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.4333 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.4324 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.4315 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.4307 0.0404 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.4300 0.0413 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.4291 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.4284 0.0385 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.4275 0.0388 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.4267 0.0403 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.4258 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.4250 0.0391 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.4244 0.0439 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.4235 0.0444 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.4229 0.0460 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.4220 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.4211 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.4204 0.0442 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.4199 0.0395 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.4193 0.0390 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.4186 0.0394 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.4177 0.0398 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.4169 0.0400 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.4160 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.4152 0.0395 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.4142 0.0444 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.4136 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.4129 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.4120 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.4111 0.0389 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.4102 0.0436 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.4095 0.0401 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.4087 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.4080 0.0445 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.4072 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.4065 0.0394 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.4057 0.0392 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.4050 0.0396 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.4044 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.4039 0.0394 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.4034 0.0387 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.4029 0.0405 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.4022 0.0395 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.4013 0.0441 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.4005 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.3569 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.2923 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.2736 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.2687 0.0442 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.2657 0.0383 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.2625 0.0411 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.2616 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.2618 0.0387 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.2629 0.0467 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.2624 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.2603 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.2596 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.2592 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.2613 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.2611 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.2605 0.0386 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.2600 0.0444 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.2614 0.0387 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.2612 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.2596 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.2588 0.0472 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.2602 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.2593 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.2580 0.0439 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.2571 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.2563 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.2555 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.2550 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.2551 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.2549 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.2547 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.2538 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.2528 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.2526 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.2518 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.2513 0.0388 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.2506 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.2491 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.2482 0.0422 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.2473 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.2465 0.0390 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.2458 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.2447 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.2437 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.2427 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.2412 0.0458 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.2410 0.0449 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.2402 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.2395 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.2394 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.2384 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.2382 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.2374 0.0449 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.2365 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.2359 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.2356 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.2351 0.0420 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.2343 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.2336 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.2336 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.2329 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.2327 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.2325 0.0418 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.2319 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.2312 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.2310 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.2305 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.2298 0.0468 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.2290 0.0467 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.2286 0.0447 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.2283 0.0407 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.2280 0.0424 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.2277 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.2270 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.2265 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.2265 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.2259 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.2257 0.0421 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.2249 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.2243 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.2236 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.2233 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.2226 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.2219 0.0452 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.2209 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.2202 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.2197 0.0449 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.2192 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.2185 0.0389 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.2182 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.2176 0.0451 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.2171 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.2165 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.2158 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.2151 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.2145 0.0392 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.2140 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.2135 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.2129 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.2122 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.2119 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.2114 0.0432 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.2107 0.0470 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.2102 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.2096 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.2092 0.0414 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.2087 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.2085 0.0419 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.2081 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.2075 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.2071 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.2067 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.2062 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.2058 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.2053 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.2046 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.2042 0.0445 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.2037 0.0406 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.2035 0.0436 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.2031 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.2029 0.0406 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.2024 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.2019 0.0396 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.2016 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.2012 0.0409 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.2006 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.2003 0.0450 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.2000 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.1995 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.1992 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.1987 0.0456 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.1981 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.1978 0.0391 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.1974 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.1970 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.1967 0.0418 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.1963 0.0429 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.1960 0.0454 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.1958 0.0398 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.1954 0.0453 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.1952 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.1947 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.1943 0.0471 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.1939 0.0418 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.1935 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.1933 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.1930 0.0399 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.1927 0.0423 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.1923 0.0417 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.1918 0.0393 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.1915 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.1914 0.0397 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.1912 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.1910 0.0395 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.1905 0.0408 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.1902 0.0411 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.1897 0.0394 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.1893 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.1887 0.0405 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.1886 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.1883 0.0459 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.1879 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.1875 0.0403 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.1871 0.0404 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.1868 0.0411 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.1864 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.1861 0.0427 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.1859 0.0402 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.1855 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.1851 0.0401 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.1848 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.1846 0.0400 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.1845 0.0415 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.1844 0.0444 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.1843 0.0420 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.1840 0.0444 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.1835 0.0422 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.1831 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.2110 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.1478 0.0448 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.1292 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.1230 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.1189 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.1142 0.0465 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.1136 0.0433 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.1144 0.0447 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.1153 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.1151 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.1129 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.1114 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.1114 0.0430 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.1133 0.0420 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.1130 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.1119 0.0393 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.1115 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.1132 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.1134 0.0429 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.1125 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.1120 0.0415 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.1138 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.1131 0.0452 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.1120 0.0449 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.1115 0.0430 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.1106 0.0448 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.1097 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.1095 0.0453 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.1099 0.0421 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.1097 0.0427 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.1094 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.1085 0.0458 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.1077 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.1079 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.1073 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.1068 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.1063 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.1049 0.0425 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.1040 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.1031 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.1023 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.1019 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.1011 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.1002 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.0996 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.0981 0.0424 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.0980 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.0973 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.0968 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.0971 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.0963 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.0963 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.0958 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.0951 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.0945 0.0417 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.0945 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.0943 0.0451 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.0936 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.0930 0.0453 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.0933 0.0457 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.0928 0.0485 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.0930 0.0511 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.0930 0.0465 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.0928 0.0450 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.0923 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.0923 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.0920 0.0453 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.0913 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.0908 0.0422 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.0906 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.0904 0.0500 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.0903 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.0902 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.0896 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.0892 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.0894 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.0889 0.0455 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.0888 0.0457 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.0882 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.0876 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.0870 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.0868 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.0861 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.0856 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.0847 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.0841 0.0424 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.0837 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.0832 0.0421 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.0826 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.0824 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.0819 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.0816 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 2.0810 0.0455 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 2.0804 0.0451 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 2.0799 0.0426 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 2.0794 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 2.0790 0.0412 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 2.0785 0.0397 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 2.0779 0.0473 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 2.0773 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 2.0770 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 2.0767 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 2.0761 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 2.0757 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 2.0752 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 2.0749 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 2.0746 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 2.0744 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 2.0741 0.0396 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 2.0737 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 2.0734 0.0428 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 2.0732 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 2.0727 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 2.0723 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 2.0719 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 2.0712 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 2.0709 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 2.0705 0.0469 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 2.0703 0.0434 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 2.0701 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 2.0699 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 2.0695 0.0414 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 2.0690 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 2.0689 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 2.0687 0.0423 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 2.0682 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 2.0680 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 2.0680 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 2.0677 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 2.0675 0.0411 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 2.0671 0.0430 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 2.0666 0.0432 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 2.0664 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 2.0662 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 2.0660 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 2.0658 0.0401 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 2.0656 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 2.0654 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 2.0655 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 2.0651 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 2.0650 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 2.0647 0.0471 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 2.0645 0.0408 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 2.0642 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 2.0639 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 2.0638 0.0409 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 2.0636 0.0425 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 2.0635 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 2.0632 0.0422 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 2.0628 0.0424 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 2.0626 0.0430 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 2.0626 0.0428 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 2.0625 0.0416 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 2.0623 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 2.0620 0.0425 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 2.0617 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 2.0613 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 2.0610 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 2.0605 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 2.0605 0.0399 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 2.0603 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 2.0600 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 2.0598 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 2.0595 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 2.0593 0.0398 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 2.0590 0.0450 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 2.0588 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 2.0587 0.0402 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 2.0585 0.0405 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 2.0582 0.0404 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 2.0580 0.0455 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 2.0578 0.0410 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 2.0578 0.0406 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 2.0577 0.0403 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 2.0576 0.0400 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 2.0574 0.0407 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 2.0571 0.0450 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 2.0569 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.1038 0.0447 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 2.0466 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 2.0288 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 2.0210 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 2.0162 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 2.0109 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 2.0104 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 2.0115 0.0452 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 2.0127 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 2.0122 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 2.0100 0.0433 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 724/3560 Training loss: 2.0083 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 2.0082 0.0410 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 2.0099 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 2.0092 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 2.0078 0.0449 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 2.0072 0.0453 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 2.0091 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 2.0091 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 2.0085 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 2.0077 0.0426 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 2.0096 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 2.0088 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 2.0078 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 2.0072 0.0414 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 2.0061 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 2.0050 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 2.0048 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 2.0055 0.0424 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 2.0053 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 2.0049 0.0410 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 2.0040 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 2.0033 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 2.0037 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 2.0031 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 2.0026 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 2.0022 0.0470 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 2.0008 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.9997 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.9987 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.9980 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.9978 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.9970 0.0458 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.9961 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.9959 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.9944 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.9942 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.9936 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.9931 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.9936 0.0457 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.9929 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.9932 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.9928 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.9922 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.9918 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.9918 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.9917 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.9911 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.9906 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.9909 0.0426 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.9904 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.9908 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.9910 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.9908 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.9905 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.9906 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.9905 0.0413 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.9898 0.0474 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.9895 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.9893 0.0424 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.9894 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.9893 0.0428 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.9893 0.0464 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.9888 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.9884 0.0427 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.9887 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.9884 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.9884 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.9878 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.9874 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.9868 0.0462 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.9868 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.9861 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.9858 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.9850 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.9845 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.9842 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.9837 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.9831 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.9830 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.9826 0.0435 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.9824 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.9818 0.0450 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.9813 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.9808 0.0431 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.9805 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.9802 0.0436 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.9797 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.9792 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.9785 0.0486 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.9784 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.9782 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.9777 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.9774 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.9769 0.0449 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.9767 0.0417 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.9764 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.9763 0.0413 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.9762 0.0419 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.9759 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.9756 0.0426 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.9754 0.0478 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.9750 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.9747 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.9743 0.0453 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.9738 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.9735 0.0410 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.9733 0.0458 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.9731 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.9730 0.0422 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.9729 0.0435 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.9726 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.9722 0.0398 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.9722 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.9720 0.0412 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.9716 0.0430 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.9716 0.0424 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.9715 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.9713 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.9711 0.0476 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.9708 0.0407 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.9704 0.0430 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.9703 0.0407 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.9702 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.9700 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.9699 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.9698 0.0425 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.9697 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.9698 0.0459 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.9695 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.9695 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.9693 0.0425 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.9691 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.9690 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.9687 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.9687 0.0408 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.9686 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.9686 0.0399 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.9684 0.0411 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.9681 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.9680 0.0436 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.9680 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.9679 0.0404 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.9678 0.0457 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.9676 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.9674 0.0397 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.9672 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.9670 0.0452 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.9666 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.9667 0.0402 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.9666 0.0422 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.9664 0.0400 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.9663 0.0425 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.9661 0.0422 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.9659 0.0454 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.9657 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.9656 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.9657 0.0416 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.9655 0.0395 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.9653 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.9651 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.9649 0.0415 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.9649 0.0409 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.9648 0.0401 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.9648 0.0405 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.9646 0.0403 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.9644 0.0406 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.9643 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 2.0209 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.9674 0.0401 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.9517 0.0430 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.9436 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.9388 0.0428 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.9316 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.9311 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.9317 0.0428 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.9333 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.9328 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.9307 0.0424 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.9287 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.9287 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.9306 0.0424 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.9299 0.0398 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.9284 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.9279 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.9298 0.0455 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.9298 0.0454 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.9295 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.9290 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.9310 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.9303 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.9294 0.0457 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.9289 0.0427 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.9277 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.9265 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.9265 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.9274 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.9273 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.9270 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.9261 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.9256 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.9262 0.0403 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.9256 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.9252 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.9248 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.9236 0.0431 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.9225 0.0423 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.9215 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.9208 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.9208 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.9200 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.9192 0.0431 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.9191 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.9177 0.0395 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.9176 0.0468 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.9169 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.9165 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.9171 0.0429 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.9165 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.9169 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.9166 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.9161 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.9157 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.9158 0.0430 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.9158 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.9153 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.9148 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.9151 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.9148 0.0427 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.9153 0.0428 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.9155 0.0438 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.9155 0.0435 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.9152 0.0484 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.9155 0.0461 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.9154 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.9149 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.9146 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.9145 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.9147 0.0436 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.9147 0.0469 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.9149 0.0409 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.9144 0.0478 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.9142 0.0458 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.9145 0.0494 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.9143 0.0427 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.9144 0.0404 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.9138 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.9136 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.9130 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.9130 0.0452 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.9124 0.0420 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.9121 0.0412 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.9115 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.9110 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.9108 0.0400 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.9104 0.0410 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.9099 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.9098 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.9095 0.0406 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.9093 0.0399 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.9087 0.0419 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.9083 0.0405 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.9079 0.0416 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.9076 0.0452 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.9073 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.9069 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.9064 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.9058 0.0433 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.9057 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.9056 0.0463 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.9051 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.9048 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.9044 0.0432 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.9043 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.9041 0.0489 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.9040 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.9039 0.0419 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.9037 0.0499 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.9035 0.0443 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.9033 0.0443 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.9030 0.0423 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.9028 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.9024 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.9019 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.9017 0.0413 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.9015 0.0473 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.9013 0.0411 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.9012 0.0433 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.9011 0.0443 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.9008 0.0491 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.9004 0.0484 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.9004 0.0435 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.9002 0.0479 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.8998 0.0422 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.8997 0.0462 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.8997 0.0437 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.8995 0.0439 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.8994 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.8991 0.0426 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.8987 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.8986 0.0441 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.8985 0.0470 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.8984 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.8983 0.0434 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.8983 0.0439 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.8982 0.0444 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.8983 0.0498 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.8981 0.0418 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.8982 0.0427 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.8980 0.0464 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.8978 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.8977 0.0444 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.8975 0.0483 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.8975 0.0421 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.8974 0.0488 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.8975 0.0470 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.8974 0.0450 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.8971 0.0491 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.8969 0.0402 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.8971 0.0488 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.8970 0.0486 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.8969 0.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.8968 0.0551 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.8966 0.0558 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.8965 0.0470 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.8963 0.0409 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.8960 0.0414 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.8961 0.0466 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.8961 0.0419 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.8959 0.0408 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.8959 0.0428 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.8957 0.0442 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.8956 0.0407 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.8954 0.0474 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.8954 0.0477 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.8955 0.0491 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.8954 0.0459 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.8952 0.0415 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.8950 0.0490 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.8948 0.0492 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.8948 0.0422 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.8948 0.0437 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.8947 0.0459 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.8946 0.0460 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.8944 0.0425 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.8943 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.9593 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.9053 0.0450 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.8906 0.0476 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.8829 0.0436 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.8773 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.8687 0.0444 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.8684 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.8682 0.0440 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.8701 0.0412 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.8698 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.8673 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.8653 0.0454 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.8654 0.0412 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.8673 0.0410 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.8668 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.8653 0.0401 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.8649 0.0466 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.8669 0.0492 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.8670 0.0459 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.8671 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.8667 0.0446 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.8686 0.0507 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.8678 0.0474 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.8671 0.0560 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.8667 0.0466 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.8654 0.0544 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.8642 0.0433 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.8643 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.8653 0.0488 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.8653 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.8650 0.0458 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.8640 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.8637 0.0438 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.8643 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.8638 0.0440 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.8635 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.8631 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.8620 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.8609 0.0408 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.8600 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.8593 0.0458 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.8593 0.0478 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.8587 0.0435 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.8579 0.0487 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.8580 0.0419 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.8566 0.0423 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.8564 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.8558 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.8554 0.0451 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.8560 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.8555 0.0426 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.8561 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.8558 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.8555 0.0452 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.8550 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.8552 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.8553 0.0480 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.8548 0.0438 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.8544 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.8547 0.0411 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.8545 0.0423 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.8550 0.0487 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.8552 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.8553 0.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.8551 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.8554 0.0400 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.8554 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.8549 0.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.8548 0.0435 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.8547 0.0457 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.8549 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.8550 0.0431 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.8552 0.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.8547 0.0470 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.8545 0.0523 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.8549 0.0461 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.8547 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.8548 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.8542 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.8540 0.0422 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.8535 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.8534 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.8528 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.8527 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.8520 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.8516 0.0505 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.8514 0.0437 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.8510 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.8505 0.0431 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.8505 0.0462 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.8502 0.0406 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.8500 0.0460 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.8494 0.0433 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.8490 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.8486 0.0412 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.8484 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.8482 0.0493 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.8477 0.0465 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.8473 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.8467 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.8466 0.0431 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.8465 0.0463 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.8461 0.0453 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.8458 0.0460 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.8454 0.0412 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.8453 0.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.8452 0.0435 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.8451 0.0475 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.8450 0.0442 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.8448 0.0435 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.8446 0.0408 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.8444 0.0432 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.8442 0.0432 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.8440 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.8437 0.0503 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.8433 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.8431 0.0455 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.8429 0.0417 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.8427 0.0467 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.8426 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.8425 0.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.8422 0.0412 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.8419 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.8419 0.0427 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.8417 0.0404 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.8413 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.8413 0.0402 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.8413 0.0431 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.8412 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.8410 0.0410 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.8407 0.0424 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.8404 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.8403 0.0430 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.8403 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.8402 0.0475 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.8402 0.0446 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.8401 0.0481 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.8401 0.0458 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.8403 0.0416 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.8401 0.0430 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.8402 0.0420 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.8400 0.0442 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.8399 0.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.8399 0.0438 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.8396 0.0445 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.8397 0.0419 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.8397 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.8398 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.8397 0.0458 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.8395 0.0504 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.8393 0.0479 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.8394 0.0507 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.8394 0.0492 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.8394 0.0436 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.8392 0.0482 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.8391 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.8391 0.0477 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.8389 0.0409 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.8386 0.0428 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.8388 0.0407 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.8388 0.0431 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.8386 0.0462 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.8386 0.0459 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.8385 0.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.8385 0.0456 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.8383 0.0405 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.8382 0.0429 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.8385 0.0499 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.8383 0.0413 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.8382 0.0493 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.8380 0.0415 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.8378 0.0423 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "for lstm_size in [128,256,512]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for learning_rate in [0.002, 0.001]:\n",
    "            log_string = 'logs/hp_selection/lr={},rl={},ru={}'.format(learning_rate, num_layers, lstm_size)\n",
    "            writer = tf.summary.FileWriter(log_string)\n",
    "            model = build_rnn(len(vocab), \n",
    "                    batch_size=batch_size,\n",
    "                    num_steps=num_steps,\n",
    "                    learning_rate=learning_rate,\n",
    "                    lstm_size=lstm_size,\n",
    "                    num_layers=num_layers)\n",
    "            \n",
    "            train(model, epochs, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i178_l512_2.463.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i178_l512_2.463.ckpt\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/anna/i3560_l512_1.122.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]\n\t [[Node: save/RestoreV2_6/_37 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_80_save/RestoreV2_6\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'save/RestoreV2_5', defined at:\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-32-91ad82b46673>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-31-a7ae04af7e97>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\n    self.build()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 684, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2500, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]\n\t [[Node: save/RestoreV2_6/_37 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_80_save/RestoreV2_6\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]\n\t [[Node: save/RestoreV2_6/_37 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_80_save/RestoreV2_6\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-91ad82b46673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"checkpoints/anna/i3560_l512_1.122.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Far\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-a7ae04af7e97>\u001b[0m in \u001b[0;36msample\u001b[0;34m(checkpoint, n_samples, lstm_size, vocab_size, prime)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1548\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]\n\t [[Node: save/RestoreV2_6/_37 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_80_save/RestoreV2_6\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'save/RestoreV2_5', defined at:\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-32-91ad82b46673>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-31-a7ae04af7e97>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\n    self.build()\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 684, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2500, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]\n\t [[Node: save/RestoreV2_6/_37 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_80_save/RestoreV2_6\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
